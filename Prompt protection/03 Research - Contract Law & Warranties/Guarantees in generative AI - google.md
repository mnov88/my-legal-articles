# Guarantees in Generative AI: A Framework for Performance, Liability, and Recourse in B2B Service Agreements

## Section 1: The Contractual Architecture of Generative AI Services

The proliferation of generative artificial intelligence (AI) has introduced a paradigm shift in enterprise technology procurement, compelling a fundamental re-evaluation of the contractual frameworks that govern the provider-customer relationship. Traditional software and cloud service agreements, built on principles of deterministic functionality and predictable performance, are ill-equipped to manage the probabilistic, dynamic, and often opaque nature of AI models. This section deconstructs the modern AI service agreement, analyzing its core components, the evolution of performance commitments, and the critical distinctions between different service delivery models. It establishes the foundational legal and commercial structures that define how performance is promised, how risk is allocated, and how the complex, multi-party AI supply chain is managed.

### 1.1 Deconstructing the Modern AI Service Agreement: Core Clauses and Evolving Norms

The contractual landscape for generative AI is characterized by a distinct bifurcation. On one side are the standardized, non-negotiable "click-through" Terms of Service that govern publicly available tools and consumer-grade access. These agreements, such as those from Google and OpenAI for their general services, offer minimal customer protection and are designed for mass-market adoption.1 On the other side, for enterprise clients, the contractual basis is typically a negotiated Master Service Agreement (MSA), often supplemented by an AI-specific addendum that becomes the focal point of risk allocation.3 This two-tiered market reality underscores that meaningful contractual protections are almost exclusively available to sophisticated B2B customers with sufficient leverage to demand them.

Central to these negotiations are clauses that extend beyond mere performance. Intellectual property (IP) and data ownership are primary battlegrounds. Standard terms often state that the customer owns their input prompts and the resulting output.4 However, this is frequently coupled with broad license-backs that grant the provider extensive rights to use this data—including prompts and, in some cases, outputs—to improve and train their models.4 For enterprise customers, negotiating an opt-out from this data usage is a critical priority to prevent confidential information or proprietary data from being absorbed into the provider's general models, where it could indirectly benefit competitors.4 The complexity of IP ownership is further compounded by the uncertain legal status of AI-generated content, with providers typically assigning whatever rights they may have in the output to the customer, while simultaneously making the customer fully responsible for ensuring the output does not violate any laws or third-party rights.5

The structure of the AI industry introduces another layer of contractual complexity: the AI supply chain. A typical enterprise customer, "Company ABC," may contract with an application vendor, "BigAI," whose service is powered by a foundational model, "Model A," accessed via an API from a large provider. In this multi-party ecosystem, it is imperative to understand whether the terms from the upstream model provider flow down to the customer.10 This creates a chain of dependencies where a disclaimer of liability or a data usage right from the foundational model provider can create significant, and sometimes hidden, risks for the end customer, even if their direct agreement with the application vendor appears robust.11 Without clear contractual alignment and back-to-back indemnities, the customer can be left with a liability gap, where they suffer harm but have no recourse against their direct vendor, who is in turn protected by the terms of their upstream provider.

### 1.2 The AI-Specific Service Level Agreement (SLA): Moving Beyond Uptime to Guarantee Quality

Service Level Agreements (SLAs) have long been the cornerstone of performance guarantees in the technology sector. However, traditional SLAs, which focus primarily on infrastructure-level metrics such as system uptime, availability, and response times, are fundamentally insufficient for evaluating generative AI services.12 An AI service can exhibit 100% uptime and near-instantaneous response times while consistently producing factually incorrect, biased, or nonsensical outputs. This disconnect between operational availability and functional quality renders traditional SLAs inadequate for capturing the true business value and risk of an AI system.

A properly structured AI SLA must therefore evolve to incorporate a new class of metrics and commitments. The foundational components of an effective AI SLA include a meticulously precise service description and scope, which defines what the AI service does and, crucially, what it does not do.14 Vague service descriptions are a leading cause of disputes, as they create ambiguity in how performance should be evaluated.14 Beyond the service description, the SLA must specify AI-specific performance metrics, address data ownership and privacy, define support and maintenance terms (including model update frequency), and establish clear liability and indemnification clauses.14

It is essential to recognize that an SLA is not a warranty of perfect performance. Rather, it is a commitment to meet a defined performance level, coupled with a pre-agreed remedy for failure to do so. In the context of AI services, these remedies are almost universally limited to service credits, which are typically a percentage of the monthly service fee.15 This creates a critical distinction: failing to meet an SLA target is not automatically a material breach of contract that would allow the customer to sue for broader damages. Instead, it triggers a specific, and often modest, financial compensation. This structure contractually limits the provider's financial exposure for performance failures, reinforcing the principle that the ultimate risk of relying on the AI's output remains with the customer.

A central theme in the architecture of AI contracts is the strategic separation of infrastructure reliability from model output quality. Providers have demonstrated a willingness to offer robust, financially backed SLAs for the aspects of their service that are deterministic and easily measurable, such as the availability of their cloud platform or the latency of their API endpoints. This is a mature area of cloud computing where performance is quantifiable and largely within the provider's control.12 Simultaneously, these same providers aggressively disclaim any and all warranties related to the quality, accuracy, or reliability of the AI-generated content itself.1This "contractual decoupling" is a deliberate risk allocation strategy. It allows a provider to meet 100% of its SLA commitments—and thus be in full compliance with the contract—even if the service produces outputs that are completely erroneous and cause significant financial or reputational harm to the customer. This legal framework effectively transfers the entire "last mile" of risk—the validation and use of the AI's output—onto the customer's shoulders.

### 1.3 A Tale of Two Models: Distinctions in Contractual Obligations for API Access vs. Integrated SaaS

The contractual obligations and performance guarantees for a generative AI service vary significantly depending on its delivery model. The two predominant models are raw Application Programming Interface (API) access to a foundational model and a fully integrated, end-to-end Software-as-a-Service (SaaS) application.

**Raw API Access:** This model provides developers and enterprises with direct access to a foundational model (e.g., GPT-4, Gemini, Llama) through an API. The contractual terms are primarily focused on the technical delivery and operational performance of the API itself. Performance guarantees, if offered, are typically confined to infrastructure-level metrics such as **availability** (uptime), **request throughput** (the number of API calls the system can handle per unit of time), and **token throughput** (the volume of tokens the model can process).17 The pricing model is almost always usage-based, charging per token or per API call.21 In this arrangement, the provider's responsibility effectively ends at the API endpoint. The customer is solely responsible for how the API is used, the application built upon it, the validation of its outputs, and the management of all associated risks.23 The terms of service will invariably contain strong disclaimers regarding the accuracy and reliability of the model's outputs.

**Integrated AI SaaS:** This model embeds generative AI capabilities within a broader software application designed to solve a specific business problem, such as an AI-powered customer relationship management (CRM) platform, a legal contract analysis tool, or an automated code generation assistant.16 Because the vendor controls the entire end-to-end solution—from the user interface to the underlying model and the specific prompts used—there is a greater opportunity for the customer to negotiate for performance guarantees tied to tangible business outcomes.9 For example, a customer procuring an AI-powered customer service chatbot could negotiate for an SLA based on the "call containment rate" or "first-call resolution rate".17A user of a contract analysis tool might seek guarantees on "clause extraction accuracy".28 While vendors will still include disclaimers, the integrated nature of the solution makes them more accountable for its overall effectiveness. The pricing for AI SaaS is typically subscription-based, often with tiered plans that offer different feature sets and usage limits.22

The inadequacy of standard MSAs to address the unique risks of generative AI has led to the rise of the "AI Addendum." Rather than undertaking the slow and complex process of rewriting entire master agreements, legal and procurement teams are increasingly using these specialized addenda to append AI-specific terms to their existing contracts.3 This modular approach is a pragmatic response to the rapid evolution of the technology. However, it also means that the AI Addendum has become the primary contractual battleground. It is within this document that the most critical negotiations over data rights for model training, ownership of AI-generated content, liability for hallucinations, and performance monitoring obligations take place. The existence and content of these addenda signal a maturation of the market, moving beyond boilerplate terms and toward bespoke, negotiated risk allocation for sophisticated enterprise customers who understand the stakes.

## Section 2: Defining and Measuring AI Model Performance

Transitioning from the legal framework to the technical reality, the ability to guarantee—or even discuss—AI performance hinges on the capacity to define and measure it. Unlike traditional software with binary outcomes (a function either works or it does not), generative AI performance is a multi-dimensional and often subjective concept. Establishing a robust measurement framework is therefore a prerequisite for any meaningful contractual commitment. This section provides a comprehensive taxonomy of the metrics used to evaluate generative AI systems, explores the process of setting realistic performance baselines, and underscores the indispensable role of human oversight in validating model quality.

### 2.1 A Comprehensive Taxonomy of Generative AI Performance Metrics

A holistic assessment of a generative AI service requires a multi-layered approach to measurement, encompassing the underlying infrastructure, the quality of the model's output, and the ultimate business impact. These metrics can be organized into distinct categories.

**Infrastructure & Operational Metrics:** These are the most traditional and easily quantifiable metrics, forming the bedrock of most AI SLAs. They measure the reliability and speed of the service delivery mechanism.

- **Uptime/Availability:** The percentage of time the service is operational and available to receive requests. This is a standard metric for any cloud service, often guaranteed to levels like 99.9% or 99.99%.13
    
- **Latency:** The time it takes for the model to generate a response. For generative AI, this is often broken down into **Time to First Token** (how quickly the response begins to appear) and **Inter-Token Latency**(the speed at which subsequent tokens are generated). Low latency is critical for real-time applications and positive user experience.17
    
- **Throughput:** The volume of work the system can handle. This is measured as **Request Throughput**(requests per second) and **Token Throughput** (tokens processed per second). High throughput is essential for scaling applications to a large user base.17
    

**Model Output Quality Metrics (General Purpose):** These metrics assess the intrinsic quality of the generated content. They are more complex to measure and often require sophisticated evaluation techniques, including human review or other LLMs acting as judges.

- **Accuracy/Correctness:** The degree to which the model's output is factually correct or aligns with predefined criteria. This is a fundamental measure of reliability.14
    
- **Coherence & Fluency:** Coherence measures the logical consistency and flow of the generated text, while fluency assesses its grammatical correctness and natural language quality.17
    
- **Relevance:** How well the model's response addresses the user's query or prompt. An answer can be fluent and factually correct but entirely irrelevant to the user's intent.31
    
- **Safety/Toxicity:** The absence of harmful, biased, or offensive content in the output. This is a critical metric for responsible AI deployment.17
    

**Metrics for Retrieval-Augmented Generation (RAG) Systems:** RAG applications, which retrieve information from a knowledge base to inform their responses, require a specialized set of metrics to evaluate both the retrieval and generation steps.

- **Groundedness:** Measures whether the generated response is factually supported by the provided context documents. A high groundedness score indicates the model is not "hallucinating" or inventing information.17
    
- **Faithfulness:** A closely related metric that assesses the factual alignment between the generated answer and the retrieved source material. It is a direct measure of the system's reliability in high-stakes domains.30
    
- **Contextual Precision & Recall:** These metrics evaluate the performance of the retrieval component. **Contextual Precision** measures the relevance of the retrieved documents, while **Contextual Recall**measures whether all necessary information was retrieved to answer the query fully.34
    

**Business & User-Centric Metrics:** These metrics connect model performance to tangible business outcomes and user satisfaction. They are often the most meaningful for assessing the overall value of an AI implementation.

- **Task Completion/Success Rate:** The percentage of times the AI successfully completes a predefined task, such as resolving a customer issue or generating a valid piece of code.35
    
- **Adoption Rate:** The percentage of active users for an AI tool, which can indicate its perceived usefulness and performance.17
    
- **Containment Rate:** For customer service applications, this measures the percentage of inquiries resolved by the AI without human intervention, directly translating to cost savings.17
    
- **User Satisfaction:** Often captured through simple feedback mechanisms like "thumbs up/thumbs down" ratings, this provides a direct, albeit subjective, measure of user perception of quality.17
    

### 2.2 Establishing Performance Baselines: From Technical Benchmarks to Business-Specific Outcomes

Before a customer can credibly claim that a model's performance has degraded, a mutually agreed-upon performance baseline must be established at the outset of the contract. This process involves creating a curated test dataset, often called a "golden set," which contains representative examples of the inputs the model will encounter in production.30 By running this golden set through the model, the parties can measure and record the initial values for key performance metrics, creating a benchmark against which all future performance will be compared.36

Setting performance targets requires a balance between ambition and realism. Providers and legal experts consistently caution that perfect accuracy is rarely achievable, especially for complex natural language tasks.14 The pursuit of extremely high performance targets can lead to significantly increased costs or may be technically infeasible.14 Therefore, targets should be tailored to the specific use case. For an internal knowledge base assistant, a task accuracy of 85-90% may be considered strong, whereas a system involved in medical or financial decision-making would demand faithfulness and accuracy rates exceeding 95%.30

The method of evaluation is as important as the metric itself. While automated metrics like BLEU (for translation) and ROUGE (for summarization) can provide quick, quantitative scores, they are often poor proxies for human judgment of quality, as they primarily measure n-gram overlap rather than semantic meaning or factual correctness.33 The gold standard for evaluating nuanced aspects like coherence, relevance, and groundedness remains human evaluation. However, this is expensive, time-consuming, and difficult to scale. This challenge has led to the emergence of "LLM-as-a-judge" evaluation frameworks, where a powerful, separate LLM is used to assess the output of the model being tested against a predefined rubric.37While not a perfect substitute for human judgment, this approach offers a scalable and more consistent way to monitor qualitative performance over time.

The sheer number and variety of available performance metrics create a complex landscape for customers. While this can be seen as a sign of a rapidly innovating field, it also presents a significant challenge. The absence of universally accepted, standardized benchmarks for specific business tasks makes it difficult for customers to conduct apples-to-apples comparisons between different vendors during the procurement process. One vendor might highlight its model's superior BLEU score, while another may emphasize its high rating on a proprietary fluency metric. This lack of a common yardstick benefits the provider, as it complicates the customer's ability to hold them to a clear and objective performance standard. It effectively shifts the burden of expertise onto the customer, requiring them to develop their own sophisticated evaluation methodologies and benchmarks to prove that performance is adequate at the start of a contract and has degraded over time.30

**Table 2.1: Comparative Analysis of Traditional vs. AI-Specific SLA Metrics**

|**Metric Category**|**Traditional Metric (Example)**|**AI-Specific Metric (Example)**|**Measurement Method**|**Contractual Enforceability**|
|---|---|---|---|---|
|**Availability**|System Uptime Percentage|Model API Availability|Automated server monitoring; logs of successful/failed API calls.|High: Easily automated and objectively verifiable. Commonly found in SLAs with service credit remedies.|
|**Responsiveness**|Server Response Time (ms)|Time to First Token (ms)|Automated API call timing from client to first byte of response.|High: Similar to traditional response time; objectively measurable.|
|**Throughput**|Transactions Per Second (TPS)|Output Token Throughput (tokens/sec)|Automated measurement of total tokens generated over a period.|Moderate: Measurable, but can be influenced by input complexity and request concurrency.|
|**Quality**|Error Rate (e.g., HTTP 500s)|Model Accuracy / Correctness|Automated for simple tasks (e.g., classification); requires human review or "LLM-as-a-judge" for complex generation.|Low: Difficult to automate objectively and at scale. Rarely included in SLAs with financial penalties.|
|**Fidelity**|Data Integrity (e.g., no data corruption)|Groundedness / Faithfulness|Comparison of model output against a source/context document; typically requires human or LLM-based evaluation.|Very Low: Highly subjective and context-dependent. Almost never found in binding SLAs; may be a "target."|
|**Compliance**|Adherence to Security Protocols|Safety / Lack of Bias|Automated classifiers for toxicity; requires complex audits and statistical analysis for bias detection.|Low: Often handled via warranties of process (e.g., "developed to mitigate bias") rather than guaranteed outcomes.|

### 2.3 The Critical Role of Human Oversight and Feedback in Performance Validation

Given the inherent limitations of automated metrics and the probabilistic nature of AI, human oversight is not merely a best practice but a contractual necessity. Many provider terms of service, particularly for services intended for high-stakes applications, explicitly obligate the customer to implement human review of AI-generated outputs before they are used or relied upon.2 This clause serves as a powerful liability-shifting mechanism, placing the final responsibility for the consequences of an AI's output squarely on the customer. By agreeing to these terms, the customer acknowledges that the AI is a tool to assist, not replace, human judgment.

Beyond risk mitigation, human interaction provides a vital feedback loop for maintaining and improving model performance. User feedback mechanisms, such as "thumbs up" or "thumbs down" ratings, are a simple yet effective way to collect data on perceived output quality at scale.17 In enterprise agreements, the handling of this feedback can be a specific point of negotiation. The contract may stipulate how this feedback is to be provided, how the vendor is expected to respond to it, and whether the resulting model improvements will benefit a general, multi-tenant model or be applied to a customer-specific, fine-tuned instance.41 This feedback loop is a crucial component of any strategy to combat model drift and ensure the AI system remains aligned with the customer's evolving needs over time.

This dynamic creates a significant "measurement gap" between the metrics that are easily contractible and those that are most critical to business value. There is a clear divide between the objective, easily automated infrastructure metrics like uptime and latency, and the more subjective, difficult-to-measure output quality metrics like groundedness and contextual relevance. Because SLAs thrive on objective, verifiable data, providers readily commit to and offer financial remedies for failures in the former category.13 However, they are extremely reluctant to make binding commitments on the latter, often framing them as "targets" or "objectives" without associated penalties. This gap means that the aspects of AI performance that carry the greatest business risk—incorrect information, biased recommendations, and irrelevant responses—are the least likely to be contractually guaranteed in a meaningful way. The burden of monitoring, measuring, and mitigating these qualitative risks falls almost entirely on the customer.

## Section 3: The Allocation of Risk: Warranties, Disclaimers, and Liability

The commercial viability of generative AI services hinges on a carefully constructed allocation of risk between the provider and the customer. Given the technology's inherent unpredictability—its capacity for "hallucination," bias, and the generation of infringing content—providers have adopted a contractual posture that aggressively minimizes their liability. This section analyzes the prevalent use of disclaimers, the scarcity of meaningful performance warranties, and the complex landscape of liability caps and indemnification. It also examines how emerging regulatory frameworks are beginning to establish a "standard of care" that may, in time, override purely contractual risk-shifting arrangements.

### 3.1 The "As Is" Doctrine in the Age of AI: Analyzing Provider Disclaimers and the Scarcity of Performance Warranties

The contractual foundation for most generative AI services is the legal principle of providing the service "as is." Standard terms of service from virtually all major providers include extensive and explicit disclaimers of warranties. These clauses typically state that the service is provided without any guarantees, express or implied, including the common implied warranties of merchantability and fitness for a particular purpose.2 This legal language effectively communicates that the provider is not promising the service will be suitable for any specific business need the customer may have.

A cornerstone of these disclaimers is the explicit refusal to guarantee the quality or reliability of the AI's output. Terms of service universally warn that outputs may be inaccurate, incomplete, unreliable, or offensive.1Customers are cautioned against relying on the output as a sole source of truth or as a substitute for professional advice in critical domains such as medicine, law, or finance, and are told that independent verification is essential.2 This places the full and unmitigated burden of validating and assuming responsibility for the use of AI-generated content on the customer.

Consequently, performance warranties, which are standard in traditional software licensing, are exceptionally rare in the generative AI space.4 A conventional software vendor might warrant that its product will perform substantially in accordance with its official documentation. AI providers, by contrast, actively avoid such commitments due to the probabilistic and non-deterministic nature of their models.44 When warranties are offered, they are typically limited to "process integrity"—for example, a promise that the service will be provided in a professional and workmanlike manner or that the model was developed using responsible AI principles. They almost never extend to the substantive quality of the output itself.44

### 3.2 Navigating Liability Caps and Indemnification in a Multi-Party Ecosystem

To further insulate themselves from risk, AI providers implement aggressive limitations of liability. A standard clause in any AI service agreement will be a cap on the provider's total financial liability. This cap is almost always tied to the fees paid by the customer over a defined period, such as the preceding 12 months, or, in some cases, a nominal fixed amount like $100.4 Furthermore, providers universally exclude liability for any indirect, incidental, special, or consequential damages, such as lost profits or business interruption.2 This creates a significant financial asymmetry: the potential damages a business could suffer from relying on a faulty AI output (e.g., a flawed engineering design or a discriminatory hiring recommendation) could be orders of magnitude greater than the fees paid for the service, yet their contractual recourse is limited to a fraction of their investment.43

In a significant and strategic move to address one of the market's most pressing concerns, major providers like OpenAI, Google, and Microsoft have begun offering IP infringement indemnities to their enterprise customers.3These "copyright shields" promise to defend and pay for any damages arising from third-party claims that the output generated by the service infringes on their copyright. This is a powerful and valuable protection, but it is also narrowly tailored. It specifically addresses the risk of copyright litigation stemming from the model's training data, a high-profile legal threat.

Conversely, these same agreements typically require the customer to provide a broad indemnity to the provider. The customer must often agree to defend the provider against any claims arising from their use of the service, the content of their input prompts, or any applications they build using the AI.4 This creates what has been termed a "liability squeeze".48 The customer is made contractually responsible for harms that may be caused by the provider's technology, while the provider's own liability is strictly limited.

The recent introduction of IP infringement indemnities by major vendors represents a calculated and sophisticated business strategy. Faced with widespread customer anxiety and high-profile lawsuits over the use of copyrighted material in training data, leading providers made a strategic decision to absorb this specific risk for their enterprise clients.3 This move serves to build trust and differentiate their offerings in a competitive market. However, this specific guarantee can also function as a "strategic decoy." By offering a robust protection against a single, highly visible risk (copyright claims), providers project an image of responsibility and partnership. This can divert a customer's attention from the much broader and more probable business risks—such as financial losses from inaccurate data, legal exposure from biased outputs, or reputational damage from defamatory content—for which the providers continue to disclaim all liability and cap their exposure at minimal levels. The IP indemnity, while valuable, addresses only one slice of the risk pie, leaving the customer to bear the full weight of all other performance-related failures.

### 3.3 The Emerging Standard of Care: How Regulatory Frameworks (NIST AI RMF, EU AI Act) Are Shaping Liability

While contracts heavily favor providers, the legal landscape is not static. A de facto "standard of care" for the development and deployment of AI is beginning to emerge from governmental and industry-led frameworks, and this standard will increasingly influence liability regardless of contractual terms.

The **NIST AI Risk Management Framework (AI RMF)**, though voluntary in the U.S., is rapidly becoming the benchmark for responsible AI governance.49 It outlines a structured approach for organizations to Govern, Map, Measure, and Manage AI risks. A customer can—and should—contractually require its AI vendor to adhere to the principles of the AI RMF. More importantly, in the event of a dispute, a provider's documented failure to follow the framework's guidelines could be presented as evidence of negligence.51 A successful negligence claim could potentially bypass the strict limitations of liability found in the contract, especially in cases of gross negligence.45

The **EU AI Act** is even more direct. As a binding regulation, it imposes specific, non-negotiable obligations on providers of AI systems classified as "high-risk".53 These obligations include mandates for robust risk management systems, high-quality data governance, comprehensive technical documentation, human oversight capabilities, and high levels of accuracy and cybersecurity.54 For any AI system falling under the Act's jurisdiction, these legal requirements will supersede any conflicting contractual disclaimers. Service agreements will need to be updated to explicitly allocate responsibilities for ensuring and documenting compliance with the Act's provisions.45

Providers also use **Acceptable Use Policies (AUPs)** as a contractual tool to manage risk. These policies prohibit users from employing the AI for illegal or specifically designated high-risk activities, such as making final, automated decisions in legal or medical contexts, engaging in social scoring, or creating child sexual abuse material (CSAM).1 A customer's violation of the AUP constitutes a breach of contract and typically voids any protections or indemnities the provider might have offered.40

The development of these external frameworks is creating a new dimension to AI liability. While contracts may attempt to create a self-contained universe of risk allocation heavily skewed towards the provider, they do not exist in a legal vacuum. These frameworks are defining what "reasonable" and "responsible" conduct looks like for an AI provider. A customer who suffers damages from a faulty AI system may have recourse beyond a simple breach of contract claim. They could pursue a tort-based claim, such as negligence, arguing that the provider breached its duty of care to the public by failing to adhere to the established standards set forth in frameworks like the NIST AI RMF or the EU AI Act. This potential legal avenue means that a provider's internal governance and compliance with these frameworks are becoming just as important as the text of their service agreements. For savvy customers, this shifts the focus of due diligence from merely negotiating liability caps to demanding verifiable proof of the provider's adherence to these emerging standards of care.

**Table 3.1: Comparative Analysis of Liability and Indemnification Clauses from Major AI Providers**

|**Provider**|**Service Tier**|**Performance Warranty**|**Liability Cap**|**Provider Indemnity (Scope)**|**Customer Indemnity (Scope)**|
|---|---|---|---|---|---|
|**OpenAI**|Consumer (ChatGPT)|None. Service provided "AS IS." Explicitly disclaims accuracy and reliability.2|Greater of $100 or fees paid in the prior 12 months.5|None offered at this tier.|Broad: Customer must indemnify OpenAI for claims arising from their use of the service or their content.|
|**OpenAI**|Enterprise / API|None on output quality. Beta services are "AS-IS".47|Typically limited to fees paid in the prior 12 months (negotiable in enterprise agreements).|Yes, for third-party IP infringement claims arising from the service's output, with exceptions.47|Broad: Customer indemnifies for claims arising from their content or use of the service in violation of terms.|
|**Google**|Generative AI Services|None. Explicitly disclaims accuracy and warns against reliance for professional advice.1|Typically limited to fees paid in the prior 12 months.|Yes, for third-party IP infringement claims, as part of their general cloud platform indemnity.|Broad: Customer indemnifies for claims arising from their content or use of the service.|
|**Typical Negotiated Enterprise Term**|Enterprise (B2B)|Limited warranty on professional service standards or compliance with documentation; no warranty on output accuracy.44|Negotiable, often 12-24 months of fees. May have "supercaps" for specific breaches like confidentiality or data security.61|IP infringement indemnity is becoming standard. May be extended to cover regulatory fines if negotiated.3|Broad, but may be negotiated to be mutual or limited to the customer's gross negligence or willful misconduct.|

## Section 4: The Challenge of Performance Degradation and the Path to Recourse

This section directly confronts the hypothetical scenario at the heart of the user's query: an enterprise, "Company ABC," finds that the performance of its procured AI service, "Model A," degrades over time for its specific use case. This is not a theoretical risk but a documented technical phenomenon inherent to machine learning systems. The analysis will first explain the technical causes of this degradation—known as model drift and collapse—and then detail the contractual mechanisms and practical steps that constitute the available avenues for recourse.

### 4.1 Understanding Model Drift, Decay, and Collapse in Commercial Deployments

The performance of a machine learning model is not static; it is a snapshot of its capabilities based on the data it was trained on. As the real world changes, the model's performance can degrade. This phenomenon is broadly known as **model drift**.62 It occurs when the statistical properties of the live data the model encounters in production begin to diverge from the data used during its training. For example, a fraud detection model trained on transaction patterns from 2019 may become less effective as consumer purchasing habits shift dramatically in subsequent years.36 A related issue is **concept drift**, where the underlying relationship between input features and the target outcome changes.

A more insidious, long-term risk is **model collapse**. This occurs when new generations of generative models are trained predominantly on the synthetic data produced by their predecessors. Over time, this recursive process can cause the models to "forget" the nuances and diversity of the original human-generated data, leading to a gradual decline in the quality, diversity, and accuracy of their outputs.64 An image generation model might start producing more homogenous faces, or a large language model might generate increasingly repetitive and nonsensical text.64

For a business like Company ABC, these technical phenomena manifest as a tangible business problem. The AI tool that was initially effective for their use case becomes progressively less reliable, producing more errors, becoming less relevant, and ultimately failing to deliver the expected value. This degradation can lead to poor decision-making, user disengagement, and a decline in knowledge or operational efficiency.64

### 4.2 Contractual Guardrails: Drafting Effective Clauses for Monitoring, Notification, and Retraining

Given that performance degradation is a known risk, sophisticated customers can and should negotiate for contractual guardrails to manage it. These clauses create a framework for identifying, reporting, and remediating performance issues.

- **Monitoring and Audit Rights:** The customer's first line of defense is the contractual right to monitor the model's performance and audit the provider's processes. A standard audit clause is insufficient. An AI-specific clause should grant access to critical documentation such as model change logs, retraining records, validation reports, and version histories.65 This transparency is essential for diagnosing the root cause of performance degradation.
    
- **Performance Thresholds as Triggers:** The contract must define objective, measurable triggers that initiate a formal remediation process. Vague commitments are unenforceable. Instead, the agreement should specify that if a key, mutually agreed-upon performance metric (e.g., accuracy, faithfulness, or a business KPI) falls below a predefined threshold for a sustained period (e.g., over a 30-day measurement window), it constitutes a "Performance Failure".62 Other triggers can include the detection of significant statistical drift in the input data or a failure to meet latency and throughput requirements.62
    
- **Notification of Model Changes:** Foundation models are not static; providers are constantly updating them. A critical, and often overlooked, contractual protection is the right to receive advance written notice of any material changes, updates, or substitutions to the underlying model. This notification should be paired with a **non-degradation clause**, which is a warranty from the provider that any such update will not materially degrade the performance of the service for the customer's established and documented use cases.44
    
- **Retraining and Remediation Obligations:** When a Performance Failure is triggered, the contract should clearly outline the provider's obligations. This typically involves a commitment to investigate the issue, perform a root cause analysis, and, if necessary, retrain or fine-tune the model to restore performance.44The agreement should specify a reasonable timeframe for this remediation and clarify how any associated costs will be allocated.
    

The contractual framework for addressing model drift is fundamentally procedural rather than substantive. The clauses focus on creating a defined process—monitor, notify, investigate, attempt to fix—but they do not, and realistically cannot, guarantee a successful outcome. The provider's obligation is typically limited to using "commercially reasonable efforts" to remediate the performance degradation.63 Retraining a massive foundation model is an enormously complex and expensive undertaking, and a fix that helps one customer might inadvertently harm others.66 If the remediation efforts fail, the customer's ultimate contractual remedy is not a claim for damages resulting from the degraded performance, but rather the right to terminate the contract.16 This structure provides an "escape hatch" but does not make the customer whole for the business disruption, lost productivity, and wasted integration costs they have incurred. This reinforces the overarching theme that the customer ultimately bears the substantive risk of a model's long-term performance failure.

### 4.3 Avenues for Recourse: From Service Credits and Retraining Obligations to Termination Rights and Dispute Resolution

When Company ABC experiences performance degradation, its path to recourse is a tiered process dictated by the terms of its agreement with BigAI.

1. **Initial Recourse: SLA Service Credits:** If the degradation manifests as a failure to meet a specific, guaranteed metric in the SLA (most likely an operational metric like latency or uptime), the first and most straightforward recourse is to claim the service credits stipulated in the agreement.14 This is a simple, contractual remedy but is unlikely to address the core issue of declining output quality.
    
2. **Formal Notification and Corrective Action:** Upon detecting a more substantive performance failure (as defined by the contractual triggers), Company ABC must provide formal written notice to BigAI. This notice activates the provider's obligation to investigate and, as per the contract, develop and execute a formal **corrective action plan** with defined timelines to restore performance.36
    
3. **Termination for Cause:** If BigAI fails to cure the performance failure within the contractually defined period, Company ABC should have the right to terminate the agreement for cause. This right is critical, as it allows the company to exit the relationship without penalty and, typically, to receive a pro-rata refund for any prepaid, unused fees.16
    
4. **Dispute Resolution:** If the parties disagree on whether a performance failure has occurred or whether it has been adequately cured, the contract's dispute resolution clause comes into play. Given the highly technical nature of AI performance issues, standard court litigation may be slow and ill-equipped to adjudicate the matter. Consequently, AI service agreements are increasingly specifying alternative dispute resolution (ADR) mechanisms like mediation or binding arbitration.67 Some organizations, like JAMS, are even developing specialized arbitration rules specifically for AI-related disputes, which allow for the appointment of neutral technical experts to help resolve complex factual disagreements about model performance.68
    

The constant cycle of model updates from providers introduces a subtle but significant risk: provider-induced performance degradation. A model that is stable and effective for a customer's specific workflow can be replaced overnight by a new version that, while perhaps better on general benchmarks, performs worse on that specific task. This phenomenon, sometimes called "legal drift" when it affects the interpretation of clauses 70, highlights the growing importance of contractual terms related to **model provenance** and **version control**. Sophisticated enterprise customers must move beyond monitoring for natural model drift and begin negotiating for the right to control their model environment. This could include the right to lock into a specific, stable model version for the duration of a contract, or to demand a robust, use-case-specific validation and non-degradation warranty from the provider before being forced to migrate to a new model version.65 Securing these rights is an advanced defensive strategy that provides a crucial layer of stability in a rapidly changing technological landscape.

**Table 4.1: Model Contractual Clauses for Addressing AI Model Drift**

|**Clause Type**|**Key Objective**|**Sample Language (Illustrative)**|**Relevant Sources**|
|---|---|---|---|
|**Monitoring & Audit Rights**|To grant the customer visibility into model performance and changes over time.|"Provider shall maintain and, upon Customer's written request no more than annually, provide Customer with access to records including model version histories, material changes to model architecture or training data, and validation reports. Customer may, at its own expense, audit Provider's compliance with the performance monitoring obligations herein."|62|
|**Performance Degradation Triggers**|To objectively define what constitutes a "Performance Failure" that requires remediation.|"A 'Performance Failure' shall be deemed to have occurred if, over any continuous 30-day period, the Model's accuracy on the mutually agreed-upon 'Golden Set' of test queries falls more than 5 percentage points below the initial performance baseline established at the Effective Date. A significant shift in input data distribution, as measured by [agreed statistical method], shall also constitute a Performance Failure."|62|
|**Provider Notification of Model Changes**|To prevent unexpected performance degradation due to provider-initiated model updates.|"Provider shall provide Customer with at least sixty (60) days' prior written notice of any material update or replacement of the underlying AI model ('Model Update'). Provider warrants that no Model Update will materially degrade the performance, accuracy, or functionality of the Services for Customer's primary use cases as documented in the Statement of Work."|44|
|**Remediation & Retraining**|To obligate the provider to take corrective action in the event of a Performance Failure.|"Upon the occurrence of a Performance Failure, Provider shall, at its own expense, use commercially reasonable efforts to conduct a root cause analysis and take necessary corrective actions, including model retraining or fine-tuning, to restore performance to the agreed baseline within thirty (30) days of receiving notice from Customer."|44|
|**Termination Rights**|To provide the customer with an exit strategy if performance cannot be restored.|"If Provider fails to cure a Performance Failure within the thirty (30) day period specified in the Remediation clause, Customer may terminate this Agreement for cause upon written notice to Provider, and shall be entitled to a pro-rata refund of any prepaid fees for the remainder of the then-current term."|16|

## Section 5: Strategic Recommendations for the Enterprise AI Consumer

The procurement and management of generative AI services represent a new frontier of technological and commercial risk. To navigate this landscape successfully, enterprises must adopt a strategic, multi-faceted approach that combines rigorous due diligence, sophisticated negotiation, and robust internal governance. This concluding section synthesizes the preceding analysis into a set of actionable recommendations designed to empower enterprise consumers to secure more favorable terms, mitigate risks, and maximize the value of their AI investments.

### 5.1 A Due Diligence Framework for Procuring Generative AI Services

Before entering into any contractual agreement, a comprehensive due diligence process is essential. This process should extend beyond a simple feature-by-feature comparison and delve into the technical, legal, and risk-related aspects of the vendor and their offering.

- **Technical Due Diligence:** The procurement team, in partnership with technical stakeholders, should assess the vendor's underlying technology and operational maturity. Key areas of inquiry include the vendor's data governance policies, their internal processes for model monitoring and management of model drift, and their documented adherence to established frameworks like the NIST AI Risk Management Framework.10 Understanding how the vendor builds, tests, and maintains its models provides crucial insight into the potential for future performance issues.
    
- **Legal and Contractual Due Diligence:** Legal counsel must meticulously scrutinize the vendor's standard contract templates, paying close attention to the clauses that allocate risk. This includes a thorough analysis of data usage rights (especially concerning model training), the scope of disclaimers, the limits of liability, and the breadth of indemnification clauses.4 The goal is to identify the provider-friendly defaults and asymmetries that are common in the market and to prepare a negotiation strategy to counter them.43
    
- **Use Case Risk Assessment:** Not all AI applications carry the same level of risk. The organization must conduct an internal risk assessment to classify the intended use case. A tool for generating low-risk internal marketing copy requires a different level of contractual scrutiny than a high-risk system used for making credit decisions, diagnosing medical conditions, or screening job applicants.26 The higher the potential for financial, legal, or reputational harm, the more robust the contractual protections must be.
    

### 5.2 Key Negotiation Strategies for Enhancing Performance Guarantees and Mitigating Risk

Armed with the findings from the due diligence process, the enterprise can enter negotiations with a clear set of objectives aimed at rebalancing the typical provider-friendly agreement.

- **Focus on Business Outcomes, Not Just Technical Metrics:** While operational metrics like uptime and latency are important, they do not guarantee business value. Where possible, especially for integrated SaaS solutions, negotiations should focus on establishing SLAs tied to specific, measurable business KPIs that are relevant to the intended use case.15 This aligns the provider's incentives with the customer's desired outcomes.
    
- **Link Warranties to External Frameworks:** Given that providers are unwilling to warrant output accuracy, a powerful alternative is to demand a contractual warranty that the service has been developed and will be maintained in accordance with a recognized standard of care, such as the NIST AI RMF or, where applicable, the EU AI Act.45 This introduces an objective, external benchmark for responsible practice into the agreement.
    
- **Negotiate "Supercaps" for High-Risk Breaches:** While it may be difficult to eliminate liability caps entirely, it is often possible to negotiate higher caps for specific, catastrophic events. These "supercaps" can apply to breaches of confidentiality, major data security incidents, or regulatory fines directly attributable to a failure of the AI system, providing a greater level of financial protection for the most severe risks.61
    
- **Demand Version Control and Non-Degradation Clauses:** To protect against provider-induced performance degradation, the contract must include a clause requiring advance notice of any material model updates. Crucially, this should be coupled with a non-degradation warranty, ensuring that any new model version will perform at least as well as the previous one for the customer's specific, established workflows.65
    

### 5.3 Implementing Internal Governance and Monitoring to Independently Validate Vendor Performance

Contractual protections are only effective if they can be enforced. The final pillar of a successful AI procurement strategy is the implementation of strong internal governance and independent monitoring capabilities.

- **Establish an Internal AI Oversight Committee:** Following the "Govern" function of the NIST AI RMF, the enterprise should establish a cross-functional governance body comprising representatives from legal, compliance, IT, security, and the relevant business units.49 This committee is responsible for overseeing all AI procurement, setting internal policies for acceptable use, and managing vendor relationships.
    
- **Deploy Independent Monitoring Tools:** An enterprise cannot rely solely on a vendor's self-reported performance data. It is essential to implement in-house or third-party monitoring tools to continuously track key performance metrics against the established baseline.36 This independent validation is the only way to reliably detect model drift and gather the objective evidence needed to trigger contractual remediation clauses.
    
- **Maintain Meticulous Records:** Documentation is paramount. The organization must keep detailed records of the entire AI lifecycle, including the vendor selection and due diligence process, internal risk assessments, performance baselines, ongoing monitoring results, all detected incidents of performance failure, and all communications with the vendor regarding these issues.51 In the event of a dispute or a regulatory inquiry, this body of evidence will be indispensable for enforcing contractual rights and demonstrating that the organization exercised a reasonable standard of care in its deployment of AI technology.