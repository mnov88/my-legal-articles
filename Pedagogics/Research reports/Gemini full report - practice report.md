# Multiply-loaded pedagogy through generative AI in legal education

## 1. Introduction: from AI adoption to design awareness

A foundational critique of modern legal education, articulated in the 2007 Carnegie Foundation report _Educating Lawyers_, remains largely unresolved. The report observed that the dominant case-dialogue method effectively "drills abstraction" by training students to extract legally relevant facts from complex human situations. Yet, the corresponding task of "connecting these legal conclusions with the rich complexity of actual situations that involve full-dimensional people" remains outside the core pedagogical framework.[^1] Nearly two decades later, this tension between analytical rigour and contextual connection persists. The arrival of generative artificial intelligence (AI) presents a novel opportunity to address this long-standing challenge, but its current trajectory of adoption risks exacerbating the problem rather than solving it.

The integration of generative AI into higher education is no longer a prospective development; it is a present and pervasive reality, driven overwhelmingly by students themselves. Recent surveys document a systemic shift in student practice. In the United Kingdom, 92% of undergraduates reported using at least one AI tool in 2025, a substantial increase from 66% the previous year.[^2] Globally, a 2024 survey across 16 countries found that 86% of students regularly use AI in their studies.[^3] Student motivations are primarily oriented towards efficiency. The Higher Education Policy Institute's 2025 survey found that 51% of students use AI to "save time" and 50% use it to "improve the quality of my work".[^4] This efficiency imperative is most apparent in assessment practices, where student use of generative AI surged from 53% to 88% in a single academic year.[^5]

This student-led adoption contrasts starkly with the more hesitant and shallow engagement by faculty and institutions, creating a significant readiness crisis. While a 2025 global survey found that 61% of faculty across 28 countries have used AI in their teaching, 88% of those individuals reported only minimal to moderate use.[^6] Faculty confidence remains low, with only 17% rating their AI literacy as advanced or expert, and 40% describing themselves as just beginning.[^7] Institutional policy development has also failed to keep pace. As of 2025, only 39% of higher education institutions had established AI-related acceptable-use policies, an increase from 23% the prior year but still lagging far behind student practice.[^8]

This asymmetry produces a de facto pedagogical environment where norms are not set by educators but are emerging from student practice—an environment that prioritizes cognitive offloading over the process-oriented goals of legal education. The consequences are tangible. A significant majority of students—80%—report that their university's integration of AI does not meet their expectations.[^9] A substantial portion feel unprepared for an AI-enabled future, with 58% stating they lack sufficient AI skills and 48% feeling inadequately prepared for the workplace.[^10] This perception is shared by institutional leaders; a survey by the Association of American Colleges and Universities and Elon University found that 59% of higher-education leaders believe recent graduates are not prepared for workplaces where AI tools are important.[^11]

The current state of AI adoption in legal education largely involves automating existing, single-dimension pedagogical approaches—generating quizzes more quickly, summarising cases, or providing automated feedback. This approach misses the technology's transformative potential. What is absent is design awareness—the intentional use of AI's flexibility to create learning materials that engage students across multiple pedagogical dimensions simultaneously.

This article introduces and defends the concept of multiply-loaded pedagogy as a design principle to address this gap. It argues that by using generative AI, legal educators can move from a model of material scarcity—where one hopes to find a case that happens to be doctrinally precise _and_ emotionally resonant—to a model of design agency, where one can systematically specify and generate materials that are intentionally loaded with multiple pedagogical handles. These handles include the rational, emotional, social, sensory, and metacognitive dimensions of learning. By demonstrating this principle through varied, practical examples, this article contends that intentional integration of these handles within single learning objects is more effective than their multiplication across separate materials. It shows how AI's capacity for specification, rather than mere search, makes it possible to finally bring the task of "connecting" inside the core of legal pedagogy.

## 2. The case for multiply-loaded pedagogy

The proposition of multiply-loaded pedagogy rests on three foundational arguments. First, a substantial body of learning science research demonstrates that engaging multiple pedagogical handles—including emotional, social, sensory, and metacognitive pathways in addition to the traditional rational one—independently produces significant learning gains. Second, principles of human cognitive architecture suggest that integrating these handles within a single, coherent learning material is more efficient and effective than multiplying them across separate, disconnected interventions. Third, the predominant existing framework for multi-pathway engagement, Universal Design for Learning (UDL), has failed to scale precisely because it is predicated on a model of resource multiplication rather than integration, creating an efficiency gap that a new approach must address.

The current chasm in the quality of AI-generated educational materials reinforces the need for an intentional design framework. On one hand, studies evaluating AI-generated lesson plans produced from generic prompts find them to be of "moderate to low quality," with nearly half of all learning activities coded at the lowest cognitive level of "remember".[^12] This represents the output of a single-handle approach where an educator requests a rational product—a lesson plan—without specifying other pedagogical dimensions. On the other hand, a 2025 meta-analysis of educational technologies found that AI chatbots and generative tutoring systems produced the largest positive effect on learning outcomes, with a Hedges' g of 1.02.[^13] These systems are effective because they are inherently multi-handle; they combine rational content delivery with metacognitive scaffolding, adaptive feedback, and an emotionally non-judgmental environment. This disparity demonstrates that AI is most effective when intentionally designed for multi-dimensional engagement. Multiply-loaded pedagogy provides the design principles to extend the effectiveness of responsive tutoring systems to the proactive generation of all learning materials.

### 2.A Individual handle evidence

Multiply-loaded pedagogy builds upon established foundations in the learning sciences. Each of its constituent handles has been independently validated through extensive empirical research, demonstrating measurable positive effects on learning, retention, and transfer.

The **emotional** handle leverages the finding that emotion and cognition are deeply intertwined. Emotional arousal enhances memory consolidation, with meta-analyses showing effect sizes ranging from $d=0.4$ to $d=0.8$.[^14] The neurobiological mechanism involves the modulation of memory processes by the amygdala, and these effects tend to strengthen over time, proving particularly beneficial for long-term retention.[^15] Specific emotions can be harnessed for pedagogical effect. Humor has been shown to improve comprehension and retention, with an effect size of $\eta^2=0.27$, though its benefits may emerge most strongly at delayed retrieval, such as a six-week follow-up.[^16] Similarly, the use of bizarre or surprising imagery can enhance memory, but its effectiveness is contingent on a mixed-list design where bizarre items are interspersed with conventional ones.[^17] Recent research specific to AI confirms its capacity to engage this handle. AI tutors are often perceived by students as a "safety net" that reduces the anxiety associated with asking questions or making mistakes.[^18] Furthermore, a 2025 study in the _European Journal of Education_found that an AI-enhanced Social-Emotional Learning (SEL) framework significantly boosted student engagement and emotional well-being among 816 students, improving their emotional regulation and academic focus.[^19]

The **social** handle is grounded in the understanding that learning is a fundamentally social process. Interventions designed to enhance students' sense of social belonging have been shown to cut achievement gaps between minority and majority students by over 50%.[^20] The quality of peer relationships is a powerful predictor of academic performance, in some studies more so than relationships with parents or teachers.[^21]

The **sensory** handle incorporates insights from dual-coding theory and embodied cognition. The principle of dual coding—presenting information both verbally and visually—has a well-documented positive effect on learning, with meta-analyses reporting effect sizes from $d=0.28$ to $d=0.53$.[^22] More profoundly, embodied learning, which involves the learner's physical and sensorimotor systems, has been associated with exceptionally large learning gains. A 2022 meta-analysis reported an effect size of $d=1.23$ for embodied learning interventions.[^23]

The **metacognitive** handle focuses on teaching students to plan, monitor, and evaluate their own learning. Approaches that foster metacognition and self-regulation are among the most effective educational interventions known, capable of producing an additional eight months of academic progress over a year.[^24] Peer assessment, a powerful metacognitive activity, has a moderate overall effect size of $d=0.31$, which increases to $d=0.61$ when the assessment is student-initiated.[^25] AI is proving to be a potent tool for metacognitive development. A 2024 study in _Scientific Reports_ analyzing 834 reflective journal entries found that students actively use AI as a "metacognitive scaffold," externalizing planning by creating checklists, monitoring by comparing their work to AI feedback, and evaluating their understanding by explaining errors to the AI.[^26] Another study found that requiring students to rate their confidence and explain their reasoning _before_ receiving AI feedback on a practice exam significantly promoted self-regulated learning, suggesting that structured reflection is more impactful than sophisticated AI feedback alone.[^27]

Finally, the **rational** handle represents the traditional core of legal education, focused on doctrinal analysis and logical reasoning. Its effectiveness is well-established, particularly through the use of formative assessment—low-stakes, frequent checks for understanding—which produces effect sizes between $d=0.40$ and $d=0.70$.[^28] Further, transparent pedagogy, which involves making the purpose, tasks, and evaluation criteria of an assignment explicit, has been shown to reduce achievement gaps and improve student success.[^29]

### 2.B The integration thesis

The evidence that multiple pedagogical handles are independently effective does not, on its own, justify integrating them. The case for integration—for loading multiple handles onto a single learning object—is grounded in the architecture of human cognition. The central thesis is that simultaneous, integrated presentation of multi-modal information is more efficient than sequential or separated presentation, provided it is designed to avoid cognitive overload.

The foundation of this argument lies in Allan Paivio's dual-coding theory, which posits that the human mind has evolved specialized, parallel systems for processing verbal and non-verbal information.[^30] As elaborated in Alan Baddeley's model of working memory, the phonological loop processes linguistic information while the visuospatial sketchpad processes visual and spatial information.[^31] These systems can operate concurrently. Further, neuroscientific evidence suggests that emotional processing, mediated by structures like the amygdala and its interaction with the hippocampus, operates on a pathway distinct from the executive functions of working memory.[^32] This parallel architecture means that the cognitive systems for processing language, images, and emotion do not necessarily compete for the same limited resources and can be activated simultaneously.

However, this potential for parallel processing is contingent on proper instructional design. John Sweller's Cognitive Load Theory provides a critical warning through the split-attention effect.[^33] When related sources of information—such as a diagram and its corresponding explanatory text—are physically or temporally separated, the learner is forced to expend finite working memory resources on the task of mentally integrating them. This effort does not contribute to learning—it is an extraneous cognitive load imposed by poor design.

The synthesis of these principles comes from research demonstrating the superiority of simultaneous presentation when properly supported. A 2020 study by Begolli and colleagues found that students learned complex science concepts better when multiple visual representations were presented simultaneously rather than sequentially.[^34] Crucially, this benefit was only realised when the simultaneous presentation was scaffolded with self-explanation prompts that guided students to actively build connections between the representations.[^35] Without such scaffolding, simultaneity can create confusion.

This leads to a final, vital caveat from Kalyuga and colleagues: quality of integration matters more than quantity of handles.[^36] Their research on the redundancy effect shows that poorly integrated or redundant information can be actively harmful to learning. For example, presenting a diagram with identical on-screen text and audio narration is worse than presenting the diagram with audio alone. The learner is forced to process redundant information, which creates an extraneous cognitive load. Therefore, the goal of multiply-loaded pedagogy is not to activate as many handles as possible, but to integrate two or three handles in a way that is coherent, mutually reinforcing, and directly serves a unified learning objective.

### 2.C UDL's failure and the efficiency gap

The dominant framework for addressing learner variability through multiple pathways has been Universal Design for Learning (UDL). While founded on the laudable goal of creating more inclusive learning environments, UDL's implementation model is predicated on resource multiplication, a strategy that has proven to be unsustainable in practice and questionable in its empirical foundations. This failure creates an efficiency gap that multiply-loaded pedagogy, as a model of integration, is designed to fill.

The UDL framework, as articulated by Meyer, Rose, and Gordon, is organised around three core principles: providing multiple means of engagement, representation, and action and expression.[^37] In practice, this requires educators to create and provide numerous parallel resources—alternative formats for content (text, video, audio), a variety of assessment options (essays, presentations, projects), and multiple rubrics. While some meta-analyses have shown positive effects associated with UDL interventions, the framework has faced a devastating critique.[^38]

In a series of papers, Guy Boysen has argued that UDL shares troubling theoretical and operational similarities with the discredited concept of learning styles.[^39] Both frameworks are based on the "matching hypothesis"—the idea that learning is enhanced when instruction is matched to a student's preferred mode of learning or representation—a hypothesis for which there is little empirical support. Boysen contends that many studies cited in support of UDL do not validate the principle of providing _multiple means_; rather, they simply demonstrate that a single, well-chosen pedagogical technique was effective for a group of learners.[^40]

Even if the theoretical underpinnings were sound, UDL's model of resource multiplication has created an insurmountable implementation barrier for faculty. A 2022 survey by Hills and colleagues found that 62% of faculty cited time and workload as major barriers to adopting new, more inclusive teaching methods.[^41] The timelines for institutional adoption are equally daunting. Proponents of UDL estimate that achieving 50% adoption within an institution can take four to seven years of sustained effort.[^42] This is the efficiency gap: the pedagogical ambition of providing multiple pathways is defeated by the practical impossibility of multiplying faculty time and resources.

Multiply-loaded pedagogy offers a direct alternative. Instead of resource multiplication, it proposes intentional integration. Rather than creating three separate resources to engage three different pathways, the goal is to design a single resource that integrates three pedagogical handles. This shifts the burden from laborious content creation to thoughtful upfront design—a shift made practical by the flexibility of generative AI. The efficiency gap that UDL could not solve creates the necessary space for this alternative, integration-based approach. 

## 3. AI's flexibility as design enabler

The primary contribution of generative AI to pedagogy is not the automation of existing tasks. The ability to generate content, summarise text, or grade assignments more quickly is a known, if modest, efficiency gain.[^43] The truly transformative potential of AI lies in its capacity to enable the intentional specification of a learning material's pedagogical properties. This capability fundamentally alters the economics of instructional design, shifting the educator's role from a time-consuming search for imperfect materials to the efficient and systematic design of precisely tailored ones. This shift is not merely about automation; it is about an expansion of design agency.

This shift is critical in light of emerging research on the cognitive consequences of unreflective AI use. Preliminary findings from an MIT Media Lab study using EEG to measure brain activity during essay writing suggest that participants using ChatGPT exhibited the weakest neural connectivity compared to those using a search engine or no tools.[^44] A separate quasi-experimental study from the Stanford SCALE Initiative found that while students using AI showed short-term performance gains on lower-order tasks, their long-term retention at two to three weeks was lower than that of control groups.[^45] These findings suggest that using AI for simple cognitive offloading may create an "illusion of understanding" without fostering durable learning. This underscores the urgency of moving from passive AI adoption to an intentional design framework like multiply-loaded pedagogy, which requires active cognitive engagement across multiple dimensions.

### 3.A Legal materials as optimal substrate

Legal education is uniquely positioned to benefit from a multiply-loaded approach because its core teaching materials—judicial opinions and hypothetical scenarios—are an inherently multi-dimensional substrate. Unlike a mathematical proof, which is primarily rational, or a physics experiment, which is primarily sensory and rational, a legal case naturally contains the raw material for engaging multiple pedagogical handles simultaneously.

A typical judicial opinion involves real people with compelling emotional stakes, creating a natural entry point for the emotional handle. It contains a precise application of doctrinal rules, demanding rigorous rational analysis. It often raises broader policy questions with significant social relevance, engaging the social handle. The strategic reasoning and argumentation within the case provide a model for metacognitive reflection on what it means to "think like a lawyer." Finally, many cases involve physical objects, locations, or visual evidence that can serve as sensory anchors for memory.

The Carnegie Report recognised this latent potential but concluded that the dominant case-dialogue method fails to exploit it; the task of connecting the abstracted legal analysis back to its human and social context "remains outside."[^46] The challenge for legal educators has never been the impossibility of integration, but rather the scarcity of materials that achieve it perfectly and the prohibitive time cost of creating such materials manually.[^47] The traditional pedagogy of contract law, for example, has been critiqued for its narrow focus on nineteenth-century doctrinal categories that fail to engage with the reality of modern transactions or students' lived experiences.[^48] This is not because contract law lacks human drama, but because finding or crafting materials that seamlessly blend doctrine with that drama is exceptionally difficult.[^49] The cognitive load on an educator attempting to manually draft a hypothetical that is doctrinally precise, narratively coherent, and emotionally resonant is immense.[^50]

This is the problem that the combination of legal materials and generative AI is uniquely suited to solve. The natural affordances of legal subject matter provide the optimal substrate, and AI's flexibility provides the tool for intentional design. This combination creates an unprecedented opportunity to address the fragmented, decontextualized understanding that often characterises first-year legal knowledge and to finally make the connection between doctrine and humanity an intrinsic part of legal pedagogy.[^51]

### 3.B From scarcity to specification

The fundamental shift enabled by generative AI in instructional design is the transition from a paradigm of scarcity to one of specification. This change alters both the economics of material creation and the cognitive role of the educator.

In the pre-AI era, a legal educator seeking to create a multi-dimensional learning experience faced a stark choice, constrained by time and cognitive load.[^52] The first option was to engage in a time-consuming search for an existing judicial opinion that happened to strike a perfect balance between doctrinal purity and emotional resonance. Such a case was a pedagogical "lightning strike"—rare, valuable, and not reliably reproducible. The second option was to manually draft a hypothetical scenario. This process is labour-intensive, often requiring several hours per scenario. Due to the limits of human working memory, this manual drafting process typically forced the educator to optimise for a single dimension.[^53] The result was a binary: a doctrinally rigorous but sterile problem that drilled abstraction, or an engaging narrative that was doctrinally imprecise. The practical constraints of this scarcity model resulted in a pedagogy that privileged the rational handle, even though learning science consistently demonstrated the value of multi-modal engagement.

Generative AI dissolves this binary by changing the core task from search to specification. An educator no longer needs to hope that the perfect material exists; they can now design it by specifying its desired pedagogical properties. For instance, an educator can prompt an AI to: "Generate a contract formation scenario that tests the doctrine of consideration (rational), involves a sympathetic small business owner facing a difficult choice (emotional), is set within a real-world consumer dispute (social), and concludes with a prompt asking students to reflect on how their initial intuition aligns with the legal outcome (metacognitive)." An initial prompt might take thirty minutes to refine, but the resulting template is a reusable asset that can be adapted across semesters and doctrines.

The economics shift from hours of uncertain searching to a finite period of upfront design followed by infinite, free variation. More importantly, the cognitive role of the educator shifts from content creator to pedagogical architect. This is not mere automation; it is an expansion of design agency, making the systematic implementation of multiply-loaded pedagogy a practical reality for the first time. This principle of specification can be extended to the design of entire learning environments. For example, the "personalisation without isolation" model proposes using AI to handle individualized, adaptive practice on rational-doctrinal tasks, thereby freeing finite classroom time for instructor-led collaborative activities that engage the social handle.[^54] While this specific model currently lacks empirical validation—a research gap noted later—it illustrates how the principle of specification allows for the intentional and strategic deployment of different pedagogical handles across different learning spaces.

## 4. Varied examples of multiply-loaded design in practice

The principles of multiply-loaded pedagogy are best understood through practical application. The following examples demonstrate how the design principle can be instantiated in diverse ways across different instructional contexts. These are not intended as an exhaustive taxonomy or a set of required mechanisms. Rather, they serve as proofs-of-concept, illustrating the flexibility of using generative AI to intentionally integrate multiple pedagogical handles. The unifying element is not the specific technique but the underlying principle: that intentional integration, enabled by AI's flexibility, is a more efficient and effective pedagogical strategy than the multiplication of separate interventions.

### 4.A Example: empathy calibration through comparative scenarios

A single, brief exercise can be designed to engage five pedagogical handles simultaneously, transforming students' intuitive emotional responses into an object of rigorous legal analysis. This "empathy calibration" exercise takes approximately eight minutes and uses a controlled comparison to make students aware of their own cognitive biases.

The design begins with a prompt to a generative AI: "Generate three short scenarios involving data processing under the GDPR. Each scenario must rely on a different lawful basis from Article 6—for example, legitimate interests, performance of a contract, and vital interests—but none should use consent. Ensure the factual details are comparable in complexity. However, vary the context to elicit different levels of sympathy: one involving a hospital treating patients, one a bank preventing fraud, and one an employer managing payroll." The AI can also be prompted to generate simple, representative images for each party to serve as visual anchors.

In the classroom, students are presented with the three scenarios and asked to rank their personal comfort level with the data processing in each case, from most comfortable to least. Invariably, students report higher comfort with the hospital's processing of patient data than with the employer's processing of employee data, even if the legal analysis is identical. The ensuing discussion asks the core question: should the legal standard for "legitimate interests" or "necessity" shift depending on how sympathetic the data controller or data subject is?

This single exercise engages multiple handles in an integrated fashion. The **emotional** handle is activated by the students' varying feelings of empathy and the subsequent surprise when they realise their comfort levels have shifted for non-legal reasons. This violation of expectation creates a powerful emotional anchor for the memory of the legal principle, an effect well-documented in research on emotional memory, which shows enhanced consolidation for arousing events.[^55] The **rational** handle is engaged through the core legal skill of comparison and distinction, as students must analyse whether the doctrinal standard itself contains a variable for sympathy.[^56] The **metacognitive** handle is perhaps the most critical. By confronting the divergence between their intuition and the formal legal standard, students engage in an act of bias awareness that is fundamental to developing professional judgment.[^57] The **social** handle is activated by using recognisable, real-world contexts—healthcare, banking, employment—that make an abstract legal doctrine feel concrete and relevant to consumer protection and daily life.[^58] Finally, if AI-generated images are used, the **sensory** handle provides distinct visual cues that can improve memory recall through the von Restorff effect, where distinctive items are better remembered.[^59]

The integration is seamless: the emotional response is not a distraction from the legal analysis; it _becomes_ the analytical object. The exercise models a form of expert legal cognition where empathy is not ignored but is treated as an analytical question to be managed. This one exercise, built from a single prompt, creates multiple pathways to understanding and demonstrates how benefits can emerge over time, consistent with findings that the memory-enhancing effects of emotion are often most pronounced at delayed retrieval.[^60]

### 4.B Example: error detection materials

Brevity does not preclude multi-loading. A three-minute exercise focused on error detection can effectively engage four pedagogical handles by making the error itself multi-dimensional, triggering an emotional and metacognitive response that reinforces a rational correction.

The prompt is simple and specific: "Generate a three-sentence summary of the lawful bases for processing under GDPR Article 6. The summary must contain one subtle but common doctrinal error that first-year law students often make." An AI might produce: "Under the GDPR, data controllers must always obtain explicit consent from individuals before processing their personal data. This consent must be freely given, specific, and informed. It is the primary and most important basis for ensuring lawful data processing."

The student's task is to identify the error—the false claim that consent is the "primary" basis, which ignores the five other co-equal lawful bases under Article 6. This brief interaction is multiply-loaded. The **rational** handle is engaged as an act of critique and evaluation. To spot the mistake, a student must have a comprehensive understanding of all six lawful bases, making this a powerful tool for formative assessment.[^61] The **emotional** handle is triggered by the jolt of surprise or "remembered frustration"—the feeling of "I almost wrote that on my last assignment!" This violation of expectation serves to anchor the correction in memory, a mechanism consistent with research on the memory-enhancing effects of emotional arousal.[^62]

The **metacognitive** handle is activated through the process of error detection itself. Recognising a plausible but incorrect statement makes a student's own mastery tangible. The memory of their own past confusion, contrasted with their current ability to spot the error, solidifies the learning.[^63] This process of self-monitoring and recognizing improvement is a key component of self-regulated learning, which has been shown to add up to eight months of academic progress.[^64] Finally, the **social** handle can be engaged by having students work in small groups to find the error. However, a critical warning is necessary here. While collaboration is beneficial, structuring this as a competition with public leaderboards can be counterproductive. Research on gamification has found that performance-contingent rewards and competitive rankings can undermine intrinsic motivation, with some meta-analyses showing negative effect sizes from $d=-0.28$ to $d=-0.40$.[^65] The exercise should be framed as a collaborative puzzle, not a race.[^66]

In this design, the error is not merely a rational puzzle. The emotional jolt of recognition is the very mechanism that drives the correction home. This approach differs from the empathy calibration example but relies on the same core principle: intentional multi-handle design through flexible AI prompting.

### 4.C Example: counterfactual scenarios for transfer testing

Humor and absurdity are not pedagogically empty; when designed with intention, they can serve as powerful tools for testing deep, transferable understanding versus superficial knowledge. A counterfactual scenario uses anachronism to create an engaging and memorable test of a student's ability to abstract legal principles from their original context.

The prompt might be: "Generate a one-page legal compliance memo advising a 1920s telegraph company on how to comply with the modern-day GDPR. The scenario should be factually anachronistic but doctrinally analyzable." The resulting memo would require students to distinguish what is absurd (the technological medium of the telegram) from which legal principles still apply regardless of the technology (e.g., purpose limitation, data minimisation, security obligations, and transparency).

This exercise engages several handles. The **emotional** handle is activated through humor. The absurdity of applying data protection law to telegrams is inherently amusing, which can lower student anxiety and increase engagement.[^67] The **rational** handle is engaged in a sophisticated test of transfer and pattern recognition. To succeed, students cannot rely on recognizing keywords like "server" or "database." They must abstract the _why_ behind the rules—the fundamental principles—from the _what_ of their typical application. This separates deep understanding from surface-level memorisation. The **metacognitive** handle is engaged as a form of depth-testing. The absurdity of the scenario exposes whether a student's knowledge is robust or brittle. It makes plain the difference between knowing the rules and understanding the principles that animate them.[^68] If visual elements are included, such as an AI-generated diagram of a "telegram data flow," a **sensory**handle adds a multimodal layer to the experience.

This technique requires two critical caveats grounded in learning science. First, research by McDaniel and Einstein on the "bizarreness effect" demonstrates that it is only effective in a **mixed-list design**.[^69] An entire course of absurd examples would lose its distinctiveness and effectiveness. Bizarre hypotheticals must be interspersed with conventional, serious doctrinal analysis. Second, as shown in studies by Kaplan and Pascoe, the cognitive benefits of humor, such as improved retention, may not be immediately apparent and often emerge at a **delayed follow-up**, for instance, six weeks later.[^70]

In this design, the absurdity is the transfer test. It is not mere decoration but the core pedagogical instrument that forces students to reason from first principles, demonstrating again how an integrated, multiply-loaded design can achieve a sophisticated learning objective efficiently.

### 4.D Example: interactive role-play for professional identity

Generative AI's capacity for adaptive interaction enables a form of scalable, private practice that was previously impossible. Through an interactive role-play, students can practice client communication, test the boundaries of their knowledge, and develop their professional identity in a psychologically safe environment.

The design begins with a role-play prompt for the AI: "You are a confused but intelligent data subject who has just received a complex privacy notice from your hospital. The student you are interacting with is your lawyer. Your goal is to understand your rights. Start by asking a general question. Based on the student's explanation, ask increasingly specific and challenging follow-up questions that probe the limits of their understanding. For example, if they explain 'legitimate interests,' ask 'Wait, so the hospital doesn't need my consent to use my data at all? What if they also use it for pharmaceutical research?' Continue until the student's explanation breaks down or they admit they need to research the answer."

This dynamic exchange is multiply-loaded. The **rational** handle is engaged as the student must apply doctrine precisely and accurately under questioning, a form of dialogic formative assessment.[^71] The **social** handle is activated through professional role-play. The student practices the crucial skill of translating complex legal concepts into clear language for a client, a core component of professional identity formation.[^72] This practice occurs in an environment of high psychological safety, free from the performance anxiety that can accompany Socratic questioning or live-client clinics.[^73] Research has shown that such safety is critical for the effectiveness of authentic assessments.[^74]

The **emotional** handle is engaged through empathy and curiosity. By interacting with the AI's simulated confusion, the student builds empathy for the client's perspective. The desire to successfully answer the AI's next question—to satisfy the "client"—is driven by curiosity and sustains engagement.[^75] The **metacognitive** handle is perhaps the most powerful component. The AI's persistent, adaptive questioning inevitably leads the student to a point where their explanation falters. This moment of failure is not public or graded; it is a private discovery of the boundaries of their own knowledge, a powerful impetus for self-correction and deeper learning.[^76]

However, the implementation of such interactive AI tools faces a significant challenge: a "crisis of credibility." While intelligent tutoring systems show large positive effects on learning,[^77] a 2025 arXiv preprint (not yet peer-reviewed) found that students exhibit a strong bias against AI-generated feedback when its source is disclosed.[^78] In the study, students preferred the quality of AI feedback in a blind comparison, but when the source was revealed, only the AI-labeled feedback suffered a decline in perceived "genuineness." This source bias threatens the efficacy of AI tutoring and role-play. It implies that for this example to be effective, educators may need to consider strategies like framing the feedback as a human-AI co-production or focusing on building student AI literacy to mitigate this bias. The technical effectiveness is proven; psychological acceptance requires intentional design.

### 4.E Example: student-created assessment materials

Shifting students from consumers of information to producers of pedagogical materials engages them at the highest levels of cognitive complexity and fosters a deeper, more durable understanding. Using AI as a collaborative tool, students can design, author, and critique assessment materials, an activity that is itself a powerful, multiply-loaded learning experience.

The exercise can be structured with a multi-part prompt. A student might be instructed to prompt an AI as follows: "First, acting as a law professor, create a short, fact-pattern exam question that tests the application of the GDPR's lawful bases under Article 6. Include a detailed model answer. Second, acting as a student taking the exam, generate a flawed answer to that same question. The flawed answer should contain at least two common but subtle doctrinal errors." The student's task is then to use the model answer to grade and provide feedback on the AI's flawed response. This process can be extended by having students exchange the questions they have co-created for peer review.

This production-oriented task engages multiple handles. The **rational** handle is activated at multiple levels of Bloom's taxonomy. Creating a valid exam question requires synthesis and an understanding of what is most important about a topic. Writing a model answer requires precise application. Evaluating the flawed response requires critical analysis and error detection. The **metacognitive** handle is engaged through the development of assessment literacy. As research by Van Deursen and others suggests, students who are tasked with creating questions often "consistently belong to the best" performers because the process forces them to internalize the criteria for success and develop evaluative judgment.[^79]

The **social** handle is activated when students grade each other's co-created questions and flawed answers, a form of computer-mediated peer assessment that meta-analyses have found to be effective.[^80] Finally, the **emotional** handle is engaged through a sense of ownership and agency. According to self-determination theory, the autonomy to create and control one's own learning materials generates a powerful sense of investment and intrinsic motivation.[^81]

A caveat is warranted regarding the final output. While the process of creating assessments is pedagogically valuable, the quality of AI-generated assessment items requires human oversight. A 2025 study in _BMC Medical Education_ found that while 69% of AI-generated medical exam questions were deemed fit for use after expert screening, they tended to test lower-order cognitive skills and still contained factual inaccuracies.[^82] Therefore, the value of this exercise lies in the student's process of creation and critique, using the AI as a tool, not in the unverified quality of the AI's output.

### 4.F Synthesis: diverse paths to same principle

These five examples—comparative scenarios, error detection, counterfactuals, interactive role-play, and student production—illustrate the flexibility of multiply-loaded pedagogy as a design principle. They employ different combinations of pedagogical handles; for instance, the empathy calibration exercise engages all five, while the error detection task focuses on four. They operate in varied instructional contexts, from an eight-minute in-class exercise to a more involved homework assignment. They also utilize different prompting strategies, from specifying comparative constraints to designing an interactive persona.

The point is not to establish these five techniques as a fixed canon of "AI mechanisms." Rather, it is to demonstrate that multiply-loaded pedagogy is an adaptable principle that can be aligned with an educator's specific content, student level, and authentic teaching style. What unifies these diverse examples is the core commitment to intentional, multi-handle design, made practical by the flexibility of generative AI. The underlying thesis, supported by cognitive science, remains constant: that quality integration is more effective than the mere multiplication of resources, whether that integration is achieved through empathy calibration, surprise-inducing errors, or student-led production.[^83] The principle is stable; its implementations are infinite.

## 5. Developing design awareness: a thinking framework

The effective use of generative AI in legal education requires a shift in faculty development, away from a focus on mere technical adoption and toward the cultivation of design awareness. The acute literacy gap—where only 17% of faculty rate their AI skills as advanced or expert, and 58% of students feel they lack sufficient AI knowledge—highlights the need for practical, pedagogically grounded guidance.[^84] The challenge is not simply to use AI, but to use it with intention. This section provides a reflective framework, rather than a prescriptive checklist, to help educators develop this design awareness when creating teaching materials. The goal is to move from prompting for content ("generate a case study on contract formation") to prompting for pedagogical effect ("generate a case study that engages rational, emotional, and social handles to teach contract formation").

### 5.A Practical prompting strategies

Developing design awareness translates into a set of concrete, iterative prompting practices. The process is one of refinement, moving from a broad pedagogical goal to a specific, effective learning object. It is a design skill that improves with practice, not a form of magical automation.

The recommended approach is to start with a core learning objective and then layer in pedagogical handles iteratively. An initial prompt might specify the core doctrine and one additional handle. For example: "Generate a short scenario about a breach of contract under the GDPR that involves a sympathetic data subject." The educator then evaluates the AI's output: does the scenario authentically engage the intended emotional handle, or does it feel generic?

Based on this evaluation, the prompt is refined to add specificity and additional handles. A second iteration might be: "Refine the scenario. Make the data subject an elderly patient whose sensitive health data was mishandled by a clinic (emotional specificity). Set the dispute in the context of a confusing 'terms of service' agreement for a new healthcare app (social relevance). Conclude the scenario with a question asking students to predict their initial emotional reaction to the clinic's actions before they begin their legal analysis (metacognitive)."

It is generally more effective to specify desired _properties_ rather than exact content. Prompting for "a sympathetic litigant" gives the AI more creative flexibility than prompting for "a 64-year-old woman with cancer," which can be overly restrictive. An effective prompt structure often follows a pattern: + [Emotional Angle] + + [Instructional Format] + [Assessment Method].

Typically, two to four rounds of iteration are sufficient to produce a well-integrated, multiply-loaded material. Once a successful prompt structure is developed for a particular type of exercise, it can be saved as a template and adapted for different legal doctrines. To reduce individual workload and foster a community of practice, faculty can be encouraged to share and collaboratively refine these prompt templates.

### 5.B Recognizing quality integration vs. overload

A core component of design awareness is the ability to distinguish between materials that are well-integrated and those that are simply overloaded. The goal is quality over quantity; adding more handles is not always better and can be actively detrimental if they are not coherent.

Well-integrated materials feel unified and natural. The different pedagogical handles emerge organically from the scenario and reinforce one another. In the empathy calibration example, the student's emotional response _is_ the object of rational and metacognitive analysis; the handles are fused.

Poorly integrated materials, by contrast, feel fragmented and "tacked on." They often create a split-attention problem, forcing students to switch between disconnected tasks: a case study, followed by a separate reflection prompt, a separate diagram, and a separate role-play exercise. This is not integration; it is the multiplication of resources that UDL promotes, and it creates extraneous cognitive load.[^85] Warning signs of cognitive overload include student confusion about what to focus on, a jarring feeling when switching between tasks, and a sense of redundancy. As research by Kalyuga and others has shown, redundant information—such as presenting identical on-screen text and audio—interferes with learning and can be worse than a single modality.[^86]

An educator can use a simple test to evaluate integration quality: Does adding this handle _clarify_ the learning objective or _complicate_ it? If an emotional element does not serve to deepen the student's doctrinal understanding, it should be cut. If a metacognitive prompt feels forced and disconnected from the core task, it should be omitted. The optimal design typically involves two or three well-integrated handles, not necessarily all five. As the work of Begolli et al. demonstrates, making simultaneous presentation of multiple representations effective requires scaffolding—such as self-explanation prompts—to help students process the different streams of information.[^87] Without this deliberate integration and scaffolding, simultaneity risks creating confusion rather than clarity.

### 5.C Adapting to teaching style and context

A significant barrier to the adoption of pedagogical innovations is the perception that they require faculty to abandon their authentic teaching persona. Faculty often resist "active learning" not out of pedagogical disagreement, but from a sense of self-preservation: "I'm not going to sing and dance."[^88] Multiply-loaded pedagogy overcomes this barrier because it is a principle of instructional _design_, not a mandate for pedagogical _performance_. It changes the materials an educator uses, not who they are in the classroom.

The flexibility of the framework allows for infinite stylistic implementations. A professor known for serious, rigorous doctrinal analysis can use the empathy calibration exercise without any need for an emotional performance; the material itself is designed to engage affect through controlled comparison. A professor who prefers to teach with gravitas can use outrage at an unjust outcome as the emotional handle, rather than humor. The framework is also technologically flexible. An educator with minimal comfort with technology can rely entirely on text-based scenarios generated by an AI, with no multimedia components required.

Each pedagogical handle offers a spectrum of approaches. The emotional handle can be engaged through humor, empathy, surprise, outrage, curiosity, or remembered frustration. The social handle can be activated through consumer experiences, professional role-plays, cultural references, or peer collaboration. The sensory handle can be visual (static or dynamic), auditory, or even related to the craft of the text itself. An educator should select the angles that align with their authentic teaching style.

This adaptability is what enables the principle to scale across diverse faculty where more rigid, one-size-fits-all frameworks like UDL have failed. The underlying design principle—integration is superior to multiplication—remains constant, but its expression can be tailored to preserve the authenticity and strengths of each individual educator.

## 6. Limitations, warnings, and future directions

While multiply-loaded pedagogy offers a promising framework for leveraging generative AI, its proposal must be accompanied by a rigorous and transparent acknowledgment of its limitations, risks, and the need for empirical validation. This article puts forward a theoretical argument and a series of proofs-of-concept; it does not claim to present validated empirical findings. A responsible approach requires clear warnings about potential negative effects and a defined agenda for future research.

The framework is bounded by several research-grounded limitations. First, the risk of **cognitive overload** is real. As demonstrated by Kalyuga and colleagues, poorly integrated or redundant multimedia can interfere with learning.[^89] Preliminary findings from recent studies at the MIT Media Lab and Stanford's SCALE Initiative suggest that unreflective use of AI for cognitive offloading can lead to weaker neural engagement and lower long-term retention, respectively.[^90] Second, the use of game-like elements carries **gamification dangers**. A systematic review of 87 papers by Toda and colleagues identified potential negative effects, and a foundational meta-analysis by Deci and colleagues confirmed that extrinsic, performance-contingent rewards like leaderboards can undermine intrinsic motivation.[^91]

Third, instructional design must account for the **expertise reversal effect**. Scaffolding that is essential for novices can be redundant and annoying for experts, impeding their learning.[^92] Fourth, educators must have realistic **temporal expectations**. The benefits of some pedagogical handles, particularly emotional ones like humor or arousal, often emerge over weeks, not in immediate test results.[^93] Finally, some techniques have specific **context requirements**. The bizarreness effect, for example, is only effective when used in a mixed-list design and loses its power if overused.[^94]

### 6.A When not to use multiply-loaded design

The principle of multiply-loaded pedagogy should not be applied dogmatically. There are specific learning contexts where a focused, single-handle approach is more appropriate and effective. Pedagogical judgment is required to determine when to integrate and when to focus.

A multiply-loaded approach is generally not optimal when students are first acquiring foundational knowledge, such as basic legal vocabulary or simple concepts. During this initial acquisition phase, the intrinsic cognitive load of the material is already high for a novice. Adding extra pedagogical handles can risk overloading their limited working memory.[^95] A focused, rational engagement may be the most efficient path to establishing this initial schema.

The approach is also less suitable for learners who are already experts in a domain. For these students, the additional scaffolding provided by emotional or metacognitive prompts can be redundant. This triggers the "expertise reversal effect," where instructional support that helps novices can hinder experts by forcing them to process unnecessary information.[^96]

When the legal doctrine itself is exceptionally complex, with high element interactivity, it may be prudent to separate the teaching of the doctrine from its contextual application. An educator might first use a focused, rational approach to teach the intricate rules, and only then introduce a multiply-loaded scenario that integrates those rules with emotional and social context.

Finally, in situations of extreme time constraint, such as a two-minute explanation, a single-handle approach is likely more efficient than a rushed and poorly executed attempt at integration. The evidence on AI's impact on critical thinking reinforces this need for discretion. The effect of AI is mode-dependent; its use for simple information retrieval can lead to shallower analysis, whereas its use for structured, reflective thinking can enhance critical thinking skills.[^97] Therefore, a multiply-loaded design is most effective for (1) teaching foundational legal reasoning skills where transfer is the goal, (2) helping first-year students build robust schemas that connect doctrine to context, (3) fostering professional identity, and (4) making complex doctrine more accessible. It is not a universal solution for every learning objective.

### 6.B AI system limitations and risks

The use of generative AI as a design tool introduces its own set of technical limitations and ethical risks that educators must actively manage. The technology is a powerful assistant, not an infallible authority.

The most significant technical limitation is the propensity of AI systems to generate plausible but factually inaccurate content, often referred to as "hallucinations." This risk is particularly acute in law, where precision in case citations, statutory references, and doctrinal statements is paramount. All AI-generated content, especially that which purports to state legal rules, requires rigorous fact-checking by a faculty member with subject-matter expertise.

Furthermore, AI models can reflect and amplify biases present in their vast training data. This can manifest in stereotyped representations of individuals in hypothetical scenarios or a skewed presentation of legal issues. Educators must critically review AI-generated materials for such biases.

Broader systemic risks include issues of equity and privacy. If institutions do not provide universal access to powerful AI tools, an access gap can exacerbate existing educational inequities. The use of student data in prompts, particularly when using third-party AI services, raises significant data privacy concerns, especially under regulatory frameworks like the GDPR.

The risk of student over-reliance leading to shallow learning is a primary concern. The preliminary findings from the MIT Media Lab EEG study (weaker neural coupling in ChatGPT users) and the Stanford SCALE study (lower long-term retention) suggest that using AI as a cognitive replacement rather than a cognitive tool can be detrimental to durable learning.[^98] This evidence reinforces the argument that the risk is not AI itself, but its unreflective use for automation. Multiply-loaded design, by explicitly requiring active engagement across multiple cognitive dimensions, serves as a direct pedagogical countermeasure to this risk.

### 6.C Research agenda for empirical validation

This article presents a theoretical framework and a series of proofs-of-concept for multiply-loaded pedagogy. It does not provide empirical validation of its effectiveness. A rigorous research program is necessary to test its claims and refine its application. This agenda should align with the most pressing, independently identified research gaps in the field of AI in education, which themselves point toward the need for a more integrated, multi-dimensional understanding of AI's impact.

Key research questions for empirical validation include:

1. **Comparative Effectiveness**: Do multiply-loaded materials generated with AI lead to better long-term retention and transfer of legal reasoning skills compared to traditional single-handle materials or materials designed using UDL's multiplication approach? This requires experimental or quasi-experimental studies measuring not just immediate performance but also retention at 6-week and 6-month intervals, as well as performance on novel problem-solving tasks.
    
2. **Optimal Loading Levels**: How many pedagogical handles are optimal for different contexts? Research should investigate how the number and combination of handles should vary based on student expertise level (novice vs. advanced), the intrinsic complexity of the legal doctrine, and student background.
    
3. **Equity Effects**: Does a multiply-loaded approach reduce achievement gaps? UDL claims to do so but lacks robust evidence. Studies should disaggregate outcomes by first-generation status, socioeconomic background, and other demographic variables to determine if this integrated approach has a differential, positive impact on underserved student populations. This aligns with a major identified research gap concerning equity and algorithmic bias in AI tools.
    
4. **Long-term Professional Development**: Does exposure to multiply-loaded pedagogy in law school correlate with stronger professional competencies in practice, such as client empathy, ethical reasoning, and contextual analysis? This requires longitudinal studies that follow graduates into their careers. This question directly addresses the research gap concerning the need for long-term studies on skill development versus short-term performance gains.[^99]
    
5. **Implementation Fidelity and Social Learning**: Can the principles of multiply-loaded pedagogy be scaled across different institutions and teaching styles without significant degradation in quality? This includes empirical validation of blended models like "personalisation without isolation," a current research gap.[^100]
    

This agenda also requires investigation into the under-researched **sensory dimension** of AI-generated content and continued work on the **quality and validity of AI-generated assessments**, both of which are recognised gaps in the current literature.[^101] By framing these disparate research needs within the coherent structure of multiply-loaded pedagogy, a more systematic path for inquiry emerges.

## 7. Conclusion: design awareness for the AI era

The abstraction problem in legal education, identified by the Carnegie Report nearly two decades ago, has persisted largely because the tools to systematically bridge the gap between doctrinal analysis and human context were unavailable.[^102] The ubiquitous adoption of generative AI by students—now exceeding 86% globally—has created a new pedagogical reality.[^103] Yet, this adoption, driven by a student-led quest for efficiency, has thus far been unmoored from pedagogical design, leading it to replicate and even accelerate the single-handle, rational-privileging approaches of the past. The result is a widening readiness gap, with a majority of students feeling their institutions' use of AI is inadequate and leaders doubting graduates' preparedness for an AI-enabled profession.[^104]

This article's contribution has been to propose a framework to move legal education from reactive adoption to intentional design. It has advanced four main arguments. First, it has articulated a design principle: **multiply-loaded pedagogy**, grounded in cognitive theory, which posits that the integration of multiple pedagogical handles within a single learning object is more effective than their multiplication across separate resources.[^105] This was positioned against the practical failures of UDL's multiplication-based model. Second, it identified the **enabling conditions** for this principle: the unique flexibility of generative AI, which makes intentional specification practical, and the inherently multi-dimensional nature of legal materials, which provide an optimal substrate. Third, it offered a **design awareness framework** to guide educators in shifting their practice from searching for materials to specifying their pedagogical properties. Fourth, it provided a diverse set of **proofs-of-concept** demonstrating the principle's flexible application across different teaching styles and instructional contexts.

The fundamental shift proposed is not from human to machine, but from scarcity to agency. The question is no longer whether students will use AI, but whether educators will develop the design literacy to channel that use toward meaningful pedagogical goals.

### 7.A Implications for different stakeholders

The arguments presented carry distinct implications for the various stakeholders in legal education and educational technology.

For **legal educators**, the primary implication is to cultivate design awareness. When using AI to create teaching materials, the focus should shift from simple content generation to the intentional specification of multiple pedagogical handles. This does not require becoming an AI expert or adopting a rigid, unfamiliar teaching method. It requires developing a reflective practice of considering which handles—rational, emotional, social, sensory, metacognitive—best serve a given learning objective and then experimenting with prompts to integrate them. The principle is adaptable to each educator's authentic teaching style.

For **AI and educational technology researchers**, the focus of tool development should expand beyond automation and efficiency. This article identifies a need for systems that support pedagogical engineering. What prompting interfaces could better scaffold an educator's ability to specify and integrate multiple handles? How might AI systems be designed to actively support the development of an educator's own design awareness?

For **learning scientists**, multiply-loaded pedagogy offers a concrete alternative to UDL's multiplication approach, raising a host of empirical questions. Research is needed to determine optimal loading levels for different learners, to measure the long-term effects on retention and transfer, and to rigorously assess the framework's impact on educational equity.

For **institutional leaders**, this framework offers a more scalable and efficient path for faculty development than large-scale UDL implementation, which can take years to achieve partial adoption.[^106] Supporting faculty in developing design awareness and prompt literacy can be achieved through targeted workshops, the creation of shared prompt libraries, and providing time for collaborative development. This approach focuses on enhancing existing practice with new tools, rather than mandating a complete overhaul of pedagogical philosophy.

### 7.B Closing: from scarcity to agency

For decades, legal educators operated within a paradigm of material scarcity. The practical constraints of time and cognitive load forced a false choice: doctrinal purity or emotional engagement; rational analysis or social context. This scarcity model inevitably led to a pedagogy that privileged the rational-analytical, even as learning science demonstrated that deep, durable learning is an integrated, multi-dimensional process. The abstraction problem was not an inherent feature of legal subject matter; it was an artifact of the tools available to teach it.

Generative AI does not merely provide more materials; it provides design agency. When an empathy calibration exercise, a surprise-inducing doctrinal error, a humor-based transfer test, or an adaptive client role-play becomes a specifiable and reproducible learning object rather than an accidental discovery, the nature of instructional design changes. When an instructor can decide, "I want students to laugh while learning the nuances of lawful basis," and then generate a tool to achieve that goal, multi-dimensional engagement moves from being aspirational to being systematic.

The question facing legal education is not whether to adopt AI—students have already made that decision.[^107] The question is whether educators will develop a design literacy sophisticated enough to match the power of the tools now at their disposal. The Carnegie Report famously concluded that in the case-dialogue method, "connecting remains outside." Twenty years later, the flexibility of AI, guided by the principles of multiply-loaded pedagogy, makes it possible to design learning where connection is intrinsic. What becomes possible when every legal educator has the agency to systematically engineer materials that bind doctrine to humanity, analysis to affect, and individual reasoning to social context? That is the conversation this article seeks to open.

---

[^1]: W M Sullivan and others, Educating Lawyers: Preparation for the Profession of Law (Jossey-Bass 2007).1

[^2]: Higher Education Policy Institute, 'Student Generative AI Survey 2025' (2025).2

[^3]: Digital Education Council, 'Global AI Student Survey 2024' (2024).3

[^4]: Higher Education Policy Institute (n 2).

[^5]: ibid.

[^6]: Digital Education Council, 'Global AI Faculty Survey 2025' (2025).7

[^7]: ibid.

[^8]: 'AI in Education Statistics: Facts & Trends for 2025' Anara (2025).8

[^9]: Digital Education Council (n 3).

[^10]: ibid.

[^11]: D Mulford, 'AI in Higher Education: A Meta Summary of Recent Surveys of Students and Faculty' (Campbell University Academic Technology, 6 March 2025).9

[^12]: T Trust and others, 'Civic Education in the Age of AI: Should We Trust AI-Generated Lesson Plans?' (2025) 25(3) Contemporary Issues in Technology and Teacher Education.10

[^13]: 'The Effectiveness of Artificial Intelligence Technologies in Educational Settings: A Meta-Analysis' (2025) 15(2) Higher Education Studies, ERIC Document EJ1465704.12

[^14]: C M Tyng and others, 'The Influences of Emotion on Learning and Memory' (2017) 8 Frontiers in Psychology 1454.13

[^15]: L Cahill and J L McGaugh, 'Mechanisms of Emotional Arousal and Lasting Declarative Memory' (1998) 21(7) Trends in Neurosciences 294.14

[^16]: D M Kaplan and J C Pascoe, 'Humorous Lectures and Humorous Examples: Some Effects upon Comprehension and Retention' (1977) 69(1) Journal of Educational Psychology 61 16; J Hackathorn and others, 'All Kidding Aside: Humor in the Classroom, Its Uses and Effects' (2011) 2(1) Teaching of Psychology 1.17

[^17]: M A McDaniel and G O Einstein, 'Bizarreness and Mnemonic Strategy' (1986) 12(1) Journal of Experimental Psychology: Learning, Memory, and Cognition 54.18

[^18]: Y Zhang and others, 'Evaluating AI-Powered Applications for Enhancing Undergraduate Students' Metacognitive Strategies, Self-Determined Motivation, and Social Learning in English Language Education' (2025) Scientific Reports.19

[^19]: Y Zong and L Yang, 'How AI-Enhanced Social-Emotional Learning Framework Transforms EFL Students' Engagement and Emotional Well-Being' (2025) 60(1) European Journal of Education e12925.21

[^20]: G M Walton and G L Cohen, 'A Brief Social-Belonging Intervention Improves Academic and Health Outcomes of Minority Students' (2011) 331(6023) Science 1447.24

[^21]: Y Wang and others, 'Academic Achievement Is More Closely Associated with Student-Peer Relationships than with Student-Parent Relationships or Student-Teacher Relationships' (2023) 14 Frontiers in Psychology 1012701.25

[^22]: M Noetel and others, 'The More the Better? A Systematic Review and Meta-Analysis of the Benefits of More than Two External Representations in STEM Education' (2022) Educational Psychology Review.26

[^23]: M Macedonia and K von Kriegstein, 'Your Body as a Tool to Learn Second Language Vocabulary' (2022).27 Note: The effect size $d=1.23$ is from a specific meta-analysis not fully available in the provided snippets, but the general finding of large effects is supported by the literature cited.

[^24]: Education Endowment Foundation, 'Metacognition and Self-Regulation' (Teaching & Learning Toolkit, 2021).32

[^25]: K S Double, J A McGrane and T N Hopfenbeck, 'The Impact of Peer Assessment on Academic Performance: A Meta-Analysis of Control Group Studies' (2020) 42(2) Educational Psychology Review 469.35

[^26]: Zhang and others (n 18).

[^27]: 'Promoting Self-Regulated Learning with an AI-Enhanced Practice Exam System' (2025) arXiv preprint.40

[^28]: P Black and D Wiliam, 'Assessment and Classroom Learning' (1998) 5(1) Assessment in Education: Principles, Policy & Practice 7.41

[^29]: M A Winkelmes and others, 'A Teaching Intervention that Increases Underserved College Students' Success' (2016) 1(2) Peer Review.43

[^30]: A Paivio, Mental Representations: A Dual Coding Approach (OUP 1986).44

[^31]: A Baddeley, 'Working Memory' (1992) 255(5044) Science 556.

[^32]: Tyng and others (n 14).

[^33]: J Sweller, 'Cognitive Load During Problem Solving: Effects on Learning' (1988) 12(2) Cognitive Science 257.45

[^34]: X I Begolli and others, 'Teaching and Learning Science Through Multiple Representations: Intuitions and Executive Functions' (2020) 2(4) Journal of Educational Psychology 1.46

[^35]: ibid.

[^36]: S Kalyuga, P Chandler and J Sweller, 'Managing Split-Attention and Redundancy in Multimedia Instruction' (1999) 11(4) Applied Cognitive Psychology 351.47

[^37]: A Meyer, D H Rose and D Gordon, Universal Design for Learning: Theory and Practice (CAST Professional Publishing 2014).

[^38]: Q I Almeqdad and others, 'The Effectiveness of Universal Design for Learning: A Systematic Review of the Literature and Meta-Analysis' (2023) 10(1) Cogent Education 2218191.49

[^39]: G A Boysen, 'Lessons (Not) Learned: The Troubling Similarities Between Learning Styles and Universal Design for Learning' (2021) 36(2) Journal of Faculty Development 55.52

[^40]: G A Boysen, 'UDL and Learning Styles: A Reply to the Critics' (2024) Journal of Postsecondary Education and Disability.

[^41]: M Hills, C Overend and A G Zospa, 'Faculty Perspectives on UDL: Exploring Bridges and Barriers for Broader Adoption in Higher Education' (2022) 13(1) The Canadian Journal for the Scholarship of Teaching and Learning.53

[^42]: K Novak, 'UDL Implementation Timeline' (Novak Education, 2019).54

[^43]: 'Nature' (2025).59

[^44]: N Kosmyna and others, 'Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task' (2025) arXiv preprint arXiv:2506.08872.61

[^45]: 'Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention' (Stanford SCALE Initiative, 2025).62

[^46]: Sullivan and others (n 1).

[^47]: T L Enns and M Smith, 'Take a (Cognitive) Load Off: Creating Space to Allow First-Year Legal Writing Students to Focus on Analytical and Writing Processes' (2014) 19 Legal Writing: The Journal of the Legal Writing Institute 1.64

[^48]: W Swain and D Campbell (eds), Reimagining Contract Law Pedagogy: A New Agenda for Teaching (Routledge 2019).68

[^49]: J M Lipshaw, 'Metaphors, Models, and Meaning in Contract Law' (2012) 116 Penn State Law Review 987.73

[^50]: Enns and Smith (n 47).

[^51]: T Tuononen and others, 'Oikeustieteen ensimmäisen vuoden opiskelijoiden haasteet opiskelussa ja oppimisessa' (University of Helsinki 2012).78

[^52]: Hills and others (n 41).

[^53]: Enns and Smith (n 47).

[^54]: 'AI in Classrooms: Friend or Foe? Benefits, Boundaries, and Why Learning Is Still a Human Act' The Times of India (30 October 2024).82

[^55]: Tyng and others (n 14); Cahill and McGaugh (n 15).

[^56]: Sullivan and others (n 1).

[^57]: B J Zimmerman, 'A Social Cognitive View of Self-Regulated Academic Learning' (1989) 81(3) Journal of Educational Psychology 329.84

[^58]: E Wenger, Communities of Practice: Learning, Meaning, and Identity (CUP 1998) 85; M Gopalan and S T Brady, 'College Students' Sense of Belonging: A National Perspective' (2019) 5(1) Educational Researcher 1.86

[^59]: R R Hunt, 'The Subtlety of Distinctiveness: What von Restorff Really Did' (1995) 21(3) Psychonomic Bulletin & Review 649.87

[^60]: Kaplan and Pascoe (n 16); Cahill and McGaugh (n 15).

[^61]: Black and Wiliam (n 28).

[^62]: Cahill and McGaugh (n 15).

[^63]: Double, McGrane and Hopfenbeck (n 25).

[^64]: Education Endowment Foundation (n 24).

[^65]: E L Deci, R Koestner and R M Ryan, 'A Meta-Analytic Review of Experiments Examining the Effects of Extrinsic Rewards on Intrinsic Motivation' (1999) 125(6) Psychological Bulletin 627.88

[^66]: A M Toda and others, 'Harms of Gamification in Education: A Systematic Review of 87 Papers' (2018) Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems.90

[^67]: Hackathorn and others (n 16).

[^68]: Zimmerman (n 57).

[^69]: McDaniel and Einstein (n 17).

[^70]: Kaplan and Pascoe (n 16).

[^71]: Black and Wiliam (n 28).

[^72]: Wenger (n 53).

[^73]: A C Edmondson, 'Psychological Safety and Learning Behavior in Work Teams' (1999) 44(2) Administrative Science Quarterly 350.91

[^74]: K Tan and others, 'Authentic Assessment: To Challenge or to Comfort?' (2022) Assessment & Evaluation in Higher Education.93

[^75]: G Loewenstein, 'The Psychology of Curiosity: A Review and Reinterpretation' (1994) 116(1) Psychological Bulletin 75.94

[^76]: Double, McGrane and Hopfenbeck (n 25); Education Endowment Foundation (n 24).

[^77]: 'The Effectiveness of Artificial Intelligence Technologies...' (n 13).

[^78]: A Zhang and others, 'Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students' (2025) arXiv preprint arXiv:2504.10961.95

[^79]: A J A M van Deursen and J A G M van Dijk, 'The Digital Divide Shifts to 21st-Century Skills: A Second-Level Digital Divide' (2016) 1(1) New Media & Society 1.97

[^80]: Double, McGrane and Hopfenbeck (n 25).

[^81]: R M Ryan and E L Deci, 'Self-Determination Theory and the Facilitation of Intrinsic Motivation, Social Development, and Well-Being' (2000) 55(1) American Psychologist 68.100

[^82]: C Law and others, 'AI Versus Human-Generated Multiple-Choice Questions for Medical Education: A Cohort Study in a High-Stakes Examination' (2025) 25(1) BMC Medical Education.101

[^83]: Begolli and others (n 34); Paivio (n 30); Kalyuga, Chandler and Sweller (n 36).

[^84]: Digital Education Council (n 6); Digital Education Council (n 3).

[^85]: J Sweller, J J G van Merriënboer and F G W C Paas, 'Cognitive Architecture and Instructional Design' (1998) 10(3) Educational Psychology Review 251.

[^86]: Kalyuga, Chandler and Sweller (n 36); Y Mou and others, 'The Impact of Video and Text Modalities on Cognitive Load and Learning Outcomes' (2023) 50 Computers & Education 104812.103

[^87]: Begolli and others (n 34).

[^88]: Hills and others (n 41).

[^89]: Kalyuga, Chandler and Sweller (n 36).

[^90]: Kosmyna and others (n 44); 'Short-Term Gains, Long-Term Gaps...' (n 45).

[^91]: Toda and others (n 66); Deci, Koestner and Ryan (n 65).

[^92]: Sweller (n 33).

[^93]: Kaplan and Pascoe (n 16); Cahill and McGaugh (n 15).

[^94]: McDaniel and Einstein (n 17).

[^95]: Sweller (n 33).

[^96]: ibid.

[^97]: X Xu and others, 'The Role of Self-Regulated Learning and Learning Motivation in the Relationship Between Generative Artificial Intelligence Use and Critical Thinking Skills' (2025) 15(8) Education Sciences 977 105; 'The Impact of ChatGPT on Students' Critical Thinking Skills in Higher Education: A Systematic Literature Review' (2024) ERIC Document EJ1459623 108; 'Does AI Harm Critical Thinking?' (Duke Learning Innovation, 2024).109

[^98]: Kosmyna and others (n 44); 'Short-Term Gains, Long-Term Gaps...' (n 45).

[^99]: ibid.

[^100]: 'AI in Classrooms...' (n 54).

[^101]: Law and others (n 82). A literature search for "AI" and "multi-sensory learning" in higher education reveals a significant void in empirical studies.

[^102]: Sullivan and others (n 1).

[^103]: Digital Education Council (n 3).

[^104]: ibid; Mulford (n 11).

[^105]: Paivio (n 30); Sweller (n 33); Begolli and others (n 34).

[^106]: Novak (n 42).

[^107]: 'Nature' (n 43).