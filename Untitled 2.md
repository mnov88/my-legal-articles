// draft 13, 4-Nov-25 [edited version] // Edits: EU-localized examples, standardized "multi-handle pedagogy" terminology, removed commercial brand names, strengthened transitions, eliminated defensive repetition, rewrote conclusion

# Beyond one-dimensional AI: multi-handle pedagogy in legal education

Legal education confronts a familiar tension. Student use of AI is now ubiquitous, yet faculty adoption remains hesitant and largely one‑dimensional—summaries, quizzes, and automated feedback. At the same time, the core pedagogical challenge endures: doctrinal teaching too often abstracts from human context, while resource constraints prevent the creation of materials that engage multiple dimensions of learning within large classes. Much of the AI discourse therefore misfires. It treats accuracy and citation reliability as the decisive criteria for educational value, even where pedagogical materials neither purport to state authoritative doctrine nor require verifiable references to perform their function.

This article advances a different premise. The distinctive contribution of AI to legal education is not efficient production of the same single‑channel materials, but practical enablement of integrated design. Multi‑handle pedagogy denotes intentional construction of single learning objects that engage emotional, sensory, social, metacognitive, and rational dimensions simultaneously. The approach does not claim novelty in the aim of "multiple pathways"; it departs operationally from Universal Design for Learning by integrating handles within one object rather than multiplying parallel alternatives, and by rejecting preference‑matching in favour of design principles grounded in cognitive load theory and dual coding. Put differently, the criterion for pedagogical quality here is whether materials elicit engagement, clarify doctrine, and support durable learning—not whether they satisfy standards appropriate to legal research.

The analysis proceeds in four moves. First, it reframes the apparent student–faculty "gap" as a problem of curricular leadership and alignment, not adoption parity. Second, it explains the accuracy trap—the category error that evaluates pedagogical content by research‑tool standards—and identifies use cases where accuracy is secondary to engagement, variation, and iteration. Third, it demonstrates four applications that operationalise integrated design across contract, data‑protection, assessment‑literacy, and tort contexts. Finally, it specifies the competencies required for design‑aware implementation, synthesising UDL's diagnostic lens with TPACK and AI‑literacy frameworks.

## The gap: student use outpaces faculty adoption

AI use among students jumped from 66% to 92% between 2024 and 2025 in the UK.[^5] Globally, 86% of students now use AI, with 54% using it weekly and 25% daily.[^59] Use specifically for assessments rose from 53% to 88% over the same period. Among US students, AI writing tool usage increased 82% in a single semester—from 27% in spring 2023 to 49% in fall 2023.[^6]

At the same time, faculty adoption lags substantially. While 45% of higher education faculty used AI tools in 2024 (up from 24% in 2023), institutional leaders estimate fewer than half of faculty use AI compared to estimates that at least half of students do so.[^7] This divergence matters: students use AI extensively for high-stakes assessment work whilst faculty deploy it primarily for low-stakes planning and administration.

Students overwhelmingly report using AI to save time and to obtain explanations when support is unavailable,[^5] and survey evidence shows they explicitly request faculty guidance and curricular training.[^61] The appropriate response is not adoption parity but curricular leadership: set norms, specify permissible use, and channel AI toward learning objectives. Law schools increasingly reflect this shift through courses and other curricular opportunities.[^9] In legal education specifically, recent survey evidence suggests law students may be less engaged than practising professionals, challenging simple "students ahead, faculty behind" narratives.[^65] Performance evidence likewise shows that mere tool use does not improve achievement; gains arise when students use AI both to understand and to produce.[^60] To that extent, the "gap" is misframed as a race to adopt tools. It is, instead, an alignment problem solved through design awareness and course‑level integration.

The gap reflects more than technological unfamiliarity. Shata and Hartley's study of 294 faculty found 33.6% opted out of using generative AI entirely, identifying five primary reasons: insufficient knowledge, no perceived value, conflicts with professional identity, concerns about replacing critical thinking, and broader negative societal consequences.[^10] Both users and non-users expressed academic integrity concerns, but non-users associated AI with categorical harm rather than tool-specific risks.

This resistance stems partly from quality expectations. When AI fails to produce accurate doctrine—and it often does—faculty reject the technology entirely. This rejection extends to applications where accuracy matters far less than other qualities. Understanding why requires examining what AI actually gets wrong and why this matters less than assumed for pedagogical content—a category error to which the analysis now turns.

## The accuracy trap: misapplied standards

Hallucinations constitute documented problems.[^64] Chelli and colleagues' systematic evaluation found hallucination rates reached 39.6% for ChatGPT-3.5, 28.6% for ChatGPT-4, and 91.4% for Bard.[^11] Walters and Wilder examined 636 bibliographic citations: 55% of GPT-3.5 citations were fabricated versus 18% for GPT-4.[^12] These accuracy problems make AI unsuitable for legal research. The _Mata v. Avianca_ case exemplified risks when a New York attorney submitted briefs citing non-existent cases with fabricated ECLI numbers.[^13] A tool that fabricates citations 18-55% of the time cannot support brief writing.

But legal education is not legal practice.

Academic integrity concerns extend beyond hallucinations. Universities report surges in AI‑related integrity complaints and persistent difficulty proving AI use under policies designed for plagiarism, with instructors estimating substantial student reliance on AI.[^62][^63] These patterns explain faculty resistance. When students submit AI‑generated work as their own, assessments measure nothing. Traditional detection methods fail because AI‑generated text contains no copied passages.

The concerns are legitimate. But they create perverse expectations. Faculty conclude that AI must produce correct, authoritative content to be valuable in education. When it fails this test, they reject the technology categorically. This rejection extends to applications where accuracy matters far less than other qualities.

Consider pedagogical hypotheticals. A contracts professor needs fact patterns to teach offer and acceptance. The traditional approach involves hours searching for cases with appropriate facts or crafting original hypotheticals that trigger emotional engagement whilst maintaining doctrinal accuracy. Does it matter if the case name is fabricated? Does it matter if the jurisdiction is fictional?

The pedagogical function requires factual variation illustrating doctrinal principles clearly. It benefits from emotional engagement activating memory encoding. It gains from cultural contexts resonating with diverse student populations. None of these functions requires citation-ready references.

AI excels at generating such content rapidly. Asked to create five variations of an offer-and-acceptance scenario with different emotional valences—sympathetic plaintiff, unsympathetic defendant, absurd facts, culturally specific contexts—AI produces options in seconds. The professor selects the most effective variation or combines elements. The fabrication that undermines legal research becomes pedagogically irrelevant. What matters is emotional engagement and doctrinal clarity, not citational precision.

The same principle applies to empathy development. A professor wants students to understand how identical legal principles apply differently based on factual contexts. AI generates doctrinally similar but factually different scenarios—contract formation involving small business owner versus large corporation, data privacy violations affecting minority community versus general population, tort claims by sympathetic versus unsympathetic plaintiffs. The goal is perspective-taking. Whether case names are real is pedagogically irrelevant.

Or consider peer evaluation practice. Students struggle with assessment literacy—understanding what distinguishes excellent from adequate from poor legal analysis. The professor wants students to practise evaluation skills before reviewing peer work, reducing social stakes whilst building competence. AI generates legal memoranda of varying quality. Students practise identifying strengths and weaknesses without judging classmates. Whether the AI-generated memo cites real cases is pedagogically irrelevant. What matters is whether the analysis demonstrates the targeted skill level accurately enough for students to calibrate their assessment.

The criterion for pedagogical quality here is not citational correctness but whether materials reliably elicit engagement, clarify doctrine, and support durable learning. To that extent, the analysis now turns to four applications that operationalise this standard in concrete classroom practice—examining how AI enables activation of multiple pedagogical handles simultaneously within single learning objects.

[^59]: Digital Education Council, Global AI Student Survey 2024 (covering 16 countries; 86% regular use) https://www.digitaleducationcouncil.com/post/what-students-want-key-results-from-dec-global-ai-student-survey-2024 [^60]: J R Freidhoff, AI in Education: Student Usage in Online Learning (Michigan Virtual Research Publications 2024) https://michiganvirtual.org/research/publications/ai-in-education-student-usage-in-online-learning/ [^61]: B Watwood, C Crawford and T A Dousay, 'Using Student Data to Bridge the AI Divide' (2024) EDUCAUSE Review https://er.educause.edu/articles/2024/4/using-student-data-to-bridge-the-ai-divide

## Multi-handle pedagogy: theory meets practice

Having established that pedagogical effectiveness depends less on doctrinal precision than on engagement across multiple dimensions, the analysis examines the theoretical foundations enabling multi-handle design. Legal education has understood this for decades but lacked resources to implement it systematically. Four pedagogical "handles"—emotional, sensory, social, and metacognitive—operate through well-established mechanisms. Before demonstrating how AI enables activating these handles simultaneously, each requires brief theoretical foundation.

**Emotional engagement.** Pekrun's Control-Value Theory identifies how achievement emotions interact dynamically with cognitive processes.[^15] Positive emotions promote attention, motivation, and flexible learning strategies. Neuroscience confirms these connections. Immordino-Yang and Damasio established that meaningful thinking and learning are inherently emotional—humans only think deeply about things they care about.[^16] LaBar and Cabeza demonstrated that the amygdala mediates emotional learning and facilitates memory operations, with emotional events attaining privileged memory status.[^17]

Legal education traditionally resists emotional engagement, valuing logic above affect. Yet clinical legal education has embraced empathy as necessary competence.[^18] The split reveals a resource problem. Clinical programmes with lower student-to-faculty ratios can address emotional dimensions through one-on-one supervision. Doctrinal courses with 80-100 students lack such capacity. Faculty understand that emotional engagement would enhance learning but finding or creating hypotheticals that trigger appropriate emotions for diverse students whilst maintaining doctrinal accuracy requires time that competing demands prevent.

**Sensory and multimodal pathways.** Dual coding theory posits that information processes through separate verbal and nonverbal channels.[^19] Presenting information in both formats engages distinct brain areas, enhancing comprehension and retention significantly. Mayer's multimedia learning theory, derived from extensive experimental studies, establishes that words and graphics together produce better learning than words alone.[^20] The contiguity principle requires aligning words to corresponding graphics. The modality principle recommends presenting words as audio narration rather than on-screen text. Cognitive load theory cautions that split‑attention and redundancy effects can harm learning when related text and graphics are separated or duplicated; effective integration mitigates these risks.[^70]

Legal education has employed visual aids sporadically. Shabiralyani and colleagues found 92% of students agreed visual aids provide direct experience, with correlation coefficient of 0.956 indicating visual aids explain 78.5% of variance in learning outcomes.[^21] Despite documented benefits, visual approaches remain underutilised. McLachlan and Webley found use of information visualisations in legal literature extremely rare.[^22] The underutilisation stems from resource constraints. Creating effective visual representations requires design skills faculty lack. Hiring designers exceeds most budgets. Software tools require learning curves.

**Social learning.** Vygotsky's social constructivism establishes that knowledge constructs through social interactions.[^23] The Zone of Proximal Development defines distance between independent problem-solving ability and potential development through collaboration. Meta-analysis found students in collaborative learning scored higher than 61% of students in individualistic or competitive settings.[^24] Legal education confronts social pressures complicating collaborative learning. Curved grading creates artificially scarce resources where "my good grade depends on my classmates getting bad grades."[^25] Lack of feedback exacerbates competition—comparing oneself to other students provides primary feedback during semesters. Brief belonging interventions have produced durable gains in university settings, including substantial reductions in racial achievement gaps and improved retention for socially disadvantaged students.[^66][^67]

Peer evaluation offers potential benefits but introduces social pressure. Ashley and Goldin found peer-generated review scores correlated with instructor grades, supporting peer review as useful feedback source.[^26] However, peer assessment raises concerns about competence, bias, favouritism, stress, and anxiety. Knowing peers will evaluate one's work creates performance anxiety distinct from instructor evaluation.

**Metacognitive development.** Metacognition—"thinking about thinking"—distinguishes between metacognitive knowledge and metacognitive regulation.[^27] Zimmerman developed a cyclical three-phase model of self-regulated learning: forethought involving task analysis, performance involving self-control and self-observation, and self-reflection involving self-judgment and self-reaction.[^28] The Education Endowment Foundation's meta-analysis showed metacognition provides high impact for low cost, with an additional seven months of progress, particularly pronounced for disadvantaged students.[^29]

Legal education has embraced reflective practice. Casey developed a comprehensive six-stage model for teaching reflective practice in law schools.[^30] Exam wrappers foster metacognitive self-assessment, helping students identify understanding gaps and supporting transitions from novice to expert reasoning.[^31] Despite these developments, metacognitive instruction remains resource-intensive. Providing individualised feedback on reflection requires reading lengthy journal entries. Faculty teaching large sections lack time for intensive engagement.

These four handles operate through well-established mechanisms. Educational psychology demonstrated their effectiveness decades ago. Legal education has implemented them sporadically where resources permit. But systematic implementation at scale has remained impractical. The resource investment required to create materials engaging multiple handles simultaneously exceeds what faculty can produce given competing demands.

AI changes this equation. The technology generates emotionally engaging scenarios in seconds. It produces visual representations from text descriptions. It creates content for social learning exercises without competitive pressure. It scaffolds metacognitive reflection through adaptive prompting. Most significantly, it can activate multiple handles within single generated objects—the integration that represents AI's distinctive pedagogical contribution. To that extent, the analysis now demonstrates this capacity through four concrete applications.

### Application one: humorous lecture summaries

A contracts professor assigns students to use AI to generate the funniest possible half‑page summary of offer and acceptance. Students must ensure doctrinal accuracy whilst maximising humour. They submit anonymously. The class votes on effectiveness. The professor discusses what makes certain summaries both accurate and memorable.

This exercise engages three handles concurrently—not sequentially, but simultaneously with each dimension reinforcing others.

**Emotional engagement** occurs through humour creation and reception. Creating humorous content requires understanding underlying concepts sufficiently to identify incongruities and exaggerate elements whilst maintaining accuracy. A student cannot make contract formation funny without grasping what makes it counterintuitive or fussy in practice. The cognitive demand of identifying which doctrinal elements permit humorous treatment forces deep engagement. Students must evaluate what matters most—perhaps the difference between an offer and an invitation to treat, perhaps why silence does not amount to acceptance, perhaps how a counter‑offer kills the original offer.

The humour itself triggers affective responses that enhance memory encoding. Reviews find that appropriate, content‑relevant humour helps students attend to material and can support persistence and creative engagement while avoiding off‑task distraction.[^33] The neurological mechanism operates through the same pathways that make emotional events attain privileged memory status. A student who creates or reads a humorous summary about offer and acceptance—say, "a shop window is not whispering 'take me'" or "silence is the loudest 'no' in contract law"—encodes doctrine through affective pathways unavailable in traditional case briefing.

**Metacognitive engagement** occurs through evaluation required for humour construction. Students must judge which concepts are fundamental versus peripheral. They must assess whether their humour clarifies or obscures. They must calibrate whether attempted jokes actually work—a metacognitive judgment requiring awareness of audience and pedagogical purpose. When students revise AI-generated drafts, they engage in iterative evaluation: Does this version capture the doctrine accurately? Does the humour enhance memorability or merely distract? Would my classmates understand this without lecture context?

This metacognitive process develops transferable skills. Students learn to evaluate their own understanding by testing whether they can explain concepts in novel ways. They practise self-assessment necessary for self-regulated learning—monitoring whether they grasp material sufficiently to manipulate it creatively whilst maintaining accuracy. The humour requirement prevents superficial paraphrasing. Students cannot simply restate definitions. They must reconstruct doctrine in ways that reveal deep understanding whilst achieving emotional impact.

**Social learning** activates when students share summaries and vote on effectiveness. The voting itself constitutes peer assessment, but with reduced stakes because the content is explicitly creative rather than formal academic work. Students develop assessment literacy—recognising what makes legal explanation both accurate and engaging—through evaluating multiple examples. The anonymity removes fear of judgment whilst preserving peer feedback benefits. Students see how classmates approached the same material differently, expanding their conceptual repertoires.

The shared laughter creates community. Legal education's competitive structures isolate students. An exercise where students collaborate in making doctrine memorable rather than competing for scarce high grades shifts social dynamics. The best summaries become shared resources. Students refer to humorous examples in later discussions, creating inside jokes that signal group membership. The humour humanises legal education whilst serving pedagogical functions.

**Implementation** requires minimal faculty time once designed. The professor provides the prompt: "Use AI to generate a half‑page summary of today's lecture on offer and acceptance. Make it as funny as possible whilst maintaining doctrinal accuracy. Submit anonymously by midnight." Students interact with AI, evaluating and revising outputs. The professor reviews submissions for doctrinal accuracy, selects five finalists, and facilitates voting and discussion. Total faculty time: perhaps forty‑five minutes for review and one class session for discussion.

Faculty lack time to create multiple humorous examples themselves. Students without AI support would struggle to generate humour that maintains doctrinal accuracy—the cognitive load of simultaneously being funny and correct exceeds many students' capacities, particularly for students whose first language is not English or who lack confidence in creative writing. AI scaffolds the humour generation, enabling students to focus metacognitive effort on evaluation and revision rather than initial creation. The exercise produces dozens of examples per class session; some fail, and the best ones enter the professor's teaching materials for future use. Over several semesters, the professor accumulates a library of student‑generated, AI‑assisted humorous examples covering major doctrines. To that extent, this exercise integrates emotional and metacognitive handles whilst preserving doctrinal control.

---

[^1]: Examples from institutional implementations documented in surveys discussed infra. [^2]: Richard E Mayer, Multimedia Learning (2nd edn, Cambridge University Press 2009). [^3]: Detailed examination follows in section 2. [^4]: Lawrence S Krieger and Kennon M Sheldon, 'What Makes Lawyers Happy?' (2015) 83 Geo Wash L Rev 554. [^5]: Higher Education Policy Institute, Student Generative AI Survey 2025 (3 March 2025) https://www.hepi.ac.uk/reports/student-generative-ai-survey-2025/ [^6]: Cengage Group, GenAI Report 2024 (2024) https://www.cengagegroup.com/news/press-releases/2024/ [^7]: Ellucian, AI in Higher Education Survey (2024) https://www.ellucian.com; Elon University and AAC&U, Survey of Higher Education Leaders (2024) https://www.aacu.org [^8]: Notre Dame Law School, 'Harvey AI Partnership Announcement' (2024) https://law.nd.edu [^9]: ABA Task Force on Law and Artificial Intelligence, 'AI and Legal Education Survey Results' (June 2024) https://www.americanbar.org/news/abanews/aba-news-archives/2024/06/aba-task-force-law-and-ai-survey/ [^10]: Doaa Shata and Ryan L Hartley, 'Generative AI in Higher Education' (2024) 32 International Journal of Educational Technology in Higher Education 1. [^11]: Shadi Chelli and others, 'Large Language Models in Generating Systematic Reviews: A Quality Assessment' (2024) 25 BMC Medical Research Methodology 112. [^12]: William H Walters and Esther Isabelle Wilder, 'Fabrication and Errors in the Bibliographic Citations Generated by ChatGPT' (2023) 13 Scientific Reports 14045. [^13]: Mata v Avianca Inc No 22-cv-1461 (PKC) (SDNY 2023). [^15]: Reinhard Pekrun, 'Control-Value Theory of Achievement Emotions' (2006) 18 Educational Psychology Review 315. [^16]: Mary Helen Immordino-Yang and Antonio Damasio, 'We Feel, Therefore We Learn' (2007) 1 Mind, Brain, and Education 3. [^17]: Kevin S LaBar and Roberto Cabeza, 'Cognitive Neuroscience of Emotional Memory' (2006) 7 Nature Reviews Neuroscience 54. [^18]: Fiona Gerdy, 'Clients, Empathy, and Compassion' (2008) 87 Nebraska Law Review 1. [^19]: Allan Paivio, Mental Representations: A Dual Coding Approach (Oxford University Press 1990). [^20]: Richard E Mayer, Multimedia Learning (2nd edn, Cambridge University Press 2009). [^21]: Ghulam Shabiralyani and others, 'Impact of Visual Aids' (2015) 6 J Education and Practice 226. [^22]: Daire McLachlan and Lisa Webley, 'Visualisations of Law' (2019) 26 International Journal of the Legal Profession 195. [^23]: Lev S Vygotsky, Mind in Society (Harvard University Press 1978). [^24]: David W Johnson and others, 'Cooperative Learning Returns to College' (1998) 30 Change 26. [^25]: Nancy Levit and Douglas O Linder, The Happy Lawyer (Oxford University Press 2010). [^26]: Kevin D Ashley and Ilya Goldin, 'Supporting Assess As You Go' (2011) 22 Journal of Law and Policy 759. [^27]: John H Flavell, 'Metacognition and Cognitive Monitoring' (1979) 34 American Psychologist 906. [^28]: Barry J Zimmerman, 'Self-Regulated Learning' (1990) 25 Educational Psychologist 3. [^29]: Education Endowment Foundation, 'Metacognition and Self-Regulation' (2023) https://educationendowmentfoundation.org.uk/education-evidence/teaching-learning-toolkit/metacognition-and-self-regulation [^30]: Timothy Casey, 'Reflective Practice in Legal Education' (2014) 20 Clinical Law Review 317. [^31]: Marsha C Lov