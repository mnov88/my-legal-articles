## STRATEGIC BOUNDARY CASES MATRIX: AI Processing × Administrative Contexts

| **AI USE ↓ / SCENARIO →**                                      | **FOI Requests** (deciding what to disclose)                                                                                                                                                                                                                                                                                                                                               | **Complaint Triage** (routing to departments)                                                                                                                                                                                                                                                                                                                               | **Benefit Eligibility Screening**(initial assessment before human review)                                                                                                                                                                                                                                                                                                                         | **Public Procurement Evaluation** (assessing bid submissions)                                                                                                                                                                                                                                                                                                                                            | **Regulatory Inspection Selection** (choosing entities to inspect)                                                                                                                                                                                                                                                                                                                                     |
| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Prioritization/Ranking**(reordering by predicted importance) | **"Decision"**: Is ordering a decision or information management?<br>**"Effects"**: Delayed disclosure ≠ denial, but timing affects usability<br>**"Solely automated"**: All eventually reviewed, but order matters<br>**RISK: LOW-MEDIUM**<br>_Critical if backlog means low-priority never processed_                                                                                    | **"Decision"**: Routing is procedural, not substantive determination<br>**"About whom"**: Decision about complaint handling, not complainant<br>**"Effects"**: Department choice affects speed/expertise but not outcome<br>**RISK: LOW**<br>_Unless some departments systematically dismiss complaints_                                                                    | **"Solely automated"**: Human reviews all, but order affects wait times for benefits<br>**"Effects"**: Priority = faster benefit receipt (significant economic effect)<br>**"Profiling"**: Likely uses applicant characteristics to prioritize urgent cases<br>**RISK: MEDIUM-HIGH**<br>_Delay in benefit access = tangible harm_                                                                 | **"Effects"**: Evaluation order shouldn't matter if all assessed, but cognitive bias (primacy/recency)<br>**"About whom"**: Bid evaluation vs. bidder evaluation<br>**"Solely automated"**: Humans review all bids eventually<br>**RISK: MEDIUM**<br>_High economic stakes make "similarly significant" easier to establish_                                                                             | **"Solely automated"**: Priority ranking informs inspection schedule but humans decide<br>**"Effects"**: Inspection timing affects compliance costs, reputational harm<br>**"Decision"**: Scheduling vs. selecting<br>**RISK: MEDIUM**<br>_Enforcement context heightens significance of any differentiation_                                                                                          |
| **Clustering/Categorization**(grouping inputs into topics)     | **"Decision"**: Categorization affects handling team, not disclosure determination<br>**"About whom"**: Grouping requests by topic, not requester characteristics<br>**"Effects"**: Category assignment doesn't determine outcome<br>**RISK: LOW**<br>_Unless categories route to teams with different disclosure rates_                                                                   | **"Decision"**: Topic clustering is organizational, not determinative<br>**"Effects"**: Department routing affects expertise but not rights<br>**"Profiling"**: Content-based, not person-based<br>**RISK: VERY LOW**<br>_Pure organizational efficiency, minimal impact on complainant_                                                                                    | **"Decision"**: Benefit category determines applicable rules and assessment pathway<br>**"Effects"**: Miscategorization = wrong rules applied = wrong outcome<br>**"Profiling"**: Uses application content + applicant circumstances<br>**RISK: HIGH**<br>_Automated categorization that determines legal framework = Art. 22 core concern_                                                       | **"Decision"**: Lot/category assignment may be dispositive if bid placed in wrong procurement category<br>**"Effects"**: Category determines evaluation criteria, budget limits<br>**"About whom"**: Categorizing offer vs. evaluating bidder<br>**RISK: HIGH**<br>_Miscategorization can render bid non-compliant automatically_                                                                        | **"Decision"**: Industry/risk category determines inspection regime, frequency, intensity<br>**"Effects"**: High-risk categorization = more frequent/invasive inspections (significant burden)<br>**"Profiling"**: Evaluates entity characteristics to assign risk category<br>**RISK: VERY HIGH**<br>_Risk categorization = paradigmatic profiling with legal consequences_                           |
| **Sentiment Analysis**(evaluating tone/position)               | **"Decision"**: Tone assessment doesn't determine legal obligation to disclose<br>**"Profiling"**: Evaluating requester attitude/intent (suspicious vs. legitimate)<br>**"Effects"**: Hostile requesters shouldn't be treated differently legally<br>**RISK: MEDIUM**<br>_Risk of discriminatory processing based on tone; GDPR fairness concerns_                                         | **"Decision"**: Complaint urgency/severity assessment based on emotional content<br>**"Profiling"**: Evaluating complainant's distress level from language<br>**"Effects"**: Emotional complaints prioritized = sentiment determines service level<br>**RISK: MEDIUM-HIGH**<br>_Sentiment = proxy for urgency, but systematically disadvantages unemotional complainants_   | **"Decision"**: Applicant attitude shouldn't affect eligibility determination<br>**"Profiling"**: Evaluating applicant characteristics from application tone<br>**"Effects"**: Neutral/formal applicants disadvantaged vs. emotional appeals<br>**RISK: HIGH**<br>_Irrelevant characteristic affecting benefit access = discrimination concern_                                                   | **"Decision"**: Bid sentiment (confidence, aggressiveness) irrelevant to technical/price criteria<br>**"About whom"**: Evaluating offer content, but sentiment reveals bidder characteristics<br>**"Effects"**: Tone affecting economic opportunities = significant<br>**RISK: VERY HIGH**<br>_Evaluation of bidder psychology rather than objective bid merits violates procurement principles_         | **"Decision"**: Entity attitude toward regulation (cooperative vs. resistant) informs inspection approach<br>**"Profiling"**: Evaluating compliance culture from communications<br>**"Effects"**: Antagonistic entities selected for more aggressive inspection<br>**RISK: VERY HIGH**<br>_Using attitude as enforcement criterion = potential rights violation (freedom of expression, fair process)_ |
| **Anomaly Detection**(flagging unusual submissions)            | **"Decision"**: Flagging unusual requests for special review (abuse detection)<br>**"Profiling"**: Pattern analysis of requester behavior (frequency, scope, timing)<br>**"Effects"**: Unusual requests face heightened scrutiny, potential delay/denial<br>**RISK: VERY HIGH**<br>_Investigative journalists, researchers = "anomalous" but legitimate users_                             | **"Decision"**: Anomalous complaints flagged for senior review or special handling<br>**"Effects"**: Flagging can mean enhanced attention (positive) or suspicion (negative)<br>**"Profiling"**: Comparing complainant to typical patterns<br>**RISK: MEDIUM**<br>_Dual nature: anomaly = urgency OR abuse; interpretation matters_                                         | **"Decision"**: Unusual applications flagged for fraud investigation<br>**"Profiling"**: Detecting "non-standard" applicant profiles<br>**"Effects"**: Fraud flagging = investigation, delay, potential criminal referral<br>**RISK: VERY HIGH**<br>_Classic profiling scenario; false positives = severe consequences; discriminatory impact on marginalized groups_                             | **"Decision"**: Anomalous bids flagged for collusion/fraud analysis<br>**"Effects"**: Suspicion flagging can lead to disqualification, investigation, blacklisting<br>**"Profiling"**: Pattern analysis across bidder history and bid characteristics<br>**RISK: VERY HIGH**<br>_Procurement fraud detection = enforcement; false positives destroy business opportunities_                              | **"Decision"**: Entities with unusual compliance patterns selected for investigation<br>**"Effects"**: Investigation triggers legal obligations, reputational harm, potential sanctions<br>**"Profiling"**: Behavioral pattern analysis = quintessential profiling<br>**RISK: CRITICAL**<br>_Enforcement selection based on algorithmic suspicion = Art. 22 paradigm case_                             |
| **Predictive Scoring**(assigning relevance/quality scores)     | **"Decision"**: Predicted disclosure likelihood doesn't determine legal obligation<br>**"Effects"**: Scores influence processing effort, lawyer review, redaction thoroughness<br>**"Solely automated"**: Humans make final disclosure decision based on law<br>**RISK: MEDIUM-HIGH**<br>_Risk that low-quality predictions become self-fulfilling through differential effort allocation_ | **"Decision"**: Predicted resolution difficulty/time affects routing and resource allocation<br>**"Effects"**: "Difficult" complaints get senior handlers vs. junior staff<br>**"Profiling"**: May use complainant characteristics to predict difficulty<br>**RISK: MEDIUM**<br>_Service level differentiation based on predicted characteristics rather than actual needs_ | **"Decision"**: Predicted eligibility likelihood guides assessment intensity<br>**"Effects"**: Low-score applicants face more skeptical review, higher documentation burdens<br>**"Profiling"**: Uses applicant characteristics to predict fraud/error risk<br>**RISK: CRITICAL**<br>_Differential scrutiny based on predicted risk = discriminatory administration; Art. 22 + equality concerns_ | **"Decision"**: Predicted bid quality scores directly inform evaluation outcome<br>**"Effects"**: Scores determine winner = direct economic consequences<br>**"Solely automated"**: If humans rely on scores without independent assessment = solely automated<br>**RISK: CRITICAL**<br>_Using AI quality scores in procurement evaluation = automated decision-making unless humans genuinely reassess_ | **"Decision"**: Predicted violation risk scores determine inspection selection<br>**"Effects"**: High scores = inspection with all attendant legal/economic consequences<br>**"Profiling"**: Risk prediction inherently evaluates entity characteristics<br>**RISK: CRITICAL**<br>_Risk scoring for enforcement selection = paradigmatic Art. 22 automated decision; well-established as problematic_  |

---

## DOCTRINAL PRESSURE POINTS REVEALED

### **1. THE "SOLELY AUTOMATED" ILLUSION**

**Highest Risk Cells:**

- Benefit Eligibility + Predictive Scoring
- Public Procurement + Predictive Scoring
- Regulatory Inspection + Predictive Scoring/Anomaly Detection

**Problem**: When AI assigns scores/flags that humans nominally review but practically defer to, is this "solely automated"? SCHUFA suggests meaningful human review must involve substantive reconsideration, not rubber-stamping. **Critical question**: What level of human engagement defeats "solely automated"?

**Case law gap**: No definitive CJEU guidance on degree of human deference that transforms "human-in-the-loop" into "automation by proxy."

---

### **2. THE PROCEDURAL vs. SUBSTANTIVE DISTINCTION**

**Highest Risk Cells:**

- Complaint Triage + Sentiment Analysis
- FOI Requests + Prioritization
- Any scenario involving routing/sequencing decisions

**Problem**: Article 22 protects against "decisions" producing legal/significant effects. Are procedural determinations (routing, sequencing, prioritization) "decisions" or merely administrative housekeeping?

**Functional reality**: Routing decisions can be **dispositive** if they:

- Direct cases to teams with different approval/denial rates
- Create delays that effectively deny time-sensitive rights
- Allocate resources differentially (expert vs. junior review)

**SCHUFA implications**: Court's emphasis on "determining role" in final outcome suggests procedural decisions can qualify if they effectively predetermine results.

---

### **3. THE PROFILING BOUNDARY**

**Highest Risk Cells:**

- Sentiment Analysis across ALL scenarios
- Anomaly Detection across ALL scenarios
- Clustering/Categorization in Benefit Eligibility and Regulatory Inspection

**Problem**: Article 4(4) defines profiling as evaluating "personal aspects" including "performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements."

**Key distinctions**:

- **Content-based processing** (clustering FOI requests by topic) vs. **person-based evaluation** (categorizing requesters)
- **Objective submission characteristics** (bid price, technical specifications) vs. **subjective bidder characteristics**(sentiment, predicted reliability)
- **Single-instance evaluation** vs. **pattern analysis across time/behavior**

**Critical insight**: Sentiment analysis and anomaly detection **inherently evaluate personal aspects** (emotional state, behavioral patterns) even when ostensibly evaluating submission content. This is profiling even if not explicitly person-focused.

---

### **4. THE "SIMILARLY SIGNIFICANT EFFECTS" FRONTIER**

**Highest Risk Cells:**

- Public Procurement + any AI use (economic stakes)
- Benefit Eligibility + Predictive Scoring (survival needs)
- Regulatory Inspection + Anomaly Detection/Predictive Scoring (enforcement consequences)

**Problem**: What makes effects "similarly significant" to legal effects?

**CJEU guidance (SCHUFA ¶¶ 57-62)**: Must "significantly affect the circumstances, behaviour or choices of the individuals concerned" or "produce legal effects."

**Three tiers emerging**:

**TIER 1 - Clearly Significant:**

- Benefit denial/delay (affects survival, housing, healthcare)
- Procurement disqualification (affects business viability, employment)
- Inspection selection (triggers legal obligations, sanctions risk)

**TIER 2 - Arguably Significant:**

- FOI delay (affects ability to exercise rights, pursue litigation, journalistic work)
- Complaint routing (affects resolution likelihood and timing)
- Priority service access (affects opportunity costs, stress)

**TIER 3 - Likely Insignificant:**

- Pure organizational categorization with no service differentiation
- Processing order when all items eventually receive identical treatment
- Metadata generation for internal management

**Critical gap**: No case law on **democratic participation effects** (consultation exclusion, legislative input filtering, policy feedback synthesis). Is influence on democratic processes "similarly significant"?

---

### **5. THE "ABOUT WHOM" PROBLEM**

**Highest Risk Cells:**

- Public Procurement + any evaluation of bids (bidder vs. bid)
- FOI Requests + any processing (request vs. requester)
- Regulatory Inspection + Clustering/Categorization (entity vs. sector)

**Problem**: Article 22 protects data subjects from automated decisions "about" them. When does processing of submissions/applications become evaluation of submitters/applicants?

**Functional test needed**:

- **Pure content evaluation**: Assessing bid technical specifications = not about bidder
- **Performance prediction**: Predicting bidder delivery reliability = about bidder
- **Hybrid evaluation**: Scoring bid quality using bidder history = about bidder through bid
- **Pattern analysis**: Detecting requester abuse patterns = explicitly about requester

**Procurement law insight**: Tender evaluation must assess **offers**, not **offerors** (unless qualification criteria explicitly permit). AI evaluation that considers bidder characteristics beyond qualification criteria violates procurement principles AND potentially Article 22.

---

## CROSS-CUTTING PATTERNS

### **Technology Increases Risk When:**

1. **Processing involves person-characteristics rather than submission-characteristics**
    
    - Sentiment analysis = inherently personal
    - Anomaly detection = inherently comparative (person vs. population)
    - Predictive scoring = inherently judgmental of predicted behavior
2. **Consequences are enforcement-adjacent**
    
    - Inspection selection, fraud investigation, abuse detection
    - Any outcome involving suspicion, scrutiny, or sanction triggers
3. **Economic/survival stakes are high**
    
    - Benefits, procurement, business licensing
    - Legal doctrine recognizes material impact more readily than dignitary/participatory harms
4. **Human review is perfunctory**
    
    - Volume overwhelming (thousands of cases)
    - Time pressure severe (seconds per case)
    - Expertise insufficient (reviewers don't understand AI methodology)

### **Article 22 Risk Increases When:**

1. **Multiple problematic elements converge**
    
    - Benefit Eligibility + Predictive Scoring = profiling + legal effects + automated decision
    - Regulatory Inspection + Anomaly Detection = profiling + legal effects + enforcement
2. **Procedural determination becomes substantively dispositive**
    
    - Routing to dismissive vs. receptive department
    - Prioritization creating prohibitive delays
    - Categorization determining applicable legal framework
3. **Information asymmetry prevents effective review**
    
    - Subjects unaware of AI use
    - Subjects cannot access scores/reasoning
    - Subjects cannot challenge without knowing basis

---

## STRATEGIC LITIGATION/POLICY PRIORITIES

### **IMMEDIATE REGULATORY ATTENTION NEEDED:**

**CRITICAL RISK (Article 22 almost certainly applies):**

1. **Benefit Eligibility + Predictive Scoring**: Automated fraud scoring that determines investigation intensity
2. **Regulatory Inspection + Predictive Scoring/Anomaly Detection**: Risk-based enforcement selection
3. **Public Procurement + Predictive Scoring**: AI quality scores influencing bid evaluation

**HIGH RISK (Strong Article 22 arguments):** 4. **Benefit Eligibility + Clustering/Categorization**: Automated benefit type determination 5. **Public Procurement + Anomaly Detection**: Collusion/fraud flagging 6. **Sentiment Analysis across enforcement contexts**: Using attitude as selection criterion

### **DOCTRINAL DEVELOPMENT NEEDED:**

**Courts must resolve:**

1. **"Solely automated" in scoring contexts**: When do predictive scores become dispositive despite human review?
2. **Procedural decisions**: Can routing, sequencing, prioritization produce "similarly significant effects"?
3. **Democratic participation effects**: Does consultation exclusion, policy feedback filtering constitute "significant effects"?
4. **Positive vs. negative differentiation**: Does Article 22 protect against automated prioritization for benefits/advantages?

### **ADMINISTRATIVE LAW INTERACTION:**

**Scenarios where admin law provides clearer protection:**

- FOI Requests: Transparency law obligations regardless of Article 22
- Complaint Triage: Investigation duties under administrative procedure law
- Public Procurement: Specialized procurement law on evaluation methodology

**Scenarios where Article 22 extends beyond admin law:**

- Sentiment Analysis: Admin law rarely prohibits tone-based differentiation explicitly
- Anomaly Detection: Pattern analysis may not violate admin law unless demonstrably discriminatory

---

## MATRIX INSIGHTS: COMPARATIVE RISK ASSESSMENT

### **HIGHEST RISK QUADRANT** (Article 22 violations most likely):

- Bottom-right: Predictive Scoring × Enforcement/Economic contexts
- Regulatory Inspection + Predictive Scoring = **10/10 risk**
- Benefit Eligibility + Predictive Scoring = **10/10 risk**
- Public Procurement + Predictive Scoring = **9/10 risk**

### **MODERATE RISK ZONE** (Significant Article 22 arguments):

- Middle cells: Sentiment/Anomaly × Benefits/Procurement
- Any Sentiment Analysis in consequential contexts = **6-8/10 risk**
- Anomaly Detection in enforcement = **8-9/10 risk**

### **LOW RISK ZONE** (Weak or no Article 22 claims):

- Top-left: Prioritization/Clustering × FOI/Complaints
- Pure organizational categorization = **2-4/10 risk**
- Requires showing actual impact on outcomes

### **DOCTRINAL UNCERTAINTY ZONE** (Need test cases):

- Diagonal: Where technology sophistication meets legal ambiguity
- Clustering in Benefit Eligibility = unclear if categorization = "decision"
- Prioritization in Procurement = unclear if timing = "significant effect"
- Sentiment Analysis in FOI = unclear if tone-based differentiation = "profiling"

**Your original hypothetical (consultation filtering = summarization + relevance scoring)** sits in the **moderate-high risk zone** with maximum doctrinal uncertainty—precisely why it's valuable for exposing Article 22's interpretive gaps.