## CRITICAL UNRESOLVED LEGAL QUESTIONS

### **1. "SOLELY AUTOMATED" WHEN HUMANS REVIEW AI OUTPUTS**

When human decision-makers review AI scores, summaries, or recommendations before deciding, what level of substantive engagement defeats "solely automated"? SCHUFA requires "new assessment" not "purely routine," but no clear standard exists for when humans meaningfully reassess versus rubber-stamp AI outputs. Practical question: If reviewing 10,000 AI-filtered consultations at 30 seconds each, is "human review" meaningful?

---

### **2. INFORMATION GATEKEEPING AS "DECISION"**

Does algorithmic filtering that controls what information reaches human decision-makers constitute an Article 22 "decision," even though humans make final determinations? SCHUFA's "determining role" test suggests information control can be decisive, but no case law addresses whether upstream filtering triggers Article 22 when downstream humans ostensibly decide. Core issue: Is deciding what someone sees functionally equivalent to deciding for them?

---

### **3. INDIRECT EFFECTS ON THIRD-PARTY DECISIONS**

When AI excludes a consultation submission affecting a third party's concession/license application, does the excluded data subject experience "legal effects or similarly significant effects"? No case law addresses whether procedural exclusion from decisions about others' rights constitutes Article 22-protected effects. Unresolved: Do participation rights qualify as "significant effects" or are they mere political interests outside Article 22 scope?

---

### **4. PROCEDURAL DETERMINATIONS AS "DECISIONS"**

Do routing, prioritization, or categorization decisions that don't directly determine outcomes but affect their likelihood qualify as Article 22 "decisions"? Examples: routing complaints to dismissive versus receptive departments, categorizing benefit applications determining which rules apply, prioritizing FOI requests creating effective denial through delay. Gap: No framework distinguishes preparatory housekeeping from determinative procedural acts.

---

### **5. PROFILING: CONTENT VERSUS PERSON EVALUATION**

When AI evaluates submission content (consultation arguments, bid quality, request scope), does this constitute profiling of "personal aspects" under Article 4(4)? Particular uncertainty around sentiment analysis (evaluating emotional tone) and anomaly detection (comparing to behavioral patterns). Practical problem: Nearly any evaluation of human-generated content reveals something about the person, but Article 22 cannot plausibly cover all content assessment.

---

### **6. ARTICLE 22(2)(b) SAFEGUARDS IN ADMINISTRATIVE CONTEXTS**

What satisfies "suitable measures to safeguard rights and freedoms" when Member State law authorizes automated administrative decisions? Do existing administrative procedure rights (investigation duties, reasoning requirements, appeals) suffice, or are additional data protection-specific safeguards required? No guidance exists on whether general administrative powers constitute sufficiently "clear and precise" legal authorization for AI use.