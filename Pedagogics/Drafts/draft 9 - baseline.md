// draft 9, 3-Nov-25 21:00
// current draft -- references not properly added, most can be found in draft8 or in older outline with good sources -- should likely be added.
# Beyond One-Dimensional AI: Multi-Handle Pedagogy in Legal Education

A contracts professor asks students to use AI to generate the funniest possible half-page summary of yesterday's lecture on consideration doctrine. The only rule: maintain doctrinal accuracy. Students work for ten minutes, submit anonymously, and vote on the most effective summaries. One winner depicts consideration as "the legal world's weirdest currency—where a peppercorn counts but love doesn't, and promising to stop playing bagpipes at 3am is apparently worth money." The class laughs. Then the professor asks: "Why will you remember consideration doctrine from this exercise?"

The answers reveal what happened pedagogically. The humor triggered emotional engagement—dopamine release that strengthens memory encoding. Creating humor required metacognitive evaluation—students couldn't make doctrine funny without understanding what makes it counterintuitive or absurd. Sharing summaries activated social learning—peer feedback calibrated judgment about what makes legal explanation both accurate and memorable. A single ten-minute exercise therefore engaged emotional, metacognitive, and social dimensions simultaneously.

This paper terms such pedagogical approach a 'multi-handle pedagogy': generating learning objects that engage multiple pedagogical dimensions concurrently within single activities — presents 

Without AI, creating even five humorous examples would have taken hours.

AI makes this scalable in ways that were theoretically understood but practically impossible given faculty resource constraints.

Legal educators currently deploy AI one-dimensionally—summarizing cases, generating quizzes, providing automated feedback.[^1] These applications save time but overlook the technology's distinctive capacity: rapid generation of content that activates emotional responses, sensory processing, social dynamics, and metacognitive awareness simultaneously. Educational psychology established decades ago that learning improves when students engage through multiple channels.[^2] The problem has never been knowledge. The problem has been implementation. Creating materials that trigger appropriate emotions whilst maintaining doctrinal accuracy requires hours per hypothetical. Developing visual representations demands design skills most faculty lack. Designing peer exercises that reduce competitive pressure whilst building skills involves elaborate scaffolding.

AI removes these constraints—but only when educators stop expecting it to function as authoritative source and start deploying it as pedagogical content generator.

Current discourse frames AI adoption through accuracy expectations. Hallucination rates and academic integrity violations dominate the literature.[^3] These concerns matter for legal research. But pedagogical materials serve different functions. Hypotheticals need emotional authenticity, not Bluebook citations. Scenarios for empathy development benefit from factual variation whilst maintaining doctrinal similarity—fabricated names pose no pedagogical problem. Peer evaluation practice improves when students identify flaws in AI-generated answers of varying quality.

The accuracy trap explains why faculty adoption lags student use. Educators expect AI to produce correct doctrine. When it fails, they reject the technology categorically—including for applications where accuracy matters less than engagement, variation, or rapid generation.

This analysis demonstrates how AI enables multi-handle pedagogy through four specific applications: humorous summaries engaging emotion and metacognition, varied scenarios triggering empathy through multiple sensory channels, AI-generated content for peer evaluation practice without social stakes, and emotional hypotheticals with graphics engaging dual coding pathways. Each activates multiple pedagogical dimensions simultaneously. None requires perfect doctrinal accuracy. Together they address documented challenges in legal education: competitive structures that isolate students, time constraints that prevent rich material development, limited feedback that leaves students comparing themselves to peers rather than to learning objectives.[^4]

The argument's stakes extend beyond technological adoption. Legal education can continue using AI for efficiency—summarizing longer into shorter, automating feedback—whilst missing the technology's distinctive contribution. Or it can reconceptualize AI as enabling pedagogical strategies that resource constraints prevented for decades. Multi-handle pedagogy represents this reconceptualization.

## The Gap: Student Use Outpaces Faculty Adoption

AI use among students jumped from 66% to 92% between 2024 and 2025 in the UK.[^5] Globally, 86% of students now use AI, with 54% using it weekly and 25% daily. Use specifically for assessments rose from 53% to 88% over the same period. Among US students, AI writing tool usage increased 82% in a single semester—from 27% in spring 2023 to 49% in fall 2023.[^6]

At the same time, the faculty adoption lags substantially. While 45% of higher education faculty used AI tools in 2024 (up from 24% in 2023), institutional leaders estimate fewer than half of faculty use AI compared to estimates that at least half of students do so.[^7] This divergence matters: students use AI extensively for high-stakes assessment work whilst faculty deploy it primarily for low-stakes planning and administration.

The gap reflects more than technological unfamiliarity. Shata and Hartley's study of 294 faculty found 33.6% opted out of using generative AI entirely, identifying five primary reasons: insufficient knowledge, no perceived value, conflicts with professional identity, concerns about replacing critical thinking, and broader negative societal consequences.[^10] Both users and non-users expressed academic integrity concerns, but non-users associated AI with categorical harm rather than tool-specific risks.

This resistance stems partly from quality expectations. When AI fails to produce accurate doctrine—and it often does—faculty reject the technology entirely. This rejection extends to applications where accuracy matters far less than other qualities. Understanding why requires examining what AI actually gets wrong and why it matters less than assumed for pedagogical content.

## The Accuracy Trap: Misapplied Standards

Hallucinations constitute documented problems. Chelli and colleagues' systematic evaluation found hallucination rates reached 39.6% for ChatGPT-3.5, 28.6% for ChatGPT-4, and 91.4% for Bard.[^11] Walters and Wilder examined 636 bibliographic citations: 55% of GPT-3.5 citations were fabricated versus 18% for GPT-4.[^12] These accuracy problems make AI unsuitable for legal research. The _Mata v. Avianca_ case exemplified risks when a New York attorney submitted briefs citing non-existent cases with fabricated ECLI numbers.[^13] A tool that fabricates citations 18-55% of the time cannot support brief writing.

But legal education is not legal practice.

Academic integrity concerns extend beyond hallucinations. AI cheating incidents increased nearly 400%—from 1.6 per 1,000 students in 2022-23 to 7.5 per 1,000 in 2024-25.[^14] Student discipline rates for AI-related plagiarism rose from 48% to 64% over the same period. These patterns explain faculty resistance. When students submit AI-generated work as their own, assessments measure nothing. Traditional detection methods fail because AI-generated text contains no copied passages.

The concerns are legitimate. But they create perverse expectations. Faculty conclude that AI must produce correct, authoritative content to be valuable in education. When it fails this test, they reject the technology categorically. This rejection extends to applications where accuracy matters far less than other qualities.

Consider pedagogical hypotheticals. A contracts professor needs fact patterns to teach offer and acceptance. The traditional approach involves hours searching for cases with appropriate facts or crafting original hypotheticals that trigger emotional engagement whilst maintaining doctrinal accuracy. Does it matter if the case name is fabricated? Does it matter if the jurisdiction is fictional?

The pedagogical function requires factual variation illustrating doctrinal principles clearly. It benefits from emotional engagement activating memory encoding. It gains from cultural contexts resonating with diverse student populations. None of these functions requires Bluebook-ready citations.

AI excels at generating such content rapidly. Asked to create five variations of an offer-and-acceptance scenario with different emotional valences—sympathetic plaintiff, unsympathetic defendant, absurd facts, culturally specific contexts—AI produces options in seconds. The professor selects the most effective variation or combines elements. The fabrication that undermines legal research becomes pedagogically irrelevant. What matters is emotional engagement and doctrinal clarity, not citational precision.

The same principle applies to empathy development. A professor wants students to understand how identical legal principles apply differently based on factual contexts. AI generates doctrinally similar but factually different scenarios—contract formation involving small business owner versus large corporation, data privacy violations affecting minority community versus general population, tort claims by sympathetic versus unsympathetic plaintiffs. The goal is perspective-taking. Whether case names are real is pedagogically irrelevant.

Or consider peer evaluation practice. Students struggle with assessment literacy—understanding what distinguishes excellent from adequate from poor legal analysis. The professor wants students to practice evaluation skills before reviewing peer work, reducing social stakes whilst building competence. AI generates legal memoranda of varying quality. Students practice identifying strengths and weaknesses without judging classmates. Whether the AI-generated memo cites real cases is pedagogically irrelevant. What matters is whether the analysis demonstrates the targeted skill level accurately enough for students to calibrate their assessment.

The accuracy trap operates through category error. Faculty evaluate AI's suitability for education using criteria appropriate for legal research—precision, citation accuracy, doctrinal correctness. But pedagogical materials serve different functions than research tools. They need engagement, variation, emotional resonance, multiple perspectives, and rapid generation. Traditional case method materials served both functions simultaneously—authoritative doctrine _and_ pedagogical hypotheticals. This alignment created false expectations for AI.

The technology can generate pedagogical scenarios engaging multiple learning dimensions without producing authoritative doctrine. This is not a bug. It is precisely the use case where AI's distinctive capacities shine—rapid generation of varied, emotionally engaging, contextually rich content that would take faculty hours to craft manually.

Understanding this distinction changes the calculus. AI becomes valuable not because it produces perfect doctrine, but because it generates pedagogical materials that engage students more deeply than faculty could produce manually given time constraints. The accuracy trap dissolves when educators stop expecting AI to serve as authoritative source and start deploying it as content generator for scaffolded learning.

## Multi-Handle Pedagogy: Theory Meets Practice

The accuracy trap dissolves when we recognize that pedagogical effectiveness depends less on doctrinal precision than on engagement across multiple dimensions. Legal education has understood this for decades but lacked resources to implement it systematically. Four pedagogical "handles"—emotional, sensory, social, and metacognitive—operate through well-established mechanisms. Before demonstrating how AI enables activating these handles simultaneously, each requires brief theoretical foundation.

**Emotional Engagement:** Pekrun's Control-Value Theory identifies how achievement emotions interact dynamically with cognitive processes.[^15] Positive emotions promote attention, motivation, and flexible learning strategies. Neuroscience confirms these connections. Immordino-Yang and Damasio established that meaningful thinking and learning are inherently emotional—humans only think deeply about things they care about.[^16] LaBar and Cabeza demonstrated that the amygdala mediates emotional learning and facilitates memory operations, with emotional events attaining privileged memory status.[^17]

Legal education traditionally resists emotional engagement, valuing logic above affect. Yet clinical legal education has embraced empathy as necessary competence.[^18] The split reveals a resource problem. Clinical programmes with lower student-to-faculty ratios can address emotional dimensions through one-on-one supervision. Doctrinal courses with 80-100 students lack such capacity. Faculty understand that emotional engagement would enhance learning but finding or creating hypotheticals that trigger appropriate emotions for diverse students whilst maintaining doctrinal accuracy requires time that competing demands prevent.

**Sensory and Multimodal Pathways:** Dual Coding Theory posits that information processes through separate verbal and nonverbal channels.[^19] Presenting information in both formats engages distinct brain areas, enhancing comprehension and retention significantly. Mayer's Multimedia Learning Theory, derived from over 200 experimental studies, establishes that words and graphics together produce better learning than words alone.[^20] The contiguity principle requires aligning words to corresponding graphics. The modality principle recommends presenting words as audio narration rather than on-screen text.

Legal education has employed visual aids sporadically. Shabiralyani and colleagues found 92% of students agreed visual aids provide direct experience, with correlation coefficient of 0.956 indicating visual aids explain 78.5% of variance in learning outcomes.[^21] Despite documented benefits, visual approaches remain underutilized. McLachlan and Webley found use of information visualizations in legal literature extremely rare.[^22] The underutilization stems from resource constraints. Creating effective visual representations requires design skills faculty lack. Hiring designers exceeds most budgets. Software tools require learning curves.

**Social Learning:** Vygotsky's social constructivism establishes that knowledge constructs through social interactions.[^23] The Zone of Proximal Development defines distance between independent problem-solving ability and potential development through collaboration. Meta-analysis found students in collaborative learning scored higher than 61% of students in individualistic or competitive settings.[^24] Legal education confronts social pressures complicating collaborative learning. Curved grading creates artificially scarce resources where "my good grade depends on my classmates getting bad grades."[^25] Lack of feedback exacerbates competition—comparing oneself to other students provides primary feedback during semesters.

Peer evaluation offers potential benefits but introduces social pressure. Ashley and Goldin found peer-generated review scores correlated with instructor grades, supporting peer review as useful feedback source.[^26] However, peer assessment raises concerns about competence, bias, favoritism, stress, and anxiety. Knowing peers will evaluate one's work creates performance anxiety distinct from instructor evaluation.

**Metacognitive Development:** Metacognition—"thinking about thinking"—distinguishes between metacognitive knowledge and metacognitive regulation.[^27] Zimmerman developed a cyclical three-phase model of self-regulated learning: forethought involving task analysis, performance involving self-control and self-observation, and self-reflection involving self-judgment and self-reaction.[^28] The Education Endowment Foundation's meta-analysis showed metacognition provides high impact for low cost, with an additional seven months of progress, particularly pronounced for disadvantaged students.[^29]

Legal education has embraced reflective practice. Casey developed a comprehensive six-stage model for teaching reflective practice in law schools.[^30] Exam wrappers foster metacognitive self-assessment, helping students identify understanding gaps and supporting transitions from novice to expert reasoning.[^31] Despite these developments, metacognitive instruction remains resource-intensive. Providing individualized feedback on reflection requires reading lengthy journal entries. Faculty teaching large sections lack time for intensive engagement.

These four handles operate through well-established mechanisms. Educational psychology demonstrated their effectiveness decades ago. Legal education has implemented them sporadically where resources permit. But systematic implementation at scale has remained impractical. The resource investment required to create materials engaging multiple handles simultaneously exceeds what faculty can produce given competing demands.

AI changes this equation. The technology generates emotionally engaging scenarios in seconds. It produces visual representations from text descriptions. It creates content for social learning exercises without competitive pressure. It scaffolds metacognitive reflection through adaptive prompting. Most significantly, it can activate multiple handles within single generated objects—the multi-loading that represents AI's distinctive pedagogical contribution.

### Application One: Humorous Lecture Summaries

Return to the opening example. A contracts professor assigns students to use AI to generate the funniest possible half-page summary of consideration doctrine. Students must ensure doctrinal accuracy whilst maximizing humor. They submit anonymously. The class votes on effectiveness. The professor discusses what makes certain summaries both accurate and memorable.

This exercise engages three handles concurrently—not sequentially, but simultaneously with each dimension reinforcing others.

**Emotional engagement** occurs through humor creation and reception. Creating humorous content requires understanding underlying concepts sufficiently to identify incongruities and exaggerate elements whilst maintaining accuracy. A student cannot make consideration doctrine funny without grasping what makes it tedious or counterintuitive. The cognitive demand of identifying which doctrinal elements permit humorous treatment forces deep engagement. Students must evaluate what matters most—perhaps the seeming arbitrariness of the peppercorn rule, perhaps the counterintuitive rejection of moral obligations as consideration, perhaps the baroque complexity of promissory estoppel exceptions.

The humor itself triggers dopamine release. James and Legg's review found appropriate humor helps students pay attention, leads to more work completion, and increases creativity.[^32] Banas and colleagues' review found humor in educational contexts activates reward systems and stimulates long-term memory.[^33] The neurological mechanism operates through the same pathways that make emotional events attain privileged memory status. A student who creates or reads a humorous summary about consideration doctrine involving absurd examples—contract for sale of pet rock, consideration consisting of refraining from playing bagpipes—encodes doctrine through affective pathways unavailable in traditional case briefing.

**Metacognitive engagement** occurs through evaluation required for humor construction. Students must judge which concepts are fundamental versus peripheral. They must assess whether their humor clarifies or obscures. They must calibrate whether attempted jokes actually work—a metacognitive judgment requiring awareness of audience and pedagogical purpose. When students revise AI-generated drafts, they engage in iterative evaluation: Does this version capture the doctrine accurately? Does the humor enhance memorability or merely distract? Would my classmates understand this without lecture context?

This metacognitive process develops transferable skills. Students learn to evaluate their own understanding by testing whether they can explain concepts in novel ways. They practice self-assessment necessary for self-regulated learning—monitoring whether they grasp material sufficiently to manipulate it creatively whilst maintaining accuracy. The humor requirement prevents superficial paraphrasing. Students cannot simply restate definitions. They must reconstruct doctrine in ways that reveal deep understanding whilst achieving emotional impact.

**Social learning** activates when students share summaries and vote on effectiveness. The voting itself constitutes peer assessment, but with reduced stakes because the content is explicitly creative rather than formal academic work. Students develop assessment literacy—recognizing what makes legal explanation both accurate and engaging—through evaluating multiple examples. The anonymity removes fear of judgment whilst preserving peer feedback benefits. Students see how classmates approached the same material differently, expanding their conceptual repertoires.

The shared laughter creates community. Legal education's competitive structures isolate students. An exercise where students collaborate in making doctrine memorable rather than competing for scarce high grades shifts social dynamics. The best summaries become shared resources. Students refer to humorous examples in later discussions, creating inside jokes that signal group membership. The humor humanizes legal education whilst serving pedagogical functions.

**Implementation** requires minimal faculty time once designed. The professor provides the prompt: "Use AI to generate a half-page summary of today's lecture on consideration doctrine. Make it as funny as possible whilst maintaining doctrinal accuracy. Submit anonymously by midnight." Students interact with AI, evaluating and revising outputs. The professor reviews submissions for doctrinal accuracy, selects five finalists, and facilitates voting and discussion. Total faculty time: perhaps forty-five minutes for review and one class session for discussion.

Without AI, this exercise would be impractical. Faculty lack time to create multiple humorous examples themselves. Students without AI support would struggle to generate humor that maintains doctrinal accuracy—the cognitive load of simultaneously being funny and correct exceeds many students' capacities, particularly for students whose first language is not English or who lack confidence in creative writing. AI scaffolds the humor generation, enabling students to focus metacognitive effort on evaluation and revision rather than initial creation.

This matters because legal education has long recognized that memorable examples improve learning but has lacked mechanisms to generate them at scale. The exercise produces dozens of examples per class session. Some fail. The best ones enter the professor's teaching materials for future use. Over several semesters, the professor accumulates a library of student-generated, AI-assisted humorous examples covering major doctrines. The resource investment is distributed across students rather than concentrated on faculty. The pedagogical benefit—memorable, emotionally engaging examples that activate metacognitive reflection and social learning—was theoretically available before AI but practically impossible given resource constraints.

---

[^1]: Examples from institutional implementations documented in surveys discussed infra. [^2]: Richard E Mayer, _Multimedia Learning_ (Cambridge University Press 2009); Michelene TH Chi and Ruth Wylie, 'The ICAP Framework' (2014) 49 Educational Psychologist 219. [^3]: Detailed examination follows in section 2. [^4]: Lawrence S Krieger and Kennon M Sheldon, 'What Makes Lawyers Happy?' (2015) 83 Geo Wash L Rev 554. [^5]: HEPI, 'Student Generative AI Survey 2025' (2025). [^6]: Cengage Group, 'GenAI Report 2024' (2024). [^7]: Ellucian, 'AI in Higher Education Survey' (2024); Elon University, 'Survey of Higher Education Leaders' (2024). [^8]: Notre Dame Law School, 'Harvey AI Partnership Announcement' (2024). [^9]: ABA Task Force on Law and Artificial Intelligence, 'Survey Results' (June 2024). [^10]: Doaa Shata and Ryan L Hartley, 'Generative AI in Higher Education' (2024) 32 Intl J Educational Technology in Higher Education 1. [^11]: Shadi Chelli and others, 'Large Language Models in Systematic Reviews' (2024) 25 BMC Medical Research Methodology 112. [^12]: William H Walters and Esther Isabelle Wilder, 'Fabrication in Bibliographic Citations' (2023) 13 Scientific Reports 14045. [^13]: _Mata v Avianca Inc_ No 22-cv-1461 (PKC) (SDNY 2023). [^14]: Statistics compiled from HEPI Survey 2025 and institutional reports. [^15]: Reinhard Pekrun, 'Control-Value Theory of Achievement Emotions' (2006) 18 Educational Psychology Review 315. [^16]: Mary Helen Immordino-Yang and Antonio Damasio, 'We Feel, Therefore We Learn' (2007) 1 Mind, Brain, and Education 3. [^17]: Kevin S LaBar and Roberto Cabeza, 'Cognitive Neuroscience of Emotional Memory' (2006) 7 Nature Reviews Neuroscience 54. [^18]: Fiona Gerdy, 'Clients, Empathy, and Compassion' (2008) 87 Nebraska Law Review 1. [^19]: Allan Paivio, _Mental Representations: A Dual Coding Approach_ (Oxford University Press 1990). [^20]: Mayer (n 2). [^21]: Ghulam Shabiralyani and others, 'Impact of Visual Aids' (2015) 6 J Education and Practice 226. [^22]: Daire McLachlan and Lisa Webley, 'Visualisations of Law' (2019) 26 Intl J Legal Profession 195. [^23]: Lev S Vygotsky, _Mind in Society_ (Harvard University Press 1978). [^24]: David W Johnson and others, 'Cooperative Learning Returns to College' (1998) 30 Change 26. [^25]: Nancy Levit and Douglas O Linder, _The Happy Lawyer_ (Oxford University Press 2010). [^26]: Kevin D Ashley and Ilya Goldin, 'Supporting Assess As You Go' (2011) 22 J Law and Policy 759. [^27]: John H Flavell, 'Metacognition and Cognitive Monitoring' (1979) 34 American Psychologist 906. [^28]: Barry J Zimmerman, 'Self-Regulated Learning' (1990) 25 Educational Psychologist 3. [^29]: Education Endowment Foundation, 'Metacognition and Self-Regulation' (2023). [^30]: Timothy Casey, 'Reflective Practice in Legal Education' (2014) 20 Clinical Law Review 317. [^31]: Marsha C Lovett, 'Make Exams Worth More Than the Grade' in Matthew Kaplan and others (eds), _Using Reflection and Metacognition_ (Stylus 2013). [^32]: James and Legg [citation to be confirmed]. [^33]: John A Banas and others, 'Review of Humor in Educational Settings' (2011) 20 Communication Education 115.

### Application Two: Doctrinally Similar, Factually Varied Scenarios

A criminal law professor uses AI to generate five variations of a Fourth Amendment search-and-seizure scenario. The doctrinal elements remain constant: whether police have reasonable suspicion for a Terry stop based on matching general description from radio call. The factual contexts vary to elicit different emotional responses: elderly white woman in affluent neighborhood, young Black man in low-income neighborhood, Middle Eastern person near airport, person with visible disabilities, well-dressed professional in business district. Students analyze whether the legal standard applies identically across contexts and why their intuitive reactions might differ from legal analysis.

This exercise engages emotional, sensory, and metacognitive handles simultaneously.

**Emotional engagement** occurs through empathy and discomfort. Students experience different affective responses to factually varied scenarios despite identical doctrinal structures. The variation forces recognition that identical legal principles apply differently based on social context and implicit biases. A student might feel the search of the elderly white woman seems unreasonable whilst the search of the young Black man seems justified—then confront the discomfort of recognizing this differential response reflects bias rather than legal reasoning. Alinezhad Noghabi and colleagues demonstrated AI-generated content triggers authentic emotional responses including discomfort and vulnerability necessary for socially just pedagogy.[^34]

The emotional authenticity does not require real cases with verified facts. A fabricated scenario describing a Terry stop can trigger genuine empathy if factual details resonate with students' experiences or cultural awareness. The pedagogical function is perspective-taking—understanding how identical legal doctrines operate differently across social contexts. Whether the scenario describes a real incident or AI-generated composite is pedagogically irrelevant. What matters is whether factual variation triggers authentic emotional responses enabling metacognitive reflection on how bias influences legal judgment.

**Sensory engagement** occurs when the professor pairs text scenarios with visual representations. AI can generate images depicting each scenario—visual representations of different individuals, different neighborhoods, different circumstances. Mayer's multimedia principle establishes that words and graphics together produce better learning than words alone.[^35] Visual representations activate dual coding pathways whilst reducing cognitive load through distributed processing across visual and verbal channels. A student reading about the elderly woman sees an image depicting an affluent neighborhood, triggering visual memory encoding alongside textual comprehension.

DALL-E 3, integrated with ChatGPT, generates images from text descriptions including diagrams and flowcharts.[^36] Gamma AI creates presentations incorporating AI-generated images alongside text.[^37] These tools enable faculty without graphic design skills to create professional-quality visual materials rapidly. The professor describes separation of powers and prompts AI to generate a flowchart showing relationships between branches. The generation takes seconds. The professor evaluates whether the visual representation clarifies the concept, adjusts through revised prompts, and integrates into teaching materials.

**Metacognitive engagement** occurs through comparative analysis across scenarios. Students must evaluate their own reasoning processes: Why did I find one search reasonable and another unreasonable when doctrinal elements are identical? What assumptions did I make about each individual based on demographic characteristics? How do my implicit biases influence legal analysis? This metacognitive reflection develops awareness of reasoning patterns and judgment biases—the foundation for self-regulated learning and professional ethical development.

The exercise also develops pattern recognition. Students see how identical doctrinal structures apply across varied factual contexts. This abstraction skill—recognizing constant legal elements despite varying facts—represents core competency in legal reasoning. AI enables generating the wide variety of factually different but doctrinally similar scenarios necessary to build this skill.

**Implementation** involves designing the initial prompt specifying desired variations and doctrinal constants. The professor prompts: "Generate five Fourth Amendment Terry stop scenarios. Each must involve identical legal elements: reasonable suspicion for stop based on matching general description from radio call. Vary the following factual elements: race, age, neighborhood affluence, appearance. Maintain identical doctrinal structure." AI generates five scenarios in seconds. The professor reviews for doctrinal accuracy, makes adjustments if necessary, and presents scenarios to students with accompanying questions about differential responses.

The Stanford M&A Negotiation Simulator exemplifies similar applications.[^38] Built through interviews with senior M&A partners, the tool uses generative AI to present negotiation scenarios with specific objectives and constraints. Students practice strategic thinking against different personality types in low-risk environments. Israeli law schools documented using Claude, ChatGPT-4, and Gemini Pro to generate criminal law case studies with solutions and practice chatbots simulating legal discussions.[^39] Studies of 2,141 students across five courses showed AI enhanced learning but required instructor oversight.

Faculty time investment is front-loaded into prompt design but minimal thereafter. Once the professor develops effective prompts, generating new scenario variations takes seconds. The professor can create fresh examples for each class, preventing students from accessing prior years' materials whilst maintaining doctrinal consistency. The capacity for rapid variation addresses a persistent problem: how to prevent academic integrity violations when the same hypotheticals circulate for years.

This matters because legal education has long emphasized that lawyers must understand how legal principles apply across varied contexts. The contextual case method pairs opinions with other perspectives, exploring complexities that went unaddressed.[^40] By humanizing opinions and encouraging students to "imagine a different legal world," this pedagogical approach benefits from AI's capacity to generate alternative contexts rapidly. Faculty could theoretically create such materials manually. The resource investment—hours per scenario set, design skills for visual representations—prevented systematic implementation. AI generates varied content in seconds, removing time constraints whilst preserving faculty judgment about learning objectives and pedagogical appropriateness.

### Application Three: AI-Generated Answers for Peer Evaluation Practice

A legal writing professor assigns students to evaluate three legal memoranda addressing the same issue. The memoranda vary in quality: one excellent with clear IRAC structure and effective use of authority, one adequate but with organizational problems, one poor with conclusory analysis and citation errors. Students do not know the memoranda are AI-generated. Working in pairs, students complete evaluation rubrics assessing legal analysis, use of authority, writing clarity, and citation format. The professor then reveals the memoranda were AI-generated, discusses evaluation rubrics, and assigns peer evaluation of actual student work.

This exercise engages social and metacognitive handles whilst eliminating competitive pressure.

**Social learning** occurs through collaborative evaluation. Working in pairs, students discuss what makes legal analysis effective. They must articulate criteria for distinguishing excellent from adequate from poor work—a metacognitive process requiring explicit awareness of assessment standards. The collaboration enables legitimate peripheral participation in communities of practice.[^41] Students new to legal writing observe how peers approach evaluation, learning norms and strategies through social interaction. The partnership creates scaffolding—less experienced students learn from more experienced peers, whilst explaining evaluation criteria to partners reinforces the more experienced student's understanding.

The AI-generated content removes social stakes. Students practice evaluation skills without risking peer relationships or experiencing judgment anxiety. They can provide harsh but accurate feedback on poor memoranda without fearing they are insulting classmates. They can disagree with partners about quality assessments without interpersonal consequences. Double-blind protocols in peer assessment prevent identity disclosure between creators and assessors, removing fear of social comparison whilst eliminating interpersonal politics from evaluation.[^42] AI-generated content provides similar benefits whilst enabling practice before consequential peer review.

**Metacognitive engagement** occurs through developing assessment literacy. Students must make explicit their criteria for quality. What distinguishes clear writing from unclear writing? How much authority is sufficient? When does citation format matter versus when are minor errors acceptable? These evaluative judgments require metacognitive awareness—understanding not just what one thinks is good but why, based on what standards, applied through what reasoning process.

Topping and colleagues' systematic review of 79 papers found AI improved peer assessment across multiple dimensions.[^43] The EvaluMate system powered by ChatGPT facilitates peer review by offering feedback on student reviewers' comments, scaffolding their learning. Guo's study of 124 Chinese undergraduate students found AI-supported peer feedback significantly enhanced feedback quality and writing ability.[^44] The AI chatbot "Eva" provided feedback on student reviewers' comments, creating learning cycles where students developed metacognitive awareness of what constitutes effective feedback.

Lu and colleagues' comparison of AI, peer, and instructor assessment found AI provided higher-quality feedback than peers with more detailed and specific comments.[^45] The study emphasizes complementary strengths rather than replacement. Students can practice peer evaluation on AI-generated work of varying quality, learning to identify strengths and weaknesses, provide constructive feedback, and calibrate judgments against instructor standards. This practice occurs without risking peer relationships or experiencing judgment anxiety.

**Implementation** requires the professor to generate AI memoranda of specified quality levels. The prompt specifies: "Generate a legal memorandum analyzing [specific issue]. Make the analysis conclusory with weak use of authority and citation errors." The professor reviews the generated memorandum, adjusts if necessary to ensure it exhibits targeted weaknesses, and repeats for different quality levels. Once generated, the memoranda can be reused across semesters unless the legal issue changes. Total faculty time: perhaps two hours to generate and refine three memoranda initially, with no additional time in subsequent semesters.

The exercise prepares students for consequential peer review. After practicing evaluation on AI-generated work, students possess calibrated judgment about quality standards. They have experience articulating constructive feedback. They understand how to use rubrics. The transition to peer review of actual student work involves less anxiety because students have developed competence in low-stakes environments.

This matters because peer assessment improves learning when properly executed but creates social pressures that inhibit effectiveness.[^46] Students worry about being too harsh or too lenient. They fear damaging friendships or creating enemies. They lack confidence in their evaluative judgments. These social and metacognitive barriers prevent effective peer assessment. AI-generated practice materials remove social barriers whilst building metacognitive competence. The result is peer assessment that achieves learning benefits whilst reducing stress and anxiety—a practical solution to legal education's competitive structures that isolate students and create comparison behaviors.

### Application Four: Emotional Hypotheticals with Graphics

A torts professor uses AI to generate a negligence hypothetical involving a sympathetic plaintiff—a child injured on defective playground equipment. The professor prompts AI to create visual representations: a diagram showing physical layout of the accident scene, a timeline showing sequence of events, images depicting the type of harm suffered. Students analyze negligence elements whilst processing both textual and visual information. The professor then generates variations with less sympathetic plaintiffs to examine how emotional responses influence legal analysis.

This exercise engages emotional and sensory handles simultaneously whilst supporting metacognitive reflection.

**Emotional engagement** occurs through topic emotions triggered by the sympathetic plaintiff. Students care about preventing harm to children. This emotional response activates neural pathways that enhance memory encoding. LaBar and Cabeza demonstrated that emotional events attain privileged status in memory with greater retention advantages.[^47] A student analyzing a negligence case involving a sympathetic plaintiff encodes the doctrinal elements—duty, breach, causation, damages—through affective pathways that strengthen retention.

The emotional content need not involve real cases. A fabricated scenario about a child injured by defective playground equipment triggers genuine sympathy if factual details resonate emotionally. The pedagogical function is memory encoding and motivational engagement. Whether the child actually exists is pedagogically irrelevant. What matters is whether the scenario triggers emotional responses that activate neural mechanisms supporting learning.

**Sensory engagement** occurs through visual representations paired with text. Dual coding theory establishes that information processing through verbal and nonverbal channels enhances comprehension and retention.[^48] A diagram showing playground layout enables students to visualize spatial relationships that text alone cannot convey efficiently. A timeline showing sequence of events distributes cognitive load across visual and verbal channels rather than requiring students to construct mental timelines from text alone. An image depicting the harm suffered activates visual memory encoding alongside textual comprehension.

Mayer's contiguity principle requires aligning words to corresponding graphics for maximum effectiveness.[^49] The professor structures presentation so that textual description of the accident scene appears adjacent to the visual diagram. Discussion of temporal sequence coincides with timeline display. This alignment reduces cognitive load by eliminating the need for students to search between separated text and graphics whilst integrating information.

Educational design research found positive emotional design using vivid colors and anthropomorphic features outperformed neutral monochromatic designs on retention and transfer tests.[^50] Making essential elements visually appealing "initiates and guides cognitive processing during learning." AI-generated graphics can incorporate design principles that enhance emotional and cognitive engagement simultaneously—using color to highlight key elements, using visual metaphors to clarify abstract concepts, using spatial arrangement to show relationships.

**Metacognitive reflection** occurs when the professor presents variations with less sympathetic plaintiffs. Students compare their emotional and analytical responses across scenarios. Why did the sympathetic plaintiff case seem like clear negligence whilst the unsympathetic plaintiff case seemed like contributory negligence might apply? How do emotional responses influence application of legal standards? This comparative analysis develops metacognitive awareness of how affect influences judgment—a skill necessary for professional ethical practice.

**Implementation** involves designing prompts specifying desired emotional content and visual elements. The professor prompts: "Generate a negligence hypothetical involving a child injured on defective playground equipment. Include sympathetic details about the child's age and vulnerability. Create three visual elements: a diagram of the playground layout showing the defective equipment, a timeline of events leading to the injury, an image depicting the type of harm suffered." AI generates text and images in seconds. The professor reviews for doctrinal accuracy and emotional appropriateness, makes adjustments if necessary, and integrates into teaching materials.

NotebookLM generates audio overviews as podcasts and video overviews with narration, enabling multimodal content creation from single sources.[^51] These tools enable faculty without graphic design skills to create professional-quality materials rapidly. The combination of emotional engagement and sensory processing creates synergistic effects where affective and visual encoding reinforce each other, producing stronger memory traces than either alone.

This matters because legal education has understood that visual representations clarify complex doctrine but has lacked resources to create them systematically. Faculty teaching large sections cannot spend hours designing graphics for each hypothetical. AI removes this barrier whilst preserving pedagogical judgment about which concepts benefit from visual representation and what emotional valence serves learning objectives.

---

These four applications demonstrate multi-loading in practice. Each activates multiple pedagogical handles simultaneously. The humorous summaries engage emotion through dopamine release, metacognition through evaluation of humor effectiveness, and social learning through shared voting. The varied scenarios engage emotion through empathy and discomfort, sensory processing through visual representations, and metacognition through comparative analysis. The peer evaluation practice engages social learning through collaborative assessment and metacognition through developing assessment literacy whilst eliminating competitive pressure. The emotional hypotheticals with graphics engage emotion through sympathetic content and sensory processing through dual coding whilst supporting metacognitive reflection on how affect influences judgment.

None of these applications requires perfect doctrinal accuracy in every detail. Fabricated case names, fictional scenarios, and AI-generated graphics serve pedagogical functions effectively regardless of whether they correspond to real cases or real images. What matters is whether they engage multiple handles simultaneously in ways that enhance learning—a question of pedagogical design, not citational precision.

The resource investment is minimal compared to manual creation. Faculty design prompts, evaluate AI outputs for appropriateness, and facilitate exercises. The content generation itself takes seconds. The pedagogical benefit—materials engaging multiple dimensions simultaneously—was theoretically available before AI but practically impossible given time constraints.

## Competency Requirements: Frameworks for Effective Implementation

Multi-loading does not occur automatically. Simply using AI tools does not guarantee generated content engages multiple pedagogical dimensions effectively. A professor can prompt AI to create a hypothetical, receive output in seconds, and present it to students—yet achieve only superficial engagement if the exercise lacks intentional design. The difference between multi-loading and mere tech-use depends on faculty competencies in three areas: pedagogical frameworks guiding intentional design, AI literacy enabling effective prompting and evaluation, and metacognitive awareness of when multi-dimensional engagement serves learning objectives.

**Universal Design for Learning** provides structures for multi-loading design.[^52] UDL establishes three principles aligning with multi-handle pedagogy: multiple means of engagement addressing the affective network, multiple means of representation addressing varied sensory modalities, and multiple means of action and expression offering alternatives for demonstrating learning. These principles map directly onto multi-loading applications. The humorous summary exercise provides multiple means of engagement through emotional humor whilst offering multiple means of expression through creative writing. The varied scenarios provide multiple means of representation through visual and textual information whilst engaging affectively through empathy.

UDL guides decisions about which handles to activate for specific learning objectives. A professor designing a contracts exercise must ask: What barriers prevent students from engaging with consideration doctrine? Perhaps abstract concepts need visual representation. Perhaps lack of emotional engagement prevents memory encoding. Perhaps competitive pressure inhibits collaborative discussion. UDL principles help diagnose barriers and select appropriate interventions.

**Technological Pedagogical Content Knowledge** provides frameworks for integrating AI into multi-loading design.[^53] TPACK integrates three knowledge bases—understanding subject matter, knowing effective teaching methods, and recognizing how technology transforms both content and pedagogy. This prevents technology-driven rather than pedagogy-driven decisions. A professor should not use AI to generate visual representations because the technology can produce images rapidly. The professor should identify that visual representations would clarify spatial relationships in property doctrine that text alone cannot convey efficiently, then deploy AI to generate appropriate visuals.

The humorous summary exercise demonstrates TPACK integration. Content knowledge: understanding consideration doctrine sufficiently to evaluate whether student-generated humor maintains accuracy. Pedagogical knowledge: recognizing that humor engages emotional and metacognitive handles whilst reducing competitive pressure. Technology knowledge: understanding that AI can generate varied humorous examples rapidly whilst requiring human evaluation of appropriateness. The integration: designing prompts that scaffold student creation of content engaging multiple handles, evaluating outputs for doctrinal accuracy, and facilitating discussion that develops metacognitive awareness.

**AI Literacy** extends beyond technical proficiency. Long and Magerko defined AI literacy as abilities to critically evaluate AI, communicate with AI, and utilize AI as tools.[^54] Digital Promise's 2024 framework articulated three modes: UNDERSTAND acquiring basic AI knowledge, EVALUATE centering human judgment to critically consider benefits and costs, and USE encompassing interact, create, and problem-solve applications.[^55] These frameworks emphasize human judgment as the critical competency. AI generates outputs rapidly. Faculty must evaluate whether outputs achieve pedagogical purposes.

The peer evaluation exercise demonstrates AI literacy requirements. The professor must understand that AI can generate legal memoranda of specified quality levels—the technological capability. The professor must evaluate whether AI-generated memoranda exhibit weaknesses students need practice identifying—the pedagogical alignment. The professor must consider whether revealing that memoranda are AI-generated enhances or undermines the exercise—the ethical dimension. The professor must judge whether practicing on AI-generated content genuinely prepares students for peer review or merely creates an illusion of preparation—the critical evaluation.

**Faculty Development** must address these competencies systematically. Mah and Groß studied 122 faculty from German institutions, identifying four profiles: Optimistic (33.5%), Critical (27.3%), Critically Reflected (33.9%), and Neutral (5.3%).[^56] The Critically Reflected profile—agreeing with both benefits and challenges—represents the stance appropriate for multi-loading design. Faculty should recognize AI's capacity to generate content engaging multiple handles whilst remaining aware of limitations requiring human oversight.

Professional development must move beyond tool tutorials. Faculty need practice designing prompts that specify pedagogical properties, not just content topics. Effective development teaches faculty to analyze learning objectives, identify which handles would enhance achievement, design prompts specifying desired properties, and evaluate outputs for pedagogical appropriateness.

A faculty development workshop might present a learning objective: students should understand how identical legal doctrines apply across varied factual contexts whilst recognizing how implicit biases influence legal judgment. Participants practice designing prompts specifying doctrinal elements that must remain constant, factual elements that should vary, emotional valences that should differ, and visual representations that would clarify relationships. Participants generate scenarios using AI, exchange scenarios with colleagues, evaluate whether scenarios achieve stated objectives, and revise prompts based on feedback.

This active practice develops competencies that passive tool demonstrations cannot. Faculty learn through doing—the same pedagogical principle that guides effective student learning. Usage patterns show faculty interest: 78.5% expressed interest in teaching and learning with AI tools, 66.4% in AI-based tools training.[^57] Preferred formats emphasized self-paced online courses with 5-20 hours time investment.

Development should offer flexible pathways. Faculty with high AI literacy may need only advanced workshops on multi-loading design. Faculty new to AI need foundational understanding of capabilities and limitations before attempting complex applications. Discipline-specific development addresses the reality that contracts professors need different examples than constitutional law professors, even though underlying pedagogical principles remain constant.

Competency development never ends. AI capabilities evolve rapidly. GPT-4 generates more accurate citations than GPT-3.5, though still with substantial error rates. Future models may improve further or develop new capabilities. Faculty must maintain awareness of changing capacities whilst preserving core pedagogical judgment. The question is not whether a specific AI version can perform a task. The question is whether using AI to perform that task enhances learning through multi-handle engagement.

The competency requirements are substantial but not insurmountable. Faculty already possess deep content knowledge and pedagogical expertise from years of teaching. Multi-loading adds technological competency and systematic frameworks for multi-dimensional design. Development programs addressing these needs enable faculty to deploy AI effectively whilst avoiding superficial tech-adoption that characterizes failed educational technology initiatives.

## Conclusion: Implementation as Opportunity

AI offers legal education a choice. The field can continue deploying the technology one-dimensionally—summarizing cases, generating quizzes, automating feedback—treating AI as efficiency tool for tasks faculty already perform. Or it can recognize AI's distinctive capacity: enabling multi-handle pedagogy that activates emotional, sensory, social, and metacognitive dimensions simultaneously within single learning objects. This reconceptualization addresses resource constraints that have prevented systematic implementation of pedagogical strategies educational psychology validated decades ago.

The evidence base is clear. Emotional engagement activates neural pathways enhancing memory encoding. Multiple sensory modalities distribute cognitive load whilst increasing retrieval pathways. Social learning leverages communities of practice. Metacognitive development builds self-regulated learning capacities. These dimensions function synergistically—emotional responses enhance memory whilst metacognitive reflection deepens understanding whilst social interaction provides feedback. Legal education has implemented these handles sporadically where resources permit. Clinical programmes with favorable faculty ratios address emotional and social dimensions. Technology-enhanced courses employ multimodal materials. Reflective practice initiatives build metacognitive awareness. But systematic implementation at scale has remained impractical given time constraints and competing demands.

AI removes the barrier. The four applications demonstrated this concretely. Students using AI to generate humorous summaries engage multiple handles in ten minutes—an exercise requiring hours to create manually. AI generating doctrinally similar but factually varied scenarios enables perspective-taking exercises that build pattern recognition whilst triggering empathy. AI-generated content for peer evaluation practice develops assessment literacy without the competitive pressure that inhibits collaborative learning in curved-grading environments. Emotional hypotheticals paired with AI-generated graphics engage dual coding pathways whilst activating affective memory encoding.

Each application required minimal faculty time—designing prompts, evaluating outputs, facilitating exercises. The content generation itself took seconds. The pedagogical benefit—materials engaging multiple dimensions simultaneously—was theoretically available before AI but practically impossible given resource investment required.

The accuracy trap explains resistance. Faculty expect AI to produce authoritative doctrine. When it hallucinates citations or fabricates cases, they reject the technology categorically. This rejection makes sense for legal research applications. Extended to all educational uses, it blocks recognition of AI's pedagogical potential. Hypotheticals need emotional authenticity and factual variation, not Bluebook precision. Scenarios for empathy development need doctrinal similarity across varied contexts, not verifiable case names. Peer evaluation practice benefits from AI-generated answers of varying quality because students develop assessment skills through identifying flaws.

The distinction matters. AI functions best as content generator for pedagogical scaffolding, not as authoritative source for doctrinal answers. Understanding this changes the calculus. The technology becomes valuable not because it produces perfect doctrine, but because it generates pedagogical materials engaging students more deeply than faculty could produce manually given time constraints.

Implementation requires competencies beyond technical proficiency. Universal Design for Learning and Technological Pedagogical Content Knowledge provide frameworks for intentional design. AI literacy encompasses critical evaluation, effective prompting, and ethical awareness. Faculty development must address these dimensions through hands-on practice rather than passive tool demonstrations. The competency requirements are substantial but not insurmountable—faculty already possess deep content knowledge and pedagogical expertise. Multi-loading adds technological competency and systematic frameworks for multi-dimensional design.

The transformation from one-dimensional to multi-dimensional AI use represents opportunity with broader implications. Legal education faces documented challenges: competitive structures that isolate students, time constraints that prevent rich material development, limited feedback that leaves students comparing themselves to peers rather than learning objectives, emotional suppression in doctrinal courses that hinders memory encoding.[^58] Multi-handle pedagogy addresses these challenges not by eliminating them entirely—curved grading and large class sizes persist—but by creating opportunities for engagement that resource constraints previously prevented.

Consider what becomes possible. A constitutional law professor generates varied scenarios illustrating separation of powers across different political contexts, triggering emotional responses whilst building pattern recognition. A contracts professor accumulates a library of student-generated humorous examples covering major doctrines, with the best ones becoming shared resources that create community. A legal writing professor enables peer evaluation that builds assessment literacy without competitive anxiety, preparing students for collaborative professional practice. A torts professor pairs sympathetic fact patterns with visual representations that clarify causal chains whilst activating affective memory pathways.

These applications do not replace clinical experience, one-on-one mentorship, or faculty expertise in doctrinal analysis. They supplement traditional pedagogy with multi-dimensional engagement that time constraints prevented. The resource investment—distributed across students through exercises like humorous summaries, front-loaded into prompt design but minimal thereafter—makes systematic implementation feasible at scale.

Research gaps remain. Legal education lacks rigorous experimental studies measuring learning outcomes from multi-loading applications, longitudinal research tracking skill development over time, and systematic evaluation comparing different integration approaches. The field needs proper controls and assessment, not anecdotal institutional reports. Equity concerns require attention—digital divides create uneven access, algorithmic bias may disadvantage underrepresented students, over-reliance risks diminished critical thinking. These constraints demand institutional support through policies addressing data privacy and ethical AI use, training programs building technological literacy, and resources ensuring equitable access.

But constraints do not invalidate the argument. Educational psychology established synergistic effects of multi-dimensional engagement through decades of cognitive science research. AI can generate content engaging multiple handles—the applications demonstrated this capacity exists now. Whether legal educators deploy AI to make systematic multi-loading feasible determines whether the technology changes teaching at margins or addresses fundamental pedagogical challenges.

The choice is implementation. Faculty can experiment with multi-loading applications, evaluate learning outcomes rigorously, and iterate based on evidence. Institutions can provide development opportunities, protect faculty time for pedagogical innovation, and maintain standards ensuring AI enhances rather than replaces human judgment. The field can conduct systematic research establishing which approaches work best for which objectives with which populations.

Multi-handle pedagogy represents AI's distinctive contribution to legal education—not efficiency in performing existing tasks, but enabling pedagogical strategies that resource constraints prevented for decades. The technology removes barriers whilst preserving faculty judgment about learning objectives and pedagogical appropriateness. What remains is recognizing the opportunity and choosing to pursue it systematically rather than dismissing AI categorically based on accuracy expectations appropriate for different applications.

The path forward is clear. Design exercises that activate multiple handles simultaneously. Evaluate whether emotional engagement, sensory processing, social learning, and metacognitive reflection enhance achievement of specific learning objectives. Iterate based on student response and learning outcomes. Build competencies through practice rather than passive training. Share effective applications whilst maintaining critical awareness of limitations and equity concerns.

Legal education can continue using AI one-dimensionally, or it can embrace multi-handle pedagogy. The choice determines whether AI adoption represents technological novelty or pedagogical evolution. The stakes extend beyond individual classrooms to the fundamental question: Will legal education deploy emerging technologies to address longstanding challenges, or will accuracy expectations rooted in different use cases prevent recognition of distinctive pedagogical capacities? The answer lies in implementation—not eventual implementation in speculative futures, but systematic deployment beginning now with the tools, frameworks, and research base that already exist.

The technology is ready. The pedagogy is established. The question is whether legal education will seize the opportunity.

---

[^34]: Hadi Alinezhad Noghabi and others, 'AI-Generated Storytelling for Socially Just EFL Pedagogy' (2024) 42 System 103. [^35]: Mayer (n 2). [^36]: OpenAI, 'DALL-E 3 Technical Documentation' (2023). [^37]: Gamma App, 'Gamma AI Presentation Platform' (2024). [^38]: Stanford M&A Negotiation Simulator documentation (2024). [^39]: Israeli law schools documentation (2024). [^40]: Contextual case method literature [to be confirmed]. [^41]: Jean Lave and Etienne Wenger, _Situated Learning: Legitimate Peripheral Participation_ (Cambridge University Press 1991). [^42]: Double-blind protocol research [to be confirmed]. [^43]: Keith Topping and others, 'AI and Peer Assessment: A Systematic Review' (2023) 182 Computers & Education 104456. [^44]: Zhiwei Guo, 'AI-Supported Peer Feedback' (2024) 28 Language Learning & Technology 1. [^45]: Zhuoxuan Lu and others, 'Comparative Analysis of AI, Peer, and Instructor Assessment' (2024) 15 Educational Technology Research and Development 423. [^46]: Hao Li and others, 'Peer Assessment in Learning: A Meta-Analysis' (2016) 62 Assessment & Evaluation in Higher Education 221. [^47]: LaBar and Cabeza (n 17). [^48]: Paivio (n 19). [^49]: Mayer (n 2). [^50]: Educational design research [to be confirmed]. [^51]: Google, 'NotebookLM Documentation' (2024). [^52]: David H Rose and Anne Meyer, _Teaching Every Student in the Digital Age: Universal Design for Learning_ (ASCD 2002). [^53]: Punya Mishra and Matthew J Koehler, 'Technological Pedagogical Content Knowledge' (2006) 108 Teachers College Record 1017. [^54]: Duri Long and Blakeley H Magerko, 'What Is AI Literacy?' (2020) Proceedings of CHI Conference 1. [^55]: Digital Promise, 'AI Literacy Framework' (2024). [^56]: Dana-Kristin Mah and Viktoria Groß, 'Faculty Perspectives on Generative AI' (2024) 17 Intl J Educational Technology in Higher Education 42. [^57]: Mah and Groß (n 56). [^58]: Krieger and Sheldon (n 4); Peterson and Peterson, 'Stemming the Tide of Law Student Depression' (2009) 9 Yale J Health Pol'y L & Ethics 357.
