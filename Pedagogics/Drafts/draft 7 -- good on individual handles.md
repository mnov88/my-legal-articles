# Beyond one-dimensional AI: multi-handle pedagogy in legal education

Artificial intelligence in legal education should move beyond one-dimensional applications to recognise its potential for creating content that engages multiple pedagogical dimensions simultaneously. While current implementations focus primarily on summarisation, quiz generation, and automated feedback, this narrow approach overlooks AI's capacity to address fundamental challenges in legal pedagogy. AI tools can generate learning materials that activate emotional, sensory, social, and metacognitive engagement concurrently, offering solutions to long-standing resource constraints that have limited educators' ability to create richly layered teaching content.

The argument for multi-dimensional AI use rests on three foundations. First, educational research demonstrates that learning improves when multiple cognitive and affective systems activate simultaneously. Second, legal education faces documented challenges in producing such materials due to time constraints and the labour-intensive nature of crafting hypotheticals that trigger appropriate emotional responses whilst maintaining doctrinal accuracy. Third, generative AI can create varied, contextualised content at scale, enabling educators to deploy pedagogical strategies that were previously impractical.

This analysis examines how AI in legal education has developed, why expectations of purely doctrinal output limit its potential, and how four pedagogical dimensions operate in legal learning. The article then demonstrates how AI-generated content can engage multiple dimensions concurrently through specific applications, before addressing the broader framework of multi-loading pedagogics and the competencies required for effective implementation.

## Adoption patterns and documented outcomes in higher education

AI adoption in higher education has accelerated dramatically. UK student use increased from 66% in 2024 to 92% in 2025, according to the HEPI Student Generative AI Survey 2025. Globally, 86% of students use AI in their studies, with 54% using it weekly and 25% daily. The proportion using generative AI specifically for assessments rose from 53% in 2024 to 88% in 2025. Among US students, usage of AI writing tools jumped from 27% in spring 2023 to 49% in fall 2023, an 82% increase in one semester.

Faculty adoption lags behind students but shows similar growth trajectories. The Cengage Group's 2024 GenAI Report found 45% of higher education faculty using AI tools in 2024, up from 24% in 2023. However, Elon University survey data indicate 62% of higher education leaders estimate fewer than half of faculty use AI, compared to 89% estimating at least half of students do so. Ellucian's survey of 445 faculty and administrators from 330+ institutions found 61% use AI for both personal and professional purposes in 2024, up 35 percentage points from 2023, with 93% expecting to expand use over the next two years.

Empirical studies reveal nuanced effects on learning outcomes. Abbas, Jam, and Khan conducted a three-wave time-lagged study of 494 university students, finding that academic workload and time pressure positively predicted ChatGPT use, but usage correlated with increased procrastination, memory loss concerns, and decreased academic performance. Guo, Li, and Cunningham's year-long experimental study with 169 students found that students perceived ChatGPT as beneficial to learning, improving confidence and motivation, though ability to address individual needs or replace instructors rated less favourably. Graduate students showed greater motivation to use ChatGPT than undergraduates.

Bai and Wang's two-wave longitudinal survey of 323 Chinese university students using structural equation modelling found that GAI interaction quality and output quality positively influenced learning motivation and creative self-efficacy. GAI output quality had a direct positive effect on learning outcomes. A meta-analysis of 17 empirical studies with 1,735 students examining ChatGPT's impact on student engagement found medium effect sizes for behavioural engagement, large effect sizes for cognitive engagement, and medium effect sizes for emotional engagement, though risk of over-reliance emerged as a potential source of disengagement.

Legal education has implemented AI more slowly than other fields, but documented initiatives demonstrate growing integration. Notre Dame Law School became the first to partner with Harvey AI in 2024, providing access through Advanced Legal Research courses in spring 2025. Harvey AI had been adopted by 70% of AmLaw 10 firms and nearly 50% of AmLaw 100 firms. Harvard Law School launched its Initiative on Artificial Intelligence and the Law in 2023. UC Berkeley School of Law announced the first LL.M. degree focused on AI, beginning summer 2025, covering AI regulation including the EU AI Act, AI capabilities, law and policy development, and benefits and harms. Case Western Reserve University School of Law made comprehensive AI training mandatory for its entire first-year class in 2024, partnering with Wickard.ai for pre-class assignments, live instruction, guest speakers, and hands-on exercises.

The ABA Task Force on Law and Artificial Intelligence survey from June 2024 found 55% of law schools offer classes dedicated to teaching about AI, with growing incorporation into curricula. Suffolk University Law School developed multiple initiatives including a generative AI course assessing ethics and strengths and weaknesses of AI tools, partnerships with video-based learning platforms used by top law firms, and an AI mock judge created by the legal writing team and litigation lab. Default policy prohibits GenAI on assignments unless professors explicitly allow it.

Faculty use AI primarily for lesson planning (38-44%), research and content gathering (44%), completing administrative tasks, summarising information (38%), and supporting lectures. Students use AI predominantly for summarising or paraphrasing texts, answering homework questions, organising schedules, brainstorming (51%), and getting information (53%). In assessments specifically, 88% of students used GenAI in 2025, up from previous years.

## Resistance and the accuracy expectation

Faculty resistance to AI adoption reflects deeper concerns than technological unfamiliarity. Shata and Hartley's study of 294 full-time faculty from two US public universities found 33.6% opted out of using generative AI entirely. Non-users identified five primary reasons: insufficient knowledge about GAI technology, no perceived value for their work, conflicts with professional identity, concerns about replacing human creativity and critical thinking, and broader negative societal consequences. Non-users reported lower general comfort with technology, which negatively influenced intention to use GAI. Importantly, both users and non-users expressed academic dishonesty concerns, but non-users associated GAI with broader negative consequences beyond the classroom.

The resistance stems partly from expectations about AI-generated content quality. Educational technology adoption follows the Technology Acceptance Model, where perceived usefulness predicts attitudes and intention to use more strongly than perceived ease of use. Trust acts as a significant mediator between acceptance factors. When AI fails to meet quality expectations, trust erodes and adoption stalls.

Hallucinations and fabrication constitute major accuracy concerns. Chelli and colleagues' systematic evaluation of 471 references from 11 systematic reviews found hallucination rates of 39.6% for ChatGPT-3.5, 28.6% for ChatGPT-4, and 91.4% for Bard. Precision rates were 9.4% for GPT-3.5, 13.4% for GPT-4, and 0% for Bard. The study concluded that LLMs should not serve as primary tools for systematic reviews, with all references requiring validation. Walters and Wilder examined 636 bibliographic citations from 84 ChatGPT-generated papers on 42 topics, finding 55% of GPT-3.5 citations fabricated versus 18% of GPT-4 citations. Among real citations, 43% from GPT-3.5 had substantive errors versus 24% for GPT-4, most commonly incorrect volume, issue, or page numbers and incorrect dates.

Legal education confronts particular accuracy challenges. Legal research requires precision in citations and case law. The Mata v. Avianca case exemplified risks when a New York attorney used ChatGPT for legal research and a federal judge found fabricated citations and case law. UC Davis Law Library research guides warn that Lexis+ AI remains prone to hallucinations and conceptual errors, whilst ChatGPT produces citations that are not Bluebook ready, with flaws including missing italics and invented abbreviations.

Academic integrity concerns extend beyond hallucinations to plagiarism and contract cheating. Birks and Clare connected AI-facilitated academic misconduct to crime-prevention frameworks, noting how ChatGPT enables new forms of cheating. Barrot, Dianati, and Rossi surveyed 100 educators about AI writing tools including ChatGPT, Grammarly, Google Translate, and Quillbot, finding educators recommend regulating ChatGPT in formative assessments but banning it in summative tests. Preferred allowable AI-generated content was 10-15%. Top concerns included dependency, declining creativity, validity and accuracy issues, and ethical problems. Critically, 96% lacked institutional guidelines for AI use.

AI cheating incidents increased from 1.6 students per 1,000 in 2022-23 to 7.5 students per 1,000 in 2024-25, nearly 400% growth. Student discipline rates for AI-related plagiarism rose from 48% in 2022-23 to 64% in 2024-24. The University of Pennsylvania reported a seven-fold increase in violations for attaining unfair advantage. Among students not using AI, 64% said they did not trust information provided. Over 20% of students reported not being allowed to use AI.

These accuracy concerns create an expectation that AI should produce correct, purely doctrinal content. The expectation follows logically from AI's professional presentation and confident tone. AI outputs look legitimate at first glance, and users expect technology to enhance performance based on TAM framework assumptions. The educational context creates further assumptions that recommended tools are reliable. However, this expectation overlooks AI's actual capabilities and optimal use cases. AI excels at generating varied scenarios, offering multiple perspectives, and creating content that triggers emotional responses, functions that do not require perfect doctrinal accuracy to achieve pedagogical value.

## Emotional engagement as pedagogical foundation

Emotions powerfully influence learning through well-established neural mechanisms. Pekrun's Control-Value Theory of Achievement Emotions identifies four groups of academic emotions: achievement emotions, epistemic emotions, topic emotions, and social emotions. Positive emotions such as enjoyment, hope, and pride promote attention, motivation, flexible learning strategies, and self-regulation. Negative emotions can obstruct learning but can be deployed productively with appropriate precautions. Self-confidence and task values serve as primary influences on emotions. Pekrun's model demonstrates that emotions interact dynamically with cognitive processes to facilitate effective learning.

Neuroscience research confirms these connections. Immordino-Yang and Damasio established that meaningful thinking and learning are inherently emotional, arguing that humans only think deeply about things they care about. Emotional experiences activate brain mechanisms that evolved to manage basic survival, making emotions powerful motivators of learning. Brain regions managing viscera and survival mechanisms co-opt to construct complex emotional experiences. Learning requires leveraging emotional aspects rather than suppressing them. LaBar and Cabeza demonstrated that the amygdala directly mediates emotional learning and facilitates memory operations in the hippocampus and prefrontal cortex. Emotion-memory interactions occur at encoding, consolidation, and retrieval stages. Emotional events attain privileged status in memory with greater retention advantages. Tyng and colleagues found that emotion modulates attention, selectivity, motivation, and action through distinct neural pathways.

Legal education traditionally resists emotional engagement. The discipline trains students to be suspicious of emotions, valuing logic and rationality. Bone argues that emotions play an integral part in effective legal learning, skill development, and wellbeing, yet traditional rigid focus on thinking like a lawyer suppresses emotional engagement. Attempts to disregard emotions in law school are both misguided and destined to fail. Legal education produces psychological distress: 44% of law students have clinically elevated distress levels, and 70% of lawyers develop alcohol problems at some point. Contributing factors include student debt, fear of rejection, lack of feedback, emphasis on grades, competitive atmosphere, and isolation.

Despite this resistance, clinical legal education has embraced emotional intelligence and empathy. Gerdy identifies empathy as crucial for effective lawyering alongside technical competence. Empathic communication builds rapport and trust, enabling clients to provide more information. Williams, Sifris, and Lynch measured empathy in undergraduate law students using the Jefferson Scale of Empathy, finding empathy varies significantly across individuals. Empathy training in legal education remains in early stages with limited empirical evaluation, but clinical programmes provide unique opportunities to engage with emotions. Field, Duffy, and Huggins explored emotions' role in clinical legal education through an international workshop, finding that clinical education provides unique space to deeply engage with emotions whilst traditional doctrinal courses suppress them.

Storytelling and hypotheticals serve as primary vehicles for emotional engagement in legal education. Cases provide narrative elements that illuminate legal principles. Resolution of legal cases relies heavily on courts adopting particular stories. Menkel-Meadow argues that case studies and stories effectively teach legal ethics. Steslow and Gardner demonstrate that storytelling integrates into law courses as critical elements of teaching methodology, engaging students in the learning process. Blissenden documents storytelling as a teaching model in Australian law schools, finding students respond positively to narrative approaches.

Humour represents another emotional engagement tool, though research remains limited. James and Legg examined humour's role in the law classroom, finding literature "generally limited to anecdotal evidence". However, they link appropriate humour to relaxed learning environments, student motivation, attendance, engagement, and positive evaluations. Benefits include helping students pay attention, leading to more work completion, reducing discipline problems, reducing stress, and increasing creativity. Laughter releases dopamine, activating the reward system and stimulating long-term memory. The critical caveat is that humour must avoid cruelty or inappropriateness that makes classrooms uncomfortable.

Faculty face significant time constraints in creating emotionally engaging materials. Crafting hypotheticals that trigger appropriate emotional responses whilst maintaining doctrinal accuracy requires substantial investment. Development of shared teaching resource platforms such as CALI, H2O, eLangdell, and LEAPS indicates systemic recognition of these time burdens. Adjunct faculty compensation disparity suggests time investment is not valued appropriately, with reports of over 100 hours of preparation for a single semester course.

AI addresses this constraint by generating emotionally engaging content rapidly. Nair, Jassim, and Dimitrova demonstrated AI-based learning content generation and learning pathway augmentation increase learner engagement through narrative fragments. Automation enables generation on the fly when learning pathways change. Beguš compared human crowdsourced storytelling with AI storytelling, finding AI capable of producing narratives grounded in psychological archetypes. Alinezhad Noghabi and colleagues examined AI-generated storytelling for socially just EFL pedagogy, exploring learner emotions through Fraser's justice theory. AI-generated content triggered authentic emotional responses including discomfort, vulnerability, and caring. The study demonstrated AI-powered storytelling provides space for pedagogy of discomfort, focusing on emotions like shame and anger to facilitate adoption of socially just practices.

AI-generated images in elementary education showed no negative impact on learning outcomes with a "slightly more favourable trend in learning attitude". Pellas found AI-generated instructional videos impacted problem-based learning positively in science teacher education. The capacity to generate adaptive educational materials tailored to individual preferences and learning styles boosts learning motivation through personalised, contextualised design. Manzoni and colleagues demonstrated AI narrative modelling reproduces archetypal storytelling patterns, allowing instructors to use AI-generated narrative variations to demonstrate key concepts through guided storytelling exercises exploring personal identity and emotional conflict.

## Sensory and multimodal learning pathways

Dual Coding Theory, developed by Paivio, posits that information processes through two separate channels: verbal and nonverbal. Presenting information in both formats engages two distinct brain areas—the visuospatial sketchpad and phonological loop—enhancing comprehension and significantly improving knowledge retention. Research demonstrates participants recall concrete words better than abstract words, and pictures are recalled better than words alone due to dual coding effects. Clark and Paivio applied dual coding specifically to education, demonstrating how verbal and visual information presentation improves learning outcomes.

Mayer's Multimedia Learning Theory extends dual coding principles with fifteen evidence-based design principles derived from over 200 experimental studies. The multimedia principle establishes that words and graphics together produce better learning than words alone. The contiguity principle requires aligning words to corresponding graphics. The modality principle recommends presenting words as audio narration rather than on-screen text. The redundancy principle cautions against presenting the same content in multiple formats simultaneously. The coherence principle emphasises removing extraneous content. Additional principles address personalization, segmenting, and other factors. Moreno and Mayer demonstrated that cognitive principles of multimedia learning depend on modality and contiguity, with visual information paired with audio narration distributing cognitive load across visual and auditory channels.

Cognitive Load Theory provides the theoretical foundation for understanding why multimodal presentation improves learning. Sweller established that cognitive load during problem solving affects learning, identifying three types: intrinsic load reflecting inherent difficulty, extraneous load caused by poor instructional design, and germane load supporting schema construction. Working memory has limited capacity, processing approximately seven items simultaneously. Effective instruction minimises extraneous load whilst managing intrinsic load and promoting germane cognitive processing. Mousavi, Low, and Sweller found that reducing cognitive load by mixing auditory and visual presentation modes improves learning outcomes by distributing processing across multiple channels.

Educational research indicates humans process information primarily through vision, at 83%, compared to 11% hearing, 3.5% smell, 1.5% touch, and 1% taste. Retention rates demonstrate the power of multimodal engagement: people remember 10% of what they read, 20% of what they hear, 30% of what they see, 50% of what they hear and see, 70% of what they say, and 90% of what they say and do. These statistics underscore the importance of engaging multiple sensory modalities simultaneously.

Legal education has employed visual aids, though less systematically than other disciplines. Shabiralyani and colleagues studied 200 students and teachers, finding 70% agreed visual aids help motivation, 75% agreed they help clarification, 68% agreed they increase vocabulary, 82% agreed they save time, 71% agreed they avoid dullness, and 92% agreed they provide direct experience. The correlation coefficient of 0.956 with R² of 0.785 indicates visual aids explain 78.5% of variance in learning outcomes. Caplan discussed visual aids complementing law professor teaching strategies, applying multimedia learning psychology to legal education through illustrations, visual renderings of legal texts, visualisations of legal concepts, and interactive visual aids.

Flowcharts and concept maps represent specific visual tools applicable to legal analysis. Deplano conducted a pilot study on Constitutional and Administrative Law, finding concept maps supplement lecture content whilst maintaining interactive character of tutorials. Concept maps help students understand abstract concepts such as constitution and separation of powers. Based on Novak's concept mapping theory and Ausubel's cognitive psychology, concept maps increase active retrieval pathways in the brain, lengthen memory of complex concepts, and significantly increase recall efficiency. They help visualise relationships between procedural steps and elements in Black Letter Law, breaking down complicated legal analysis into manageable parts.

McLachlan and Webley examined visualisation of law and legal process, finding use of information visualisations in legal literature extremely rare, with not more than ten articles per year. Concept flow diagrams appear most commonly, with UML as the most frequently applied representational approach. Visual representation aids organisation, understanding, collaboration, and recall of complicated legal concepts, reducing confusion for professionals and laypeople alike. Despite documented benefits, visual approaches remain underutilised in legal pedagogy.

Podcasts represent audio learning modalities in legal education. Major platforms include the ABA Law Student Podcast addressing issues affecting law students, bar exams, and job searches; Law School Toolbox Podcast offering study tips, bar exam preparation, and legal career guidance; Law to Fact featuring substantive legal issues with distinguished professors; and Thinking Like a Lawyer providing legal analysis of everyday topics. Podcasts offer accessible learning anywhere at any time, with episode lengths shorter than traditional textbooks. The modality effect established by Tindall-Ford, Chandler, and Sweller demonstrates learning enhances when visual information pairs with audio narration rather than on-screen text, distributing cognitive load across visual and auditory channels.

AI tools enable multimodal content generation at scale. DALL-E 3, integrated with ChatGPT, generates images from text descriptions, creates diagrams, flowcharts, and graphs for e-learning courses, thumbnails, and promotional materials. MIT's Integrated Learning Initiative evaluated AI-generated personalised learning content for improving engagement and outcomes through vocabulary learning using GPT-3/4 and DALL-E, generating examples tailored to interests, learning styles, and skill levels.

Gamma AI generates presentations from text or outlines in minutes using multiple LLMs including Claude Haiku, ChatGPT, and DALL-E. Features include an AI Agent for editing and enhancement, web research with citations, image generation through DALL-E 3, Ideogram, and Flux, real-time collaboration, analytics tracking views and engagement, and export to PowerPoint and Google Slides. Over 250 million presentations, websites, and documents have been generated, with 700,000 daily creations. Damien Germino, Associate Dean at Mercy University, describes it as a "100% workflow tool, all-in-one assistant that adapts to educators' needs".

Google NotebookLM provides AI-powered research and writing assistance using Gemini 1.5. The system is source-grounded, using only uploaded documents. It supports 50 sources in the free version and 300 in paid versions, accepting PDFs, Google Docs, Google Slides, URLs, YouTube videos, and audio files up to 25 million words. Audio Overview generates AI podcasts from documents. Video Overview creates presentations with narration. The system generates study guides, FAQs, timelines, and briefing documents with citations linking to sources. User data remains private and is not used for training. NotebookLM enables converting lecture notes to study materials, creating presentation outlines, generating podcasts for audio learning, and research synthesis.

Chan and Tsi surveyed 399 undergraduate and postgraduate students at Hong Kong universities, finding generally positive attitudes toward GenAI. Students identified benefits including personalised learning support, writing assistance, and research capabilities. Concerns centred on accuracy, privacy, ethical issues, and impact on critical thinking and creativity. A 2025 study titled "Mastering Knowledge: The Impact of Generative AI on Student Learning Outcomes" analysed 192 student reflections, finding higher learning outcomes when using GenAI for knowledge construction through mastery approaches, but lower outcomes with procedural use without knowledge augmentation.

A comparative analysis of AI-generated content tools in higher education across humanities, STEM, and social sciences involving 1,099 students found 37% increases in interdisciplinary project outcomes with strategic AIGC implementation, measured by collaborative problem-solving scores, cross-domain knowledge integration, and peer evaluations. Challenges included algorithmic bias, digital equity, and maintaining discipline-specific skills. Studies of AI-generated personalized learning content show effectiveness depends heavily on implementation approach, with mastery-focused use producing superior outcomes to efficiency-focused use.

## Social learning and collaborative dynamics

Vygotsky's social constructivism establishes that knowledge constructs through social interactions within cultural and historical contexts. Learning occurs on two levels: first through interaction with others (interpsychological), then integrated into individual mental structures (intrapsychological). The Zone of Proximal Development defines "the distance between the actual developmental level as determined by independent problem solving and the level of potential development as determined through problem-solving under adult guidance, or in collaboration with more capable peers". The More Knowledgeable Other concept identifies someone with higher ability or understanding who provides guidance. Scaffolding provides temporary support enabling learners to accomplish tasks beyond current capabilities.

Teaching styles grounded in constructivism shift from traditional didactic models to student-centered approaches. Five basic characteristics of cooperative learning align with Vygotsky's theory: positive dependency, face-to-face supportive interaction, individual accountability, interpersonal and small group skills, and assessment of group process. Meta-analysis by Johnson, Johnson, and Smith found students in collaborative learning scored higher than 61% of students in individualistic or competitive settings, demonstrating social learning's effectiveness.

Communities of Practice theory, developed by Lave and Wenger, describes how learning occurs through participation in social communities. Three essential components define communities of practice: a shared domain of interest defining community identity, community members who interact and engage in shared activities helping each other, and practice where members develop shared repertoires of resources including experiences, stories, tools, and ways of addressing recurring problems. Legitimate peripheral participation describes how newcomers initially observe and perform simple tasks whilst learning community norms, developing identity through this socialization process.

Legal education confronts significant social pressures that complicate collaborative learning. Pasley documented that 44% of law students have clinically elevated distress levels, with 70% of lawyers developing alcohol problems at some point. Curved grading creates artificially scarce resources where "my good grade depends on my classmates getting bad grades". Students engage in comparison behaviours, with one describing it as "an arms race. Your opponent gets a supplement, so you have to as well". Students report using stimulants to work longer hours. Hundreds of students enter an "upward spiral of pushing themselves to work harder, longer, and faster to out-compete everyone else for the scarce resource of high grades".

Lack of feedback exacerbates competition. Comparing oneself to other students provides the primary source of feedback during semesters. This "lack of institutional feedback tends to result in people creating their own feedback, and using each other as a guide". Perceived arbitrariness of grading undermines confidence. Peterson and Peterson examined law student depression through positive psychology lenses. Sturm and Guinier analysed the "law school matrix" of competition and conformity culture. Sheldon and Krieger evaluated whether legal education has undermining effects on motivation, values, and wellbeing, finding systematic negative impacts.

Competition and anxiety push students apart and train them not to trust each other. Students form small cliques of friends who "can have our backs", but experience universal feelings of isolation despite some friendships. Self-esteem roller-coasters tie to grades. Law school culture affects how students see law, themselves as lawyers, and how they interact with others. Krieger's research documents identity loss and shifts away from own values during law school.

Peer evaluation offers potential benefits but introduces social pressure. Ashley and Goldin examined computer-supported peer review in law school contexts, finding peer-generated review scores using either legal domain-related or problem-specific rubrics correlated with independently assigned instructor grades. This supports the belief that peer review can serve as additional sources of useful feedback including on substantive legal issues, with analysis informing instructors about how well writing exercises met instructional goals.

Legal writing pedagogy resources emphasise peer review and self-evaluation exercises as formative assessment tools. These focus on specific assignment goals, crafting forms, laying foundations, debriefing, and timing. Students review feedback on legal analysis, use of legal authority, and crafting effective legal rules. Li and colleagues' meta-analysis found peer assessment promotes student learning when properly executed with clear rubrics, training, and multiple assessors, achieving reliability comparable to instructor evaluations.

However, peer assessment raises concerns about student competence, bias, favouritism, variability, stress, and anxiety. The social dynamics of knowing peers will evaluate one's work create performance anxiety distinct from instructor evaluation. Students may inflate grades for friends or deflate grades for competitors. Fear of judgment inhibits risk-taking and authentic expression.

AI reduces social pressure through several mechanisms. Topping and colleagues' systematic review examined 79 papers on enhancing peer assessment with artificial intelligence from 2013-2023, finding vast majorities showed AI improved peer assessment, with only two papers finding AI outcomes equal to or worse than non-AI approaches. Research focused on diversity in grades and feedback, fuzzy logic, and analysis of feedback, whilst automated assignment, automated assessment, and calibration remained under-researched.

Anonymisation and bias reduction represent primary mechanisms. Double-blind protocols prevent identity disclosure between creators and assessors, removing fear of judgment and social comparison whilst eliminating interpersonal politics from evaluation. Calibration and training provide additional benefits. Russell examined Calibrated Peer Review evolution, finding students assess exemplar artefacts before reviewing peer work, with systems weighting reviews based on calibration scores. Hamer, Kell, and Spence developed Aropä with reputation systems for automatic grade calibration, diminishing impact of "rogue reviews".

Automated feedback on reviews helps develop assessment skills without peer judgment. Nguyen, Xiong, and Litman found automated formative feedback enhanced quality of student feedback through iterative design. Stelmakh, Shah, and Singh developed methods to detect strategic behaviour in peer assessment. Badea and Popescu created hybrid approaches for mitigating learners' rogue review behaviour.

AI-generated content for training enables peer evaluation practice without social stakes. Guo examined effects of an AI-supported approach to peer feedback on 124 Chinese undergraduate EFL students, finding the intervention significantly enhanced students' feedback quality and writing ability. The AI chatbot "Eva" integrated into the peer review system provided feedback on student reviewers' comments, scaffolding their learning. The EvaluMate system leverages ChatGPT and other LLMs to scaffold student reviewers' feedback generation, offering feedback on comments in real-time. Students can review AI suggestions critically before finalising feedback.

Lu and colleagues compared generative AI, peer, and instructor assessments with 76 undergraduate students. AI provided higher grades than human assessors and higher-quality feedback than peers, with more detailed and specific comments, though occasionally including irrelevant information. Peer feedback proved more personalised and context-sensitive. The study emphasises complementary strengths rather than replacement, suggesting AI can train students in peer evaluation without social risks, preparing them for human peer review.

AI presents multiple stakeholder perspectives, valuable for understanding different viewpoints in legal analysis. Karran and colleagues surveyed 1,198 multi-stakeholder participants including students, teachers, parents, and school directors across four AI use case scenarios manipulating agency, transparency, explainability, and privacy. Key mediators of acceptance included global utility, justice, and confidence. Different stakeholder groups demonstrated varying perceptions: parents concerned about transparency and risk, students concerned about justice, and confidence affected by agency levels. AI role-play serves as research methodology, systematically generating diverse stakeholder viewpoints, simulating authentic stakeholder reasoning patterns, though limitations include potential cultural misrepresentation and oversimplified social dynamics.

Systematic reviews of AI-powered collaborative learning in higher education found machine learning, natural language processing, and recommender algorithms facilitate collaborative learning. Predictive analytics and multimodal approaches enhance student engagement and motivation. Personalised learning systems ensure effectiveness of collaborative environments. Critical issues include task design, emotional engagement, and social presence in AI-based environments. Ethical considerations encompass transparency, data protection, and balance between automation and human touch.

## Metacognitive development and self-regulation

Metacognition, defined by Flavell as "thinking about thinking", distinguishes between two primary components: metacognitive knowledge and metacognitive regulation. Metacognitive knowledge encompasses declarative knowledge (what one knows), procedural knowledge (how to apply strategies), and conditional knowledge (when and why to apply strategies). The model of metacognitive monitoring focuses on person, task, and strategy variables. Metacognitive processes interact dynamically to facilitate effective learning.

Schraw developed the Metacognitive Awareness Inventory with 52 items organised into two main components: knowledge of cognition with 17 questions, and regulation of cognition with 35 questions. Regulation includes planning, monitoring, and evaluation. Planning involves performing activities related to background information. Monitoring encompasses self-testing skills and controlling learning. Evaluation assesses product information from the learning process. Schraw emphasised metacognition as both task-specific and task-general, finding metacognitive skills are transferable across domains.

Zimmerman developed a cyclical three-phase model of self-regulated learning: the forethought phase involving task analysis and self-motivation beliefs, the performance phase involving self-control and self-observation, and the self-reflection phase involving self-judgment and self-reaction. Self-regulated learning includes metacognitive, motivational, and behavioural active participation. The framework emphasises goal setting, strategic planning, and monitoring progress, connecting self-efficacy beliefs to self-regulatory processes. SRL skills are teachable and enhance academic achievement.

Çetin studied 276 university students using Schraw and Dennison's inventory, finding metacognition and self-regulated learning are correlated constructs involving metacognitive, motivational, and behavioural participation. Dignath and Büttner found training interventions foster use of strategies and transfer. Metacognitive skills prove task-general and transferable. Hybrid training combining metacognitive and cognitive strategies supports near transfer. Planning, monitoring, and evaluating constitute core metacognitive skills.

The Education Endowment Foundation's meta-analysis showed metacognition provides high impact for low cost, with an additional seven months of progress when metacognitive strategies are implemented. Benefits prove particularly pronounced for disadvantaged students, suggesting metacognitive instruction addresses equity concerns.

Legal education has embraced reflective practice as metacognitive development. Casey developed a comprehensive six-stage model for teaching reflective practice in law schools, responding to ABA accreditation Standard 305(e)(7) requiring "opportunities for student reflection". The six stages progress from comparing performance to standards of reasonable competence, identifying different ways to accomplish tasks, analysing personal preferences and biases, considering preferences and biases of others, examining systemic power dynamics and social structures, and reflecting on how reflection affects thinking processes. The model aligns with cognitive development theories from Perry, Kohlberg, and King and Kitchener, moving students from dualistic to relativistic to contextual understanding.

Ogilvy examined journal use in legal education as tools for reflection, finding journals provide "windows into thinking and lives of students". Challenges include reflection tending toward superficial content without systematic approaches, necessitating structured prompts and guidance. Think-aloud protocols integrate into clinical legal education to externalise thinking processes, model expert reasoning, identify metacognitive gaps, and provide opportunities for feedback.

Exam wrappers foster metacognitive self-assessment in legal education. Suffolk University Law School documented exam wrapper use addressing the Dunning-Kruger effect, where poor performers demonstrate overconfidence. Exam wrappers help students identify gaps in understanding, support transitions from novice to expert reasoning, and include questions such as "How did you study? What errors did you make? What will you do differently?". Lovett established exam wrappers make exams worth more than grades by promoting metacognition. Soicher and Gurung found exam wrappers increased metacognitive awareness and improved study habits.

Nicholson and Kay studied reflective learning at King's College London MSc Law and Professional Practice, integrating reflective writing into assessment frameworks. Practice Projects included 12,000-word assignments plus reflective components. Challenges included student resistance to reflective practices. Solutions involved scaffolding, ongoing guidance, and demonstrating benefits to students. Reflective practice enabled critical evaluation of learning experiences whilst supporting metacognition and professional identity development.

AI facilitates metacognitive engagement through intelligent tutoring systems and adaptive scaffolding. MetaTutor represents over ten years of research on hypermedia-based ITS, designed to scaffold college students' self-regulated learning. Based on Winne's model of SRL, the architecture includes four pedagogical agents: Gavin as Guide, Pam as Planner, Mary as Monitor, and Sam as Strategizer. An SRL palette enables learner-initiated metacognitive processes. Adaptive scaffolding responds to learner interactions.

Metacognitive processes supported include Judgments of Learning, Feelings of Knowing, Content Evaluation, and Monitoring Progress Toward Goals, alongside planning, monitoring, and evaluating strategies. Research findings demonstrate adaptive scaffolding conditions show higher metacognitive process use. Prior knowledge affects strategy deployment: high prior knowledge learners engage in more JOLs and monitoring, whilst low prior knowledge learners need more scaffolding. Multimodal data including log files, eye tracking, and facial expressions prove essential for understanding SRL.

Jin and colleagues used speed dating methodology with 16 university students examining ten AI applications: AI Plan Organizer for goal setting and planning, Pre-question Generator for prior knowledge assessment, Virtual Human for motivational support, Intelligent Suggestion for cognitive strategy support, AI Analytics for learning monitoring dashboards, AI Companion for career-oriented motivation, AI Agent for question answering, Notelink for note review and video linking, Adaptive Quiz for personalised assessment, and AI Reflection for praise and self-satisfaction.

Students perceived AI as useful for metacognitive, cognitive, and behavioural regulation but not for motivational regulation, preferring human support for motivation. Three critical pedagogical aspects emerged: learner identity requiring AI to account for developing characteristics, learner activeness balancing AI dependence versus learner agency, and learner position considering independent versus dependent learner roles. AI Analytics proved most helpful, selected by ten of sixteen students. Virtual Human proved least helpful, selected by thirteen of sixteen students as least useful. Design implications include process-oriented feedback, content modification control, and selective disclosure.

Xu and colleagues studied 68 college students, comparing experimental groups with metacognitive support against controls. Metacognitive support enhanced self-regulated learning abilities, particularly for task strategy and self-evaluation. GenAI without metacognitive support risks reducing SRL effectiveness. Metacognitive support proves key for effective regulation in GenAI environments. Between-group differences in achievement were not significant, but SRL processes improved.

Lee and colleagues conducted a systematic mapping review of 84 studies at the intersection of AI and self-regulated learning, finding AI-SRL research focuses mainly on higher education students. AI implemented as adaptive systems, prediction and profiling, ITS, and assessment and evaluation tools. Direct impact appears on metacognitive and cognitive aspects of SRL. The motivational aspect remains underexplored. Zimmerman's model was most frequently applied, though one-third of studies did not specify SRL theory. Need exists for standardised approaches to measuring SRL with AI.

## Multi-handle content generation in practice

AI's capacity to generate content engaging multiple pedagogical dimensions simultaneously creates opportunities unavailable through traditional methods. Four examples illustrate how AI-generated materials can activate emotional, sensory, social, and metacognitive engagement concurrently whilst addressing practical pedagogical challenges.

Students using AI to generate humorous lecture summaries engage emotional and metacognitive handles simultaneously. Creating humorous content requires understanding underlying concepts sufficiently to identify incongruities and exaggerate elements whilst maintaining accuracy. This metacognitive process demands evaluation of which concepts matter most, how they connect, and what makes them memorable. The humour itself triggers emotional engagement through dopamine release, activating reward systems and stimulating long-term memory as demonstrated in educational psychology research. James and Legg found humour in law classrooms links to relaxed learning environments, student motivation, and engagement. Banas and colleagues' review of four decades of educational humour research found appropriate humour helps students pay attention, leads to more work completion, and increases creativity.

The student creation process with AI scaffolds metacognitive development. Students must prompt AI effectively, requiring explicit articulation of learning objectives and conceptual relationships. When AI produces humorous summaries, students evaluate accuracy and appropriateness, engaging in quality assessment requiring deep understanding. This iterative process develops metacognitive skills whilst producing materials serving both individual review and potential peer learning. Sharing humorous summaries creates social learning opportunities, enabling communities of practice where students build shared repertoires of resources. The reduced stakes of humorous content, acknowledged as student-generated creative work rather than authoritative sources, diminishes social pressure associated with peer evaluation.

AI generating doctrinally similar but factually different scenarios facilitates perspective-taking and empathy development. Gallacher argued that thinking like a lawyer is "fundamentally negative; it is critical, pessimistic, and depersonalizing", yet lawyers must communicate with non-lawyers frequently, requiring empathy. Legal education systematically eliminates empathy from students despite empathy being essential for effective client representation. Varying factual contexts whilst maintaining doctrinal similarity enables students to understand how identical legal principles apply differently based on circumstances, developing cognitive flexibility and contextual reasoning.

The Stanford M\u0026A Negotiation Simulator exemplifies this approach, using generative AI to mimic characters and present negotiation scenarios with specific objectives and constraints. Students practice strategic thinking against different personality types. Built by computer science students interviewing senior M\u0026A partners, the tool reflects "personalities and thinking of senior lawyers", previewing for young lawyers "experiences of senior partners, walking in their shoes and understanding nuance of those practices in a low-risk environment". Currently used by law firms for new associates, this application demonstrates AI generating varied perspectives that would be impractical to simulate through traditional means.

Israeli law schools documented using Claude, ChatGPT-4, and Gemini Pro 1.5 to generate criminal law case studies with solutions and practice chatbots for interactive learning simulating legal discussions. Studies of 2,141 students across five courses showed AI enhanced learning but required instructor oversight. The capacity to generate multiple variations of scenarios enables students to encounter diverse factual contexts systematically, building pattern recognition and critical thinking skills identified by Simpson as essential for developing legal reasoning through the FIRAC model applied across wide varieties of assignments.

The contextual case method pairs opinions with other perspectives on the same legal questions, exploring complexities that went unaddressed. By humanizing opinions and encouraging students to "imagine a different legal world", this pedagogical approach benefits from AI's capacity to generate alternative contexts rapidly. Research on perspective-taking in negotiation distinguishes cognitive perspective-taking (considering how others think) from emotional empathy (experiencing sympathy and concern), with different skills valuable in different negotiation contexts. AI-generated scenarios enable systematic practice across contexts.

AI-generated answers of varying quality for peer evaluation practice reduce social pressure whilst developing assessment literacy. Topping and colleagues' systematic review found AI improves peer assessment across multiple dimensions. The EvaluMate system powered by ChatGPT facilitates peer review by offering feedback on student reviewers' comments, scaffolding feedback generation. AI provides feedback to reviewers, not just about reviewed work, enabling students to practice evaluation skills without social stakes.

Lu and colleagues' comparison of AI, peer, and instructor assessment found AI provided higher-quality feedback than peers with more detailed and specific comments. Students can practice peer evaluation on AI-generated work of varying quality, learning to identify strengths and weaknesses, provide constructive feedback, and calibrate judgments against instructor standards. This practice occurs without risking peer relationships or experiencing judgment anxiety. Double-blind protocols prevent identity disclosure, removing fear of social comparison. Calibration training using exemplar artefacts before peer work review, with systems weighting reviews based on calibration scores, develops assessment competence.

Guo's study of 124 Chinese undergraduate students found AI-supported peer feedback significantly enhanced feedback quality and writing ability. The AI chatbot "Eva" integrated into peer review systems provided feedback on student reviewers' comments, creating iterative learning cycles. Students developed metacognitive awareness of what constitutes effective feedback whilst building confidence in assessment capabilities before engaging in consequential peer review with humans.

The social learning dimension activates when students practice on AI-generated content, then apply skills to human peer review. The scaffolding reduces initial social pressure, enabling skill development in low-stakes environments. Legitimate peripheral participation occurs as students observe AI feedback on their reviewing, gradually developing competence before full participation in peer assessment communities. This addresses concerns about student competence, bias, and favouritism in peer assessment whilst maintaining benefits of multiple feedback sources.

Hypotheticals with emotional content paired with graphics engage emotional and sensory handles simultaneously. Mayer's multimedia principle establishes words and graphics together produce better learning than words alone. Combining emotional content triggering topic emotions and social emotions with visual representations engaging dual coding pathways creates powerful memory encoding. Educational design research found positive emotional design using vivid colours, anthropomorphic features, and sound outperformed neutral monochromatic designs on retention and transfer tests. Making essential elements visually appealing "initiates and guides cognitive processing during learning" according to emotional design hypothesis.

Trial graphics research demonstrates emotional cues in legal presentations significantly influence perceptions and decision-making. Strategic emotional triggers including fear, anger, empathy, and surprise capture attention and enhance memory retention. Emotional triggers activate the amygdala, enhancing retention of crucial information. Visual law movements advocate for graphs, icons, tables, and charts supplementing text, focusing on user needs and context, maximizing clarity, and organising information for both overview and detail. Venn diagrams for overlapping legal categories such as double jeopardy analysis illustrate how visuals reveal essential elements of legal reasoning.

AI tools enable generating emotionally engaging hypotheticals paired with appropriate visual representations rapidly. DALL-E 3 generates images from text descriptions including diagrams and flowcharts. Gamma AI creates presentations incorporating AI-generated images through DALL-E 3, Ideogram, and Flux alongside text. NotebookLM generates audio overviews as podcasts and video overviews with narration, enabling multimodal content creation from single sources.

The integration of emotional and visual elements creates memorable learning experiences addressing legal education's challenge of helping students retain complex doctrine. Immordino-Yang established that meaningful learning is inherently emotional, with students only thinking deeply about things they care about. Visual representations leverage the 83% of information processing occurring through vision. LaBar and Cabeza demonstrated emotional events attain privileged status in memory with greater retention advantages. Combining these pathways creates synergistic effects where emotional and visual encoding reinforce each other, producing stronger memory traces than either alone.

Faculty time constraints that previously limited creating emotionally engaging visual hypotheticals diminish significantly with AI tools. Generating variations for different student learning styles, creating multiple examples to illustrate subtle distinctions, and adapting materials for diverse cultural contexts become feasible at scale. The capacity to produce such materials rapidly enables pedagogical approaches that were theoretically sound but practically infeasible due to resource limitations.

## Multi-loading pedagogics and competency requirements

Multi-modal pedagogy has developed as educational response to changing communication landscapes. The New London Group established contemporary communication requires "multiplicities of media and modes" beyond language-focused literacy, addressing transcultural and multicultural classroom needs. Lim, Toh, and Nguyen's systematic review of ten years of multimodal pedagogies research examined visual, aural, linguistic, gestural, and spatial modes. Verbal modes blend with non-verbal modes for effective learning, challenging traditional verbal-dominated education. Stein's work on multimodal pedagogies in diverse classrooms addresses representation, rights, and resources. The shift from single-mode instruction to integrated multimodal approaches reflects workforce demands for broader competencies.

Universal Design for Learning provides frameworks for multi-loading pedagogics through three main principles. Multiple means of engagement address the "why" of learning, tapping into learners' interests, challenging appropriately, and motivating through the affective network. Multiple means of representation address the "what" of learning, providing multiple ways to acquire information and build knowledge through the recognition network. Multiple means of action and expression address the "how" of learning, offering alternatives for demonstrating learning through the strategic network. Based on neuroscience research identifying three primary neurological networks, UDL aims to develop "expert learners" who are purposeful, resourceful, and strategic.

Scott, McGuire, and Shaw introduced Universal Design for Instruction as a paradigm for postsecondary education. Burgstahler and Cory extended applications across higher education contexts. Tobin and Behling's "Plus One Approach" for UDL implementation identifies course "pinch points" where students struggle, enabling targeted multimodal interventions. The University of Illinois Chicago framework integrates cognitive, socio-emotional, and behavioural domains, addressing multiple ways students access material, engage, and demonstrate knowledge.

Multiple Intelligences Theory, whilst controversial, influences educational practice. Gardner identified eight intelligences: linguistic, logical-mathematical, spatial, bodily-kinesthetic, musical, interpersonal, intrapersonal, and naturalistic. Waterhouse's critique argues MI lacks empirical support, with no direct evidence for brain-based independent intelligences. Factor studies fail to show independence. Klein challenged MI on definitional and methodological grounds. Intelligence tests show high correlations between domains supporting general intelligence "g" rather than independent faculties. Critics note conflation with learning styles and unfalsifiability due to unclear definitions.

However, Shearer reviewed 500+ fMRI studies identifying distinct brain networks: visual networks corresponding to visual-spatial intelligence, somatomotor networks to kinesthetic intelligence, fronto-parietal networks to logical intelligence, auditory networks to musical intelligence, and default mode networks to intra and interpersonal intelligences. Educational applications emphasise recognising diverse learner strengths and offering multiple pathways to learning and expression, principles consistent with UDL regardless of MI's validity as neuroscientific theory.

Technological Pedagogical Content Knowledge provides frameworks for AI integration in multi-loading pedagogics. Building on Shulman's Pedagogical Content Knowledge, Mishra and Koehler identified seven knowledge domains: Technology Knowledge understanding various technologies, Content Knowledge of subject matter, Pedagogical Knowledge of teaching methods, Pedagogical Content Knowledge, Technological Content Knowledge showing how technology transforms content, Technological Pedagogical Knowledge showing how technology changes teaching, and TPACK integrating all three knowledge bases. Koehler and Mishra emphasised context matters, with Mishra adding "Contextual Knowledge" as essential consideration. Content, pedagogy, and technology must drive selection rather than technology alone.

Celik examined Intelligent-TPACK, exploring teachers' professional knowledge to ethically integrate AI-based tools into education. TPACK applied beyond K-12 to health professions education guides technology-based teaching activities and assesses instructors' knowledge and expertise with technology integration. Schmidt and colleagues developed assessment instruments for TPACK in preservice teachers, validating measures of technological pedagogical content knowledge.

Learning objects that engage multiple dimensions simultaneously have been theorised but remain challenging to implement at scale. IEEE 1484.12.1:2002 defines learning objects as "collections of content items, practice items, and assessment items combined based on single learning objectives" with metadata structures. Core characteristics include discoverability through Learning Object Metadata, reusability across contexts through IMS Content Package standards, and interoperability through SCORM standards. Chiappe defines learning objects as "digital self-contained and reusable entities with clear educational purposes, with at least three internal and editable components: content, learning activities, and elements of context".

Object-based learning engages cognitive, psychomotor, and affective dimensions simultaneously. Chatterjee, Hannan, and Thomson introduced object-based learning and multisensory engagement in higher education. Hannan, Duhs, and Chatterjee established OBL as powerful pedagogy for higher education. Physical object handling significantly increases interest versus online learning, promoting "aisthesis" and embodied learning. Working with head (cognitive), hands (psychomotor), and heart (affective) simultaneously creates holistic learning experiences suitable for interdisciplinary learning.

Churchill identified learning object typologies: presentation, practice, simulation, conceptual models, information, and contextual representation. Each type engages different pedagogical handles, with multi-loading objects combining multiple types. AI enables creating such objects at scale previously impossible due to development costs and technical barriers.

AI literacy and digital literacy constitute competence requirements for effective AI integration in education. Long and Magerko defined AI literacy as abilities to critically evaluate AI, communicate with AI, and utilise AI as tools, encompassing four key aspects: recognising and understanding AI, using and applying AI, evaluating and creating AI, and understanding AI ethics. Kong, Cheung, and Zhang developed three-dimensional frameworks: cognitive dimensions understanding AI concepts, affective dimensions addressing attitudes toward AI, and sociocultural dimensions considering broader social impacts.

Digital Promise's 2024 framework articulated three modes of engagement: UNDERSTAND acquiring basic AI knowledge for informed decisions, EVALUATE centering human judgment to critically consider benefits and costs, and USE encompassing interact, create, and problem-solve applications. Six AI literacy practices include data privacy and security, ethics and impact, digital communication and expression, information and mis- and disinformation, data analysis and inference, and algorithmic thinking, abstraction, and decomposition. Core values emphasise human judgment and centering justice.

UNESCO's 2024 AI Competency Framework for Teachers articulated five aspects at three levels: human-centered mindset, ethics of AI, AI foundations and applications, AI pedagogy, and AI for professional development. Progression moves from acquire to deepen to create phases. The EC and OECD AILit Framework developed with Code.org identified four domains: engaging with AI, creating with AI, evaluating AI critically, and understanding AI ethics. Ng and colleagues' exploratory review conceptualised AI literacy as encompassing data literacy, computational thinking, ethics, and critical evaluation of AI-generated content.

Faculty development proves essential for effective AI integration. Mah and Groß studied 122 faculty from German higher education institutions, finding perceived benefits included greater equity in education, personalised learning, enhanced student understanding, and positive influence on learning outcomes. Perceived challenges centred on lack of AI literacy among students and faculty, ethical considerations, curriculum development needs, and infrastructure requirements. Latent class analysis identified four faculty profiles: Optimistic (33.5%) agreeing with benefits and disagreeing with challenges, Critical (27.3%) disagreeing with benefits and agreeing with challenges, Critically Reflected (33.9%) agreeing with both benefits and challenges, and Neutral (5.3%) showing low ratings for both.

Usage patterns showed highest use for conception and preparation of teaching, with 84% using AI in at least two of three areas: personal, teaching, or work. Professional development interest appeared strong, with 78.5% interested in teaching and learning with AI tools, 66.4% interested in AI-based tools training, and 48.6% interested in AI for research. Preferred formats emphasised self-paced online courses. Planned time investment ranged from 5-20 hours. Motivation included 48.8% pursuing curiosity about AI applications.

Luckin and colleagues developed seven-step AI readiness frameworks: excite, tailor and hone, identify, collect, apply, learn, and iterate. Emphasis falls on contextualising AI to diversity of occupations, workplaces, and sectors, empowering active engagement. Chiu and colleagues conducted systematic literature reviews on opportunities, challenges, and future research recommendations for AI in education. Ifenthaler and colleagues' Delphi study identified priorities: privacy and ethical usage of AI, trustworthy algorithms, and equity and fairness in AIEd.

Bond and colleagues' meta-systematic review of AI in higher education called for increased ethics, collaboration, and rigour. Faculty age differences emerged, with faculty over 30 years rating positive impact of AI on learning outcomes significantly higher than those under 30 years. Professional development delivery increasingly occurs through digital learning platforms including Coursera, edX, FutureLearn, Udemy, and specialised platforms like AI Campus in Germany. Emerging trends include digital badges and micro-credentials for AI competency, blended and self-paced formats, context-specific domain training, and integration of TPACK frameworks with AI.

Redecker's European Framework for the Digital Competence of Educators (DigCompEdu) and Vuorikari, Kluzer, and Punie's DigComp 2.2 for citizens establish digital competence foundations upon which AI literacy builds. The relationship encompasses digital readiness, media literacy, computational thinking, and critical 21st century skills: communication, collaboration, critical thinking, and creativity. Faculty competencies for AI integration require intersection of TPACK frameworks, digital and AI literacy, UDL principles for inclusive design, understanding of multimodal learning, and critical evaluation skills.

## Constraints and implementation considerations

Research gaps constrain evidence-based implementation. Legal education-specific empirical research on AI integration remains sparse. Systematic searches of major legal education journals—Journal of Legal Education, Law Teacher, Clinical Law Review—yielded minimal peer-reviewed research on AI adoption and effectiveness. Documentation comes primarily from institutional sources, law school press releases, and legal education media rather than rigorous empirical studies. The field lacks longitudinal research tracking learning outcomes over time, comparative effectiveness studies with proper controls, and systematic evaluation of different integration approaches.

Methodological limitations affect existing research. Weidlich and colleagues' systematic review found only four of nineteen studies satisfied basic methodological criteria for interpretable learning effects. Many studies rely on self-report measures introducing potential bias, single time-point surveys preventing causal inference, convenience sampling limiting generalisability, and small samples from single institutions. The "fast science" phenomenon driven by pressure to publish on trending topics risks research waste without rigorous experimental design.

Equity concerns require attention. Digital divides create uneven access to AI tools and high-quality internet connectivity. Algorithmic bias in AI systems may disadvantage underrepresented students. Limited research examines how AI adoption affects first-generation college students or students from under-resourced educational backgrounds. Data privacy and security concerns disproportionately affect vulnerable populations lacking resources to protect personal information.

Over-reliance represents documented risks. Studies show AI dialogue systems may diminish drive and commitment to learning, reduce critical thinking and analytical skills, encourage uncritical acceptance of AI-generated content, and promote over-reliance without verifying validity. Students expressing concerns about AI identified incorrect and imprecise answers (48.2%), negative impacts on critical thinking (16.5%), risk of over-dependence (16.5%), and data privacy (9.4%) as primary issues.

Faculty resistance reflects legitimate concerns beyond technological barriers. Identity-based objections, value system conflicts, ethical concerns about replacing human creativity and critical thinking, and fears about broader negative societal consequences motivate non-use. Shata and Hartley found both users and non-users express academic dishonesty concerns, but non-users associate GAI with broader negative consequences. The AAUP report found 87% of respondents considered protecting intellectual property rights over academic work important, with significant percentages reporting AI led to worse outcomes for pay equity (27%), academic freedom (20%), and job enthusiasm (38%).

Implementation requires institutional support structures. Chan developed comprehensive AI policy education frameworks encompassing pedagogical, governance, and operational dimensions. Policies addressing data privacy, ethical AI use guidelines, and training programmes for technological literacy prove essential. Incorporating critical thinking and AI literacy into curricula helps students evaluate AI-generated content limitations. Balance between AI efficiency and human-centered methods maintains educational quality whilst leveraging AI benefits.

The competence requirements extend beyond technical skills to pedagogical judgment. Educators must understand when multi-handle content serves learning objectives, how to evaluate AI-generated materials for appropriateness, how to scaffold student use avoiding over-reliance, how to integrate AI-generated content with human instruction, and how to address ethical concerns transparently. TPACK frameworks guide these judgments, but developing expertise requires practice, reflection, and ongoing professional development.

Cultural contexts affect implementation. Most research derives from Western educational settings, limiting understanding of applicability across diverse cultural contexts. Language barriers affect non-English contexts, as documented in Czech legal education where GenAI teaching assistants faced accessibility challenges. Transfer across subjects requires discipline-specific adaptation, with legal education's unique demands for precision, citation accuracy, and professional responsibility creating particular constraints.

Multi-loading pedagogics demands intentional design. Simply using AI tools does not guarantee multi-dimensional engagement. Educators must explicitly identify which pedagogical handles to activate, design prompts generating appropriate content, scaffold student interaction with AI-generated materials, assess whether multiple dimensions genuinely engage or whether engagement remains superficial, and iterate based on learning outcomes rather than technological novelty.

## Synthesis and forward paths

AI in legal education stands at a threshold between one-dimensional implementation and multi-dimensional potential. Current adoption focuses predominantly on efficiency: summarising texts, generating quizzes, providing automated feedback. These applications offer value but underutilise AI's distinctive capacities. The technology's ability to generate varied, contextualised content engaging multiple pedagogical dimensions simultaneously addresses challenges that have constrained legal education for decades.

The four pedagogical handles—emotional, sensory, social, and metacognitive—operate through well-established mechanisms. Emotions activate neural pathways enhancing memory encoding and retention. Multiple sensory modalities distribute cognitive load whilst increasing retrieval pathways. Social learning leverages communities of practice and legitimate peripheral participation. Metacognition develops self-regulated learning capacities transferable across contexts. Educational psychology research demonstrates these dimensions function synergistically rather than additively.

Legal education has embraced some dimensions whilst resisting others. Clinical programmes incorporate emotional intelligence and empathy training. Visual aids and concept maps appear sporadically. Collaborative learning occurs but competes with competitive grading structures. Reflective practice gains acceptance as metacognitive development despite initial resistance. However, integration remains inconsistent, and resource constraints limit comprehensive implementation.

AI removes resource barriers that previously prevented multi-dimensional pedagogy at scale. Generating emotionally engaging hypotheticals with varied factual contexts, creating visual representations paired with text, producing scenarios for peer evaluation practice without social stakes, and scaffolding metacognitive reflection through iterative feedback—these applications become feasible when AI handles labour-intensive content generation. Faculty time redirects from material creation to pedagogical design, instructional facilitation, and learning assessment.

The shift requires reconceptualising AI's role from answer provider to content generator. Expectations of perfect doctrinal accuracy constrain AI use inappropriately. Hallucination rates and citation errors make AI unsuitable as authoritative sources. However, pedagogical materials need not be doctrinally perfect to achieve learning objectives. Hypotheticals with fabricated case names serve educational purposes when students analyse legal principles rather than memorise facts. Scenarios exploring empathy and perspective-taking need emotional authenticity more than citation precision. Peer evaluation practice benefits from AI-generated answers of varying quality precisely because students develop assessment skills through identifying flaws.

Multi-loading pedagogics frameworks provide structures for intentional design. UDL principles ensure multiple means of engagement, representation, and action whilst addressing diverse learner needs. TPACK guides integration of technology, pedagogy, and content knowledge. Object-based learning activates cognitive, psychomotor, and affective dimensions simultaneously. These frameworks share emphasis on flexibility, multiple pathways, student-centered approaches, context-dependent application, and ethical considerations.

Competence requirements extend beyond technical skills. AI literacy encompasses recognising and understanding AI, using and applying AI, evaluating and creating AI, and understanding AI ethics. Faculty development must address not only tool proficiency but pedagogical judgment about when and how to deploy AI-generated content. TPACK integration with AI competency creates comprehensive frameworks for educator preparation. Professional development formats including self-paced online courses, digital badges, micro-credentials, and context-specific domain training enable scalable faculty support.

Research gaps demand attention. Legal education lacks empirical studies measuring learning outcomes from AI integration, longitudinal research tracking skill development and retention, comparative effectiveness studies with proper experimental controls, systematic evaluation of different pedagogical approaches using AI, and investigation of equity impacts across diverse student populations. Addressing these gaps requires collaboration between legal educators, educational researchers, and AI developers using rigorous methodologies.

The path forward balances enthusiasm with caution. AI offers genuine solutions to long-standing pedagogical challenges in legal education. Multi-dimensional engagement improves learning outcomes through synergistic activation of emotional, sensory, social, and metacognitive pathways. However, implementation requires intentional design, instructor oversight, ethical safeguards, and ongoing assessment. Over-reliance risks, academic integrity concerns, equity considerations, and faculty resistance reflect legitimate issues requiring systematic responses.

Legal education has opportunity to lead in developing multi-loading AI pedagogics. The discipline's emphasis on precision, critical thinking, and professional responsibility creates both challenges and advantages. Challenges include heightened accuracy expectations and citation requirements. Advantages include strong traditions of case method teaching, clinical education integrating multiple dimensions, and professional identity formation encompassing emotional intelligence and ethical judgment. These foundations support multi-dimensional AI integration aligned with legal education's distinctive demands.

The transformation from one-dimensional to multi-dimensional AI use represents not merely technological adoption but pedagogical evolution. It requires educators to reconceptualise AI as content generation tool enabling previously impractical teaching strategies rather than as answer provider replacing human judgment. It demands institutional support through policies, infrastructure, and professional development. It necessitates ongoing research evaluating effectiveness whilst addressing equity and ethics. Implemented thoughtfully, multi-loading AI pedagogics can address fundamental challenges in legal education whilst preparing students for professional contexts where human judgment guides AI capabilities rather than deferring to them.